{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that *evaluates* the actions taken rather than *instructs* by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior. Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken. This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken.\\\n",
    "强化学习与其他学习类型最重要的区别是，它使用的训练信息是*评估*所采取的行动，而不是通过给出正确的行动来*指导*。这就产生了积极探索的需要，明确寻找良好行为的需要。纯粹的评价性反馈显示了所采取的行动有多好，而不是它是最好的还是最坏的行动。另一方面，纯粹有益的反馈表明了应该采取的正确行动，独立于实际采取的行动。这种反馈是监督学习的基础，监督学习包括大部分的模式分类、人工神经网络和系统识别。从纯粹的形式来看，这两种反馈是截然不同的:评价性反馈完全取决于所采取的行动，而指导性反馈则独立于所采取的行动。\n",
    "\n",
    "In this chapter we study the evaluative aspect of reinforcement learning in a simplified setting, one that does not involve learning to act in more than one situation. This *nonassociative* setting is the one in which most prior work involving evaluative feedback has been done, and it avoids much of the complexity of the full reinforcement learning problem. Studying this case enables us to see most clearly how evaluative feedback di↵ers from, and yet can be combined with, instructive feedback.\\\n",
    "在本章中，我们将在一个简化的设置中研究强化学习的评估方面，即不涉及在多个情况下学习如何行动。这种*非联想*设置是大多数之前涉及到评价反馈的工作已经完成的，并且它避免了完全强化学习问题的大部分复杂性。研究这个案例使我们能够最清楚地看到评价性反馈是如何来自于，并且可以与指导性反馈相结合的。\n",
    "\n",
    "The particular nonassociative, evaluative feedback problem that we explore is a simple version of the $k$-armed bandit problem. We use this problem to introduce a number of basic learning methods which we extend in later chapters to apply to the full reinforcement learning problem. At the end of this chapter, we take a step closer to the full reinforcement learning problem by discussing what happens when the bandit problem becomes associative, that is, when actions are taken in more than one situation.\\\n",
    "我们所探讨的特殊的非联想的、评估性反馈问题是$k$臂老虎机问题的一个简单版本。我们利用这个问题来介绍一些基本的学习方法，我们将在后面的章节中扩展这些方法，以应用于完全的强化学习问题。在本章的最后，我们通过讨论当强盗问题变成关联性时(即在多种情况下采取行动时)会发生什么，向完整的强化学习问题迈进了一步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A $k$-armed Bandit Problem\n",
    "\n",
    "Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or **time steps**.\\\n",
    "考虑下面的学习问题。你反复地面对$k$个不同的选择，或者行动。在每个选择之后，你会得到一个数字奖励，这个奖励是根据你选择的行动从一个平稳的概率分布中选择的。你的目标是在一段时间内最大化期望的总奖励，例如，超过1000个行动选择或*时间步骤*。\n",
    "\n",
    "This is the original form of the **$k$-armed bandit problem**, so named by analogy to a slot machine, or “one-armed bandit,” except that it has $k$ levers instead of one. Each action selection is like a play of one of the slot machine’s levers, and the rewards are the payoffs for hitting the jackpot. Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levers. Another analogy is that of a doctor choosing between experimental treatments for a series of seriously ill patients. Each action is the selection of a treatment, and each reward is the survival or well-being of the patient. Today the term “bandit problem” is sometimes used for a generalization of the problem described above, but in this book we use it to refer just to this simple case.\\\n",
    "这是**$k$臂老虎机（$k$-armed bandit problem）**的原始形式，得名于slot machine，或“one-armed bandit”，只是它有$k$杠杆，而不是一个。每个行动选择就像是玩老虎机的杠杆，而奖励则是获得头奖的回报。通过重复的行动选择，你可以通过将你的行动集中在最好的杠杆上来最大化你的收益。另一个类比是医生在一系列重病患者的实验性治疗方案中做出选择。每一个行动都是对治疗方法的选择，每一个奖励都是病人的生存或幸福。今天，术语“bandit problem”有时被用来概括上面描述的问题，但在本书中，我们用它来指代这个简单的情况。\n",
    "\n",
    "In our $k$-armed bandit problem, each of the $k$ actions has an expected or mean reward given that that action is selected; let us call this the **value** of that action. We denote the action selected on time step $t$ as $A_t$, and the corresponding reward as $R_t$. The value then of an arbitrary action $a$, denoted $q_*(a)$, is the expected reward given that $a$ is selected:\\\n",
    "在我们的$k$臂老虎机问题中，每一个$k$的行动都有一个预期的或平均的奖励，前提是该行动被选择了；让我们把这个称为动作的**价值**。我们表示在时间步长的$t$上选择的动作为$A_t$，相应的奖励为$R_t$。那么任意动作$a$的值，表示为$q_*(a)$，就是选择$a$后的预期回报：\n",
    "$$\n",
    "q_*(a) = \\mathbb{E}[R_t | A_t = a]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-value Methods\n",
    "\n",
    "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call **action-value methods**. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received:\\\n",
    "首先，我们要更仔细地研究评估行动价值的方法，以及使用评估做出行动选择决策的方法，我们统称为**行动-价值方法**。回想一下，行动的真正价值是选择行动时获得的平均奖励。估计这一点的一个自然方法是将实际收到的奖励平均：\n",
    "$$\n",
    "Q_t(a) \n",
    "= \\frac{\\text{ sum of rewards when $a$ taken prior to $t$}}{\\text{number of times $a$  taken prior to $t$}}\n",
    "\\tag{2.1}\n",
    "$$\n",
    "If the denominator is zero, then we instead define $Q_t(a)$ as some default value, such as $0$.\\\n",
    "如果分母为0，则将$Q_t(a)$定义为某个默认值，例如$0$。\n",
    "\n",
    "As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q_*(a)$. We call this the **sample-average** method for estimating action values because each estimate is an average of the sample of relevant rewards. Of course this is just one way to estimate action values, and not necessarily the best one. Nevertheless, for now let us stay with this simple estimation method and turn to the question of how the estimates might be used to select actions.\\\n",
    "当分母趋于无穷时，根据大数定律，$Q_t(a)$收敛于$q_*(a)$。我们将其称为估算行动值的**样本-平均**方法，因为每个估算都是相关奖励样本的平均值。当然，这只是评估行动价值的一种方法，不一定是最好的方法。然而，现在让我们继续使用这个简单的评估方法，并转向如何使用评估来选择操作的问题。\n",
    "\n",
    "The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as\\\n",
    "最简单的操作选择规则是选择估算值最高的操作之一，也就是前面部分定义的贪婪操作之一。如果贪心行为不止一个，那么就会以某种任意的方式，也许是随机的方式进行选择。我们把这种贪婪行为选择方法写成\n",
    "$$\n",
    "A_t = \\underset{a}{\\operatorname{argmax}} Q_t(a)\n",
    "\\tag{2.2}\n",
    "$$\n",
    "\n",
    "Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability $\\varepsilon$, instead select randomly from among all the actions with equal probability, independently of the action-value estimates. We call methods using this near-greedy action selection rule **$\\varepsilon$-greedy** methods. An advantage of these methods is that, in the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the $Q_t(a)$ converge to $q_*(a)$. This of course implies that the probability of selecting the optimal action converges to greater than $1-\\varepsilon$, that is, to near certainty. These are just asymptotic guarantees, however, and say little about the practical effectiveness of the methods.\\\n",
    "贪婪行为选择总是利用现有知识来最大化即时回报；它不会花任何时间对明显较差的动作进行抽样，看看它们是否真的可能更好。一种简单的替代方法是在大多数时候都表现出贪婪的行为，但是每隔一段时间，比如以较小的概率$\\varepsilon$，而不是以相等的概率从所有动作中随机选择，独立于动作值的估计。我们称使用这个近乎贪婪的操作选择规则的方法为**$\\varepsilon$-greedy**方法。这些方法的一个优点是，在步数增加的极限下，每个动作将被采样无限次，从而确保所有的$Q_t(a)$收敛到$q_*(a)$。这当然意味着选择最优行动的概率收敛于大于$1-\\varepsilon$，即接近确定性。然而，这些只是渐近的保证，而很少提到这些方法的实际效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": "2",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
