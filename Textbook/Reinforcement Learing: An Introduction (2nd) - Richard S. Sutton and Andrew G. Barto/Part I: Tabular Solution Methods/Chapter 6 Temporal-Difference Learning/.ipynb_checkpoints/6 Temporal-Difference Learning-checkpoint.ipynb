{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be __temporal-difference (TD) learning__. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap). The relationship between TD, DP, and Monte Carlo methods is a recurring theme in the theory of reinforcement learning; this chapter is the beginning of our exploration of it. Before we are done, we will see that these ideas and methods blend into each other and can be combined in many ways. In particular, in Chapter 7 we introduce $n$-step algorithms, which provide a bridge from TD to Monte Carlo methods, and in Chapter 12 we introduce the TD($\\lambda$) algorithm, which seamlessly unifies them.\\\n",
    "如果非要找出一个强化学习的核心和新颖的观点，那无疑就是__时间差异学习（Temporal Difference (TD) Learning）__。TD学习是Monte Carlo思想和动态规划（DP）思想的结合。像Monte Carlo方法一样，TD方法可以直接从原始经验中学习，而不需要一个环境动态模型。与DP一样，TD方法在一定程度上根据其他学习到的估算来更新估算，而无需等待最终结果（它们是自举）。TD、DP和Monte Carlo方法之间的关系是强化学习理论中反复出现的主题；这一章是我们探索的开始。在我们完成之前，我们将看到这些想法和方法相互融合，并可以以多种方式组合。特别地，在第7章我们介绍了$n$步算法，它提供了一个从TD到Monte Carlo方法的桥梁，在第12章我们介绍了TD($\\lambda$)算法，它无缝地统一了它们。\n",
    "\n",
    "As usual, we start by focusing on the policy evaluation or prediction problem, the problem of estimating the value function $v_\\pi$ for a given policy $\\pi$. For the control problem (finding an optimal policy), DP, TD, and Monte Carlo methods all use some variation of generalized policy iteration (GPI). The differences in the methods are primarily differences in their approaches to the prediction problem.\\\n",
    "与往常一样，我们首先关注策略评估或预测问题，即估算给定策略$\\pi$的值函数$v_\\pi$的问题。对于控制问题(寻找最优策略)，DP、TD和蒙特卡罗方法都使用了广义策略迭代（GPI）的一些变异。这些方法的不同主要是它们处理预测问题的方法不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD predition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following a policy $\\pi$, both methods update their estimate $V$ of $v_\\pi$ for the nonterminal states $S_t$ occurring in that experience. Roughly speaking, Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V(S_t)$. A simple every-visit Monte Carlo method suitable for nonstationary environments is\\\n",
    "TD方法和Monte Carlo方法都是利用经验来解决预测问题。在使用策略$\\pi$的情况下，这两种方法都更新了在该策略中出现的非终止状态$S_t$的$v_\\pi$的估计值$V$。粗略地说，Monte Carlo方法会等待，直到知道访问之后的return，然后使用该return作为$V(S_t)$的目标。一种简单的适用于非平稳环境的每次访问Monte Carlo方法为\n",
    "\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha[G_t - V(S_t)]\n",
    "$$\n",
    "\n",
    "where $G_t$ is the actual return following time $t$, and $\\alpha$ is a constant step-size parameter (c.f., Equation 2.4). Let us call this method __constant-$\\alpha$ MC__.\\\n",
    "其中$G_t$是时间$t$之后的实际return，$\\alpha$是一个常量步长参数（式2.4）。让我们称这个方法为__常量-$\\alpha$ MC__。\n",
    "\n",
    "Whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known), TD methods need to wait only until the next time step. At time $t + 1$ they immediately form a target and make a useful update using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$. The simplest TD method makes the update\\\n",
    "Monte Carlo方法必须等到事件结束时才确定$V(S_t)$的增量（只有那时$G_t$才已知），而TD方法只需要等到下一个时间步骤。在时间$t +1$时，他们立即形成一个目标，并使用观察到的奖励$R_{t+1}$和估计$V(S_{t+1})$进行有用的更新。最简单的TD方法当转换到$S_{t+1}$并接收$R_{t+1}$时，立即进行下式的更新\n",
    "\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]\n",
    "$$\n",
    "\n",
    "immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$. In effect, the target for the Monte Carlo update is $G_t$, whereas the target for the TD update is $R_{t+1} + \\gamma V(S_{t+1})$. This TD method is called __TD(0)__, or __one-step TD__, because it is a special case of the TD($\\lambda$) and $n$-step TD methods developed in Chapter 12 and Chapter 7. The box below specifies TD(0) completely in procedural form.\\\n",
    "实际上，Monte Carlo更新的目标是$G_t$，而TD更新的目标是$R_{t+1} + \\gamma V(S_{t+1})$。这种TD方法被称为__TD(0)__，或者__一步TD__，因为它是在第12章和第7章中开发的TD($\\lambda$)和$n$步TD方法的一个特殊情况。下面的框完全以程序形式指定了TD(0)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __<font color = red> Algorithm </font> TD(0)__\n",
    "\n",
    "__<font color = red> Algorithm </font> Tabular TD(0) for estimating $v_\\pi$__\n",
    "* Input: the policy $\\pi$ to be evaluated\n",
    "* Algorithm parameter: step size $\\alpha \\in (0,1]$\n",
    "* Initialize $V(s)$ for all $s \\in \\mathcal{S}^+$, arbitrarily except $V(Terminal) = 0$\n",
    "* Loop for each episode:\n",
    "    * Initialize $S$\n",
    "    * Loop for each step of episode until $S$ is terminal:\n",
    "        * $A \\leftarrow$ action given by $\\pi$ for $S$\n",
    "        * Take action $A$, observe $R$, $S'$\n",
    "        * $V(S) \\leftarrow V(S) + \\alpha[R + \\gamma V(S') - V(S)]$\n",
    "        * $S \\leftarrow S'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of TD Prediction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality of TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa: On-policy TD Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn now to the use of TD prediction methods for the control problem. As usual, we follow the pattern of generalized policy iteration (GPI), only this time using TD methods for the evaluation or prediction part. As with Monte Carlo methods, we face the need to trade off exploration and exploitation, and again approaches fall into two main classes: on-policy and off-policy. In this section we present an on-policy TD control method.\\\n",
    "现在我们转到使用TD预测方法来解决控制问题。通常，我们遵循广义策略迭代(GPI)的模式，只是这次在评估或预测部分使用TD方法。与蒙特卡罗方法一样，我们需要权衡探索和利用，方法又分为两大类：同策和异策。在本节中，我们将介绍一种同策的TD控制方法。\n",
    "\n",
    "The first step is to learn an action-value function rather than a state-value function. In particular, for an on-policy method we must estimate $q_\\pi(s, a)$ for the current behavior policy $\\pi$ and for all states $s$ and actions $a$. This can be done using essentially the same TD method described above for learning $v_\\pi$. Recall that an episode consists of an alternating sequence of states and state–action pairs:\\\n",
    "第一步是学习动作价值函数而不是状态价值函数。特别地，对于同策方法，我们必须为当前行为策略$\\pi$和所有状态$s$和动作$a$估算$q_\\pi(s, a)$。这可以使用上面描述的基本相同的TD方法来学习$v_\\pi$。回想一下，一个episode由交替序列的状态和状态-动作对组成:\n",
    "\n",
    "In the previous section we considered transitions from state to state and learned the values of states. Now we consider transitions from state–action pair to state–action pair, and learn the values of state–action pairs. Formally these cases are identical: they are both Markov chains with a reward process. The theorems assuring the convergence of state values under TD(0) also apply to the corresponding algorithm for action values:\\\n",
    "在上一节中，我们考虑了状态到状态的转换，并了解了状态的值。现在我们考虑从状态-动作对到状态-动作对的转换，并了解状态-动作对的值。这两种情况在形式上是相同的：它们都是带有reward过程的Markov链。保证状态值在TD(0)下收敛的定理也适用于相应的动作值算法：\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n",
    "$$\n",
    "\n",
    "This update is done after every transition from a nonterminal state $S_t$. If $S+{t+1}$ is terminal, then $Q(S_{t+1}, A_{t+1})$ is defined as zero. This rule uses every element of the quintuple of events, $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$, that make up a transition from one state–action pair to the next. This quintuple gives rise to the name __Sarsa__ for the algorithm. The backup diagram for Sarsa is as shown to the right.\\\n",
    "这个更新在每次从非终止状态$S_t$转换之后执行。如果$S+{t+1}$是终止的，则$Q(S_{t+1}, A_{t+1})$被定义为零。该规则使用事件五元组中的每个元素$(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$，它们构成了从一个状态-动作对到下一个状态-动作对的转换。这个五元组为算法命名为__Sarsa__。Sarsa的备份图如图所示。\n",
    "\n",
    "It is straightforward to design an on-policy control algorithm based on the Sarsa prediction method. As in all on-policy methods, we continually estimate $q_\\pi$ for the behavior policy $\\pi$, and at the same time change $\\pi$ toward greediness with respect to $q_\\pi$. The general form of the Sarsa control algorithm is given in the box.\\\n",
    "基于Sarsa预测方法设计一种on-policy控制算法是很简单的。与所有的on-policy方法一样，我们不断地为行为策略$\\pi$估计$q_\\pi$，同时将$\\pi$改变为相对于$q_\\pi$的贪婪。Sarsa控制算法的一般形式在方框中给出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __<font color = red> Algorithm </font> Sarsa__\n",
    "\n",
    "__<font color = red> Algorithm </font> Sarsa (on-policy TD control) for estimating $Q \\approx q_*$__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convergence properties of the Sarsa algorithm depend on the nature of the policy’s dependence on $Q$. For example, one could use $\\varepsilon$-greedy or $\\varepsilon$-soft policies. Sarsa converges with probability 1 to an optimal policy and action-value function as long as all state–action pairs are visited an infinite number of times and the policy converges in the limit to the greedy policy (which can be arranged, for example, with $\\varepsilon$-greedy policies by setting $\\epsilon = 1/t$).\\\n",
    "Sarsa算法的收敛性取决于策略对$Q$的依赖性质。例如，可以使用$\\varepsilon$-greedy或$\\varepsilon$-soft策略。Sarsa以概率1收敛于最优策略和行为价值函数，只要所有的状态-行为对都被无限次访问，并且策略收敛于贪婪策略的极限（例如，可以通过设置$\\epsilon = 1/t$安排$\\varepsilon$贪婪策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning: Off-policy TD Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as __Q-learning__ (Watkins, 1989), defined by\\\n",
    "强化学习的早期突破之一是一种异策TD控制算法的开发，称为__Q-learning__（Watkins, 1989），由下式定义\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\underset{a}{\\max}Q(S_{t+1}, a) - Q(S_t, A_t)]\n",
    "$$\n",
    "\n",
    "In this case, the learned action-value function, $Q$, directly approximates $q_*$, the optimal action-value function, independent of the policy being followed. This dramatically simplifies the analysis of the algorithm and enabled early convergence proofs. The policy still has an effect in that it determines which state–action pairs are visited and updated. However, all that is required for correct convergence is that all pairs continue to be updated. As we observed in Chapter 5, this is a minimal requirement in the sense that any method guaranteed to find optimal behavior in the general case must require it. Under this assumption and a variant of the usual stochastic approximation conditions on the sequence of step-size parameters, $Q$ has been shown to converge with probability 1 to $q_*$. The Q-learning algorithm is shown below in procedural form.\\\n",
    "在这种情况下，学习到的动作值函数$Q$直接逼近最优的动作值函数$q_*$，与所遵循的策略无关。这极大地简化了算法的分析，并使早期的收敛性证明成为可能。策略仍然有作用，因为它决定访问和更新哪些状态-操作对。然而，所有正确收敛所需要的是所有对继续更新。正如我们在第5章中所观察到的，这是一个最低限度的要求，因为任何保证在一般情况下找到最优行为的方法都必须这样做。在此假设和步长参数序列的一般随机逼近条件的变化下，证明了$Q$以概率1收敛到$q_*$。Q-learning算法的程序形式如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __<font color = red> Algorithm </font> Q-learning__\n",
    "\n",
    "__<font color = red> Algorithm </font> Q-learning (off-policy TD control) for estimating $\\pi \\approx \\pi_*$__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Sarsa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": "6",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
