{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that *evaluates* the actions taken rather than *instructs* by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior. Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken. This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken.\\\n",
    "强化学习与其他学习类型最重要的区别是，它使用的训练信息是*评估*所采取的行动，而不是通过给出正确的行动来*指导*。这就产生了积极探索的需要，明确寻找良好行为的需要。纯粹的评价性反馈显示了所采取的行动有多好，而不是它是最好的还是最坏的行动。另一方面，纯粹有益的反馈表明了应该采取的正确行动，独立于实际采取的行动。这种反馈是监督学习的基础，监督学习包括大部分的模式分类、人工神经网络和系统识别。从纯粹的形式来看，这两种反馈是截然不同的：评价性反馈完全取决于所采取的行动，而指导性反馈则独立于所采取的行动。\n",
    "\n",
    "In this chapter we study the evaluative aspect of reinforcement learning in a simplified setting, one that does not involve learning to act in more than one situation. This *nonassociative* setting is the one in which most prior work involving evaluative feedback has been done, and it avoids much of the complexity of the full reinforcement learning problem. Studying this case enables us to see most clearly how evaluative feedback di↵ers from, and yet can be combined with, instructive feedback.\\\n",
    "在本章中，我们将在一个简化的设置中研究强化学习的评估方面，即不涉及在多个情况下学习如何行动。这种*非联想*设置是大多数之前涉及到评价反馈的工作已经完成的，并且它避免了完全强化学习问题的大部分复杂性。研究这个案例使我们能够最清楚地看到评价性反馈是如何来自于，并且可以与指导性反馈相结合的。\n",
    "\n",
    "The particular nonassociative, evaluative feedback problem that we explore is a simple version of the $k$-armed bandit problem. We use this problem to introduce a number of basic learning methods which we extend in later chapters to apply to the full reinforcement learning problem. At the end of this chapter, we take a step closer to the full reinforcement learning problem by discussing what happens when the bandit problem becomes associative, that is, when actions are taken in more than one situation.\\\n",
    "我们所探讨的特殊的非联想的、评估性反馈问题是$k$臂老虎机问题的一个简单版本。我们利用这个问题来介绍一些基本的学习方法，我们将在后面的章节中扩展这些方法，以应用于完全的强化学习问题。在本章的最后，我们通过讨论当强盗问题变成关联性时(即在多种情况下采取行动时)会发生什么，向完整的强化学习问题迈进了一步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A $k$-armed Bandit Problem\n",
    "\n",
    "Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or __time steps__.\\\n",
    "考虑下面的学习问题。你反复地面对$k$个不同的选择，或者行动。在每个选择之后，你会得到一个数字reward，这个reward是根据你选择的行动从一个平稳的概率分布中选择的。你的目标是在一段时间内最大化期望的总reward，例如，超过1000个行动选择或__时间步骤（time steps）__。\n",
    "\n",
    "This is the original form of the **$k$-armed bandit problem**, so named by analogy to a slot machine, or “one-armed bandit,” except that it has $k$ levers instead of one. Each action selection is like a play of one of the slot machine’s levers, and the rewards are the payoffs for hitting the jackpot. Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levers. Another analogy is that of a doctor choosing between experimental treatments for a series of seriously ill patients. Each action is the selection of a treatment, and each reward is the survival or well-being of the patient. Today the term “bandit problem” is sometimes used for a generalization of the problem described above, but in this book we use it to refer just to this simple case.\\\n",
    "这是**$k$臂老虎机（$k$-armed bandit problem）**的原始形式，得名于slot machine，或“one-armed bandit”，只是它有$k$杠杆，而不是一个。每个行动选择就像是玩老虎机的杠杆，而奖励则是获得头奖的回报。通过重复的行动选择，你可以通过将你的行动集中在最好的杠杆上来最大化你的收益。另一个类比是医生在一系列重病患者的实验性治疗方案中做出选择。每一个行动都是对治疗方法的选择，每一个奖励都是病人的生存或幸福。今天，术语“bandit problem”有时被用来概括上面描述的问题，但在本书中，我们用它来指代这个简单的情况。\n",
    "\n",
    "In our $k$-armed bandit problem, each of the $k$ actions has an expected or mean reward given that that action is selected; let us call this the **value** of that action. We denote the action selected on time step $t$ as $A_t$, and the corresponding reward as $R_t$. The value then of an arbitrary action $a$, denoted $q_*(a)$, is the expected reward given that $a$ is selected:\\\n",
    "在我们的$k$臂老虎机问题中，$k$个行动中的每一个都有一个期望的或平均的reward，前提是该行动被选择了；让我们把这个称为动作的**价值（value）**。我们表示在时间步长的$t$上选择的动作为$A_t$，相应的reward为$R_t$。那么任意动作$a$的值，表示为$q_*(a)$，就是选择$a$后的期望reward：\n",
    "\n",
    "$$\n",
    "q_*(a) = \\mathbb{E}[R_t \\mid A_t = a]\n",
    "$$\n",
    "\n",
    "If you knew the value of each action, then it would be trivial to solve the $k$-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action $a$ at time step $t$ as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q_*(a)$.\\\n",
    "如果你知道每个行动的价值，那么解决$k$臂老虎机问题就很简单了：你总是会选择具有最高价值的行动。我们假设您不确定地知道动作值，尽管您可能有估计。我们用$Q_t(a)$表示时间步长$t$时行动$a$的估计值。我们希望$Q_t(a)$接近$q_*(a)$。\n",
    "\n",
    "If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the **greedy** actions. When you select one of these actions, we say that you are **exploiting** your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are **exploring**, because this enables you to improve your estimate of the nongreedy action’s value. Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.\\\n",
    "如果您对动作值进行估计，那么在任何时间步骤中，至少有一个动作的估计值是最大的。我们称之为**贪婪（greedy）**行为。当你选择了这些行动中的一个，我们说你在**利用（exploit）**你对这些行动价值的现有知识。如果您选择一个非贪婪操作，那么我们说您正在**探索（explore）**，因为这使您能够改进对非贪婪动作值的估计。为了最大化一步的预期回报，利用是正确的做法，但从长远来看，探索可能会产生更大的总回报。\n",
    "For example, suppose a greedy action’s value is known with certainty, while several other actions are estimated to be nearly as good but with substantial uncertainty. The uncertainty is such that at least one of these other actions probably is actually better than the greedy action, but you don’t know which one. If you have many time steps ahead on which to make action selections, then it may be better to explore the nongreedy actions and discover which of them are better than the greedy action. Reward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore and to exploit with any single action selection, one often refers to the “conflict”between exploration and exploitation.\\\n",
    "例如，假设一个贪婪行为的值是确定的，而其他几个行为被估计为几乎相同的好，但有很大的不确定性。不确定性是这样的，至少有一种行为可能比贪婪行为好，但你不知道是哪种。如果您有很多时间步要选择动作，那么最好是探索非贪婪动作，并发现它们中哪一个比贪婪动作更好。在探索过程中，短期奖励较低，但长期奖励较高，因为在你发现更好的行动后，你可以多次利用它们。因为不可能通过任何单一的行动选择同时进行探索和开发，所以人们通常会提到探索和开发之间的“冲突”。\n",
    "\n",
    "In any specific case, whether it is better to explore or exploit depends in a complex way on the precise values of the estimates, uncertainties, and the number of remaining steps. There are many sophisticated methods for balancing exploration and exploitation for particular mathematical formulations of the k-armed bandit and related problems. However, most of these methods make strong assumptions about stationarity and prior knowledge that are either violated or impossible to verify in applications and in the full reinforcement learning problem that we consider in subsequent chapters. The guarantees of optimality or bounded loss for these methods are of little comfort when the assumptions of their theory do not apply.\\\n",
    "在任何特定的情况下，探索或利用是更好的，以一种复杂的方式取决于估计的精确值、不确定性和剩余步骤的数量。对于k-武装土匪及其相关问题的特定数学公式，有许多复杂的方法来平衡勘探和开发。然而，大多数这些方法都对平稳性和先验知识做出了强有力的假设，这些假设要么在应用中被违反，要么在我们后面章节中考虑的完全强化学习问题中无法验证。当这些方法的理论假设不适用时，这些方法的最优性或有限损失的保证并不令人满意。\n",
    "\n",
    "In this book we do not worry about balancing exploration and exploitation in a sophisticated way; we worry only about balancing them at all. In this chapter we present several simple balancing methods for the $k$-armed bandit problem and show that they work much better than methods that always exploit. The need to balance exploration and exploitation is a distinctive challenge that arises in reinforcement learning; the simplicity of our version of the $k$-armed bandit problem enables us to show this in a particularly clear form.\\\n",
    "在这本书中，我们不担心以复杂的方式平衡探索和开发;我们担心的只是平衡它们。在本章中，我们提出了几个简单的k-armed bandit问题的平衡方法，并证明它们比那些总是利用$k$-armed bandit问题的方法工作得更好。在强化学习中，需要平衡探索和开发是一个独特的挑战;我们版本的$k$-armed bandit问题的简单性使我们能够以一种特别清晰的形式展示这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-value Methods\n",
    "\n",
    "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call **action-value methods**. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received:\\\n",
    "首先，我们要更仔细地研究评估行动价值的方法，以及使用评估做出行动选择决策的方法，我们统称为**行动-价值方法**。回想一下，行动的真正价值是选择行动时获得的平均奖励。估计这一点的一个自然方法是将实际收到的奖励平均：\n",
    "$$\n",
    "Q_t(a) \n",
    "= \\frac{\\text{ sum of rewards when $a$ taken prior to $t$}}{\\text{number of times $a$  taken prior to $t$}}\n",
    "\\tag{2.1}\n",
    "$$\n",
    "If the denominator is zero, then we instead define $Q_t(a)$ as some default value, such as $0$.\\\n",
    "如果分母为0，则将$Q_t(a)$定义为某个默认值，例如$0$。\n",
    "\n",
    "As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q_*(a)$. We call this the **sample-average** method for estimating action values because each estimate is an average of the sample of relevant rewards. Of course this is just one way to estimate action values, and not necessarily the best one. Nevertheless, for now let us stay with this simple estimation method and turn to the question of how the estimates might be used to select actions.\\\n",
    "当分母趋于无穷时，根据大数定律，$Q_t(a)$收敛于$q_*(a)$。我们将其称为估算行动值的**样本-平均**方法，因为每个估算都是相关奖励样本的平均值。当然，这只是评估行动价值的一种方法，不一定是最好的方法。然而，现在让我们继续使用这个简单的评估方法，并转向如何使用评估来选择操作的问题。\n",
    "\n",
    "The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as\\\n",
    "最简单的操作选择规则是选择估算值最高的操作之一，也就是前面部分定义的贪婪操作之一。如果贪心行为不止一个，那么就会以某种任意的方式，也许是随机的方式进行选择。我们把这种贪婪行为选择方法写成\n",
    "$$\n",
    "A_t = \\underset{a}{\\operatorname{argmax}} Q_t(a)\n",
    "\\tag{2.2}\n",
    "$$\n",
    "\n",
    "Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability $\\varepsilon$, instead select randomly from among all the actions with equal probability, independently of the action-value estimates. We call methods using this near-greedy action selection rule **$\\varepsilon$-greedy** methods. An advantage of these methods is that, in the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the $Q_t(a)$ converge to $q_*(a)$. This of course implies that the probability of selecting the optimal action converges to greater than $1-\\varepsilon$, that is, to near certainty. These are just asymptotic guarantees, however, and say little about the practical effectiveness of the methods.\\\n",
    "贪婪行为选择总是利用现有知识来最大化即时回报；它不会花任何时间对明显较差的动作进行抽样，看看它们是否真的可能更好。一种简单的替代方法是在大多数时候都表现出贪婪的行为，但是每隔一段时间，比如以较小的概率$\\varepsilon$，而不是以相等的概率从所有动作中随机选择，独立于动作值的估计。我们称使用这个近乎贪婪的操作选择规则的方法为**$\\varepsilon$-greedy**方法。这些方法的一个优点是，在步数增加的极限下，每个动作将被采样无限次，从而确保所有的$Q_t(a)$收敛到$q_*(a)$。这当然意味着选择最优行动的概率收敛于大于$1-\\varepsilon$，即接近确定性。然而，这些只是渐近的保证，而很少提到这些方法的实际效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": "2",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
