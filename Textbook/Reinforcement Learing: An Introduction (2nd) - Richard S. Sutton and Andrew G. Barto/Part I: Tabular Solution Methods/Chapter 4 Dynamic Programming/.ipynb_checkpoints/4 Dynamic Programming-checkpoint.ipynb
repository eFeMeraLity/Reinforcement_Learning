{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically. DP provides an essential foundation for the understanding of the methods presented in the rest of this book. In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.\\\n",
    "术语动态规划（dynamic programming，DP）指的是一组算法，可以用来计算最优策略给出一个完美的环境模型作为马尔科夫决策过程（MDP）。经典的DP算法由于其假设是一个完美的模型，且计算量大，在强化学习方面的应用有限，但在理论上仍然很重要。DP为理解在这本书的其余部分中提出的方法提供了必要的基础。事实上，所有这些方法都可以被视为试图达到与DP相同的效果，只是需要更少的计算，而且没有假设一个完美的环境模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation (Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we consider how to compute the state-value function $v_\\pi$ for an arbitrary policy $\\pi$. This is called **policy evaluation** in the DP literature. We also refer to it as the **prediction problem**. Recall from Chapter 3 that, for all $s \\in \\mathcal{S}$,\\\n",
    "首先考虑如何计算任意策略$\\pi$的状态值函数$v_\\pi$。这在DP文献中被称为策略评估（policy evaluation）。我们也把它称为预测问题（prediction problem）。回想一下第3章，对于所有$s \\in \\mathcal{S}$，\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) \n",
    "&= \\mathbb{E}_\\pi [G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}_\\pi [R_{t+1} + \\gamma G_{t+1} | S_t=s], \\qquad \\tag{from (3.9)} \\\\\n",
    "&= \\mathbb{E}_\\pi [R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t=s] \\tag{4.3} \\\\\n",
    "&= \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_\\pi(s')] \\tag{4.4}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**<font color = green>Example 4.1</font> $4 \\times 4$ Gridworld**\\\n",
    "Consider the $4 \\times 4$ gridworld shown below.\n",
    "\n",
    "<img src = \"RL Example 4_1.png\">\n",
    "\n",
    "The nonterminal states are $S = \\{1, 2, \\cdots, 14\\}$. There are four actions possible in each state, $\\mathcal{A} = \\{up, down, right, left\\}$, which deterministically cause the corresponding state transitions, except that actions that would take the agent off the grid in fact leave the state unchanged. Thus, for instance, $p(6,-1|5,right) = 1$, $p(7,-1|7,right) = 1$, and $p(10,r|5,right) = 0$ for all $r \\in \\mathcal{R}$. This is an undiscounted, episodic task. The reward is -1 on all transitions until the terminal state is reached. The terminal state is shaded in the figure (although it is shown in two places, it is formally one state). The expected reward function is thus $r(s, a, s') = -1$ for all states $s$, $s'$ and actions $a$. Suppose the agent follows the equiprobable random policy (all actions equally likely). The left side of Figure 4.1 shows the sequence of value functions $\\{v_k\\}$ computed by iterative policy evaluation. The final estimate is in fact $v_\\pi$, which in this case gives for each state the negation of the expected number of steps from that state until termination.\n",
    "\n",
    "<img src = \"RL Figure 4_1.png\">\n",
    "\n",
    "**Figure 4.1:**\n",
    "Convergence of iterative policy evaluation on a small gridworld. The left column is the sequence of approximations of the state-value function for the random policy (all actions equally likely). The right column is the sequence of greedy policies corresponding to the value function estimates (arrows are shown for all actions achieving the maximum, and the numbers shown are rounded to two significant digits). The last policy is guaranteed only to be an improvement over the random policy, but in this case it, and all policies after the third iteration, are optimal.\\\n",
    "小网格世界迭代策略评估的收敛性。左列是随机策略(所有动作都是等可能的)的状态值函数的近似序列。右列是与值函数估计相对应的贪婪策略序列(箭头显示所有达到最大值的操作，所显示的数字四舍六入到两个有效数字)。最后一个策略被保证只是对随机策略的改进，但在这种情况下，它和第三次迭代后的所有策略都是最优策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": "4",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
