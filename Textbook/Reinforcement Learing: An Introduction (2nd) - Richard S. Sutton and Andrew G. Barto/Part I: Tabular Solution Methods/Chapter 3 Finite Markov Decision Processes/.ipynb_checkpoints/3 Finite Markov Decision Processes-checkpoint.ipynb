{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Finite Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we introduce the formal problem of finite Markov decision processes, or finite MDPs, which we try to solve in the rest of the book. This problem involves evaluative feedback, as in bandits, but also an associative aspect—choosing different actions in different situations. MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. Thus MDPs involve delayed reward and the need to tradeoff immediate and delayed reward. Whereas in bandit problems we estimated the value $q_*(a)$ of each action $a$, in MDPs we estimate the value $q_*(s,a)$ of each action $a$ in each state $s$, or we estimate the value $v_*(s)$ of each state given optimal action selections. These state-dependent quantities are essential to accurately assigning credit for long-term consequences to individual action selections.\\\n",
    "在本章中，我们将介绍有限马尔可夫决策过程（finite Markov decision processes，简称MDPs）的形式问题，我们将在本书的其余部分尝试解决这个问题。这个问题涉及到评价反馈，就像在老虎机中一样，但也涉及到associative方面——在不同的情况下选择不同的行动。MDPs是序贯决策的经典形式，其中行动不仅影响即时的reward，还影响后续的情况或状态，并通过这些未来的reward。因此，MDPs涉及延迟reward以及权衡即时和延迟奖励的需要。在bandit问题中，我们估计了每个动作$a$的value $q_*(a)$，而在MDPs中，我们估计了每个动作$a$在每个状态$s$下的value $q_*(s,a)$，或者在给定最优动作选择的情况下估计了每个状态$v_*(s)$。这些依赖于状态的数量对于准确地为个人行为选择的长期结果分配信用至关重要。\n",
    "\n",
    "MDPs are a mathematically idealized form of the reinforcement learning problem for which precise theoretical statements can be made. We introduce key elements of the problem’s mathematical structure, such as returns, value functions, and Bellman equations. We try to convey the wide range of applications that can be formulated as finite MDPs. As in all of artificial intelligence, there is a tension between breadth of applicability and mathematical tractability. In this chapter we introduce this tension and discuss some of the trade-offs and challenges that it implies. Some ways in which reinforcement learning can be taken beyond MDPs are treated in Chapter 17.\\\n",
    "MDPs是一种数学上理想化的强化学习问题，可以对其进行精确的理论陈述。我们将介绍问题的数学结构的关键元素，如返回值、值函数和贝尔曼方程。我们试图传达广泛的应用，可以制定为有限的MDPs。就像在所有的人工智能中一样，在适用性的广度和数学上的可处理性之间存在着一种张力。在本章中，我们将介绍这种张力，并讨论它所隐含的一些权衡和挑战。强化学习可以在MDPs之外采用的一些方法在第17章中讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent–Environment Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals and Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, the purpose or goal of the agent is formalized in terms of a special signal, called the **reward**, passing from the environment to the agent. At each time step, the reward is a simple number, $R_t \\in \\mathbb{R}$. Informally, the agent’s goal is to maximize the total amount of reward it receives. This means maximizing not immediate reward, but cumulative reward in the long run. We can clearly state this informal idea as the reward hypothesis:\\\n",
    "在强化学习中，agent的目的或目标被形式化为一种从环境传递给agent的特殊信号，称为**reward**。在每个time step，reward是一个简单的数字，$R_t \\in \\mathbb{R}$。非正式地说，agent的目标是使其获得的总reward最大化。这意味着最大化的不是即时reward，而是长期累积reward。我们可以将这个非正式的观点清晰地表述为reward假说：\n",
    "\n",
    "> That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\\\n",
    "我们所说的目标和目的都可以很好地理解为对接收到的标量信号（称为reward）的累积和的期望值的最大化。\n",
    "\n",
    "The use of a reward signal to formalize the idea of a goal is one of the most distinctive features of reinforcement learning.\\\n",
    "使用reward信号来形式化目标的概念是强化学习最显著的特征之一。\n",
    "\n",
    "___\n",
    "**<font color = blue>Definition</font> Reward**\\\n",
    "At each time step, the **reward** is a simple number, $R_t \\in \\mathbb{R}$.\\\n",
    "在每个time step，**reward**是一个简单的数字，$R_t \\in \\mathbb{R}$。\n",
    "___\n",
    "___\n",
    "**<font color = blue>Definition</font> Goal (目标)**\\\n",
    "Informally, the agent’s **goal** is to maximize the total amount of reward it receives.\\\n",
    "非正式地说，agent的**目标（goal）**是使其获得的总reward最大化。\n",
    "___\n",
    "\n",
    "Although formulating goals in terms of reward signals might at first appear limiting, in practice it has proved to be flexible and widely applicable. The best way to see this is to consider examples of how it has been, or could be, used.\\\n",
    "尽管从reward信号的角度制定目标一开始可能显得有限，但在实践中，它被证明是灵活和广泛适用的。要了解这一点，最好的方法是考虑它是如何被使用的，或者可能被使用的例子。\n",
    "\n",
    "**<font color = green>Example</font> Make a robot learn to walk**\\\n",
    "For example, to make a robot learn to walk, researchers have provided reward on each time step proportional to the robot’s forward motion. In making a robot learn how to escape from a maze, the reward is often -1 for every time step that passes prior to escape; this encourages the agent to escape as quickly as possible. To make a robot learn to find and collect empty soda cans for recycling, one might give it a reward of zero most of the time, and then a reward of +1 for each can collected. One might also want to give the robot negative rewards when it bumps into things or when somebody yells at it.\\\n",
    "例如，为了让机器人学会走路，研究人员对机器人前进的每一步都给予相应的奖励。在让机器人学会如何逃离迷宫的过程中，每走一步就会得到-1的奖励;这鼓励代理尽可能快地逃离。为了让机器人学会寻找和收集空汽水罐进行回收利用，人们可以在大多数情况下给它零奖励，然后每收集一个空汽水罐就给予+1奖励。当机器人撞到东西或者有人对它大喊大叫时，你也可以给它一些负面奖励。\n",
    "\n",
    "**<font color = green>Example</font> Make a robot learn to play checkers or chess**\\\n",
    "For an agent to learn to play checkers or chess, the natural rewards are +1 for winning, -1 for losing, and 0 for drawing and for all nonterminal positions.\\\n",
    "对于一个学习玩跳棋或象棋的代理来说，自然的奖励是+1赢，-1输，0和棋以及所有非终止的位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returns and Episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return\n",
    "\n",
    "So far we have discussed the objective of learning informally. We have said that the agent’s goal is to maximize the cumulative reward it receives in the long run. How might this be defined formally? If the sequence of rewards received after time step $t$ is denoted $R_{t+1}, R_{t+2}, R_{t+3}, \\cdots$, then what precise aspect of this sequence do we wish to maximize? In general, we seek to maximize the **expected return**, where the return, denoted $G_t$, is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards:\\\n",
    "到目前为止，我们已经非正式地讨论了学习的目的。我们已经说过，agent的目标是最大化其在长期内获得的累积reward。这应该如何正式定义？如果在时间步长$t$之后收到的reward序列表示为$R_{t+1}, R_{t+2}, R_{t+3}, \\cdots$，那么我们希望最大化这个序列的哪个精确方面呢？一般来说，我们寻求最大化**expected return**，其中return，记为$G_t$，被定义为reward序列的某个特定函数。在最简单的情况下，return是reward的总和:\n",
    "$$\n",
    "G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\cdots + R_{T} = \\sum_{k=t+1}^T R_k\n",
    "\\tag{3.7}\n",
    "\\label{Eq 3.7}\n",
    "$$\n",
    "where $T$ is a final time step.\\\n",
    "其中，$T$是最后的time step。\n",
    "\n",
    "___\n",
    "**<font color = blue>Definition</font> Return**\\\n",
    "**Return**, denoted $G_t$, is defined as some specific function of the reward sequence.\\\n",
    "**Return**，记为$G_t$，被定义为reward序列的某个特定函数。\n",
    "___\n",
    "\n",
    "### Episodic Task\n",
    "\n",
    "This approach makes sense in applications in which there is a natural notion of final time step, that is, when the agent–environment interaction breaks naturally into subsequences, which we call **episodes**, such as plays of a game, trips through a maze, or any sort of repeated interaction. Each episode ends in a special state called the **terminal state**, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. Even if you think of episodes as ending in different ways, such as winning and losing a game, the next episode begins independently of how the previous one ended. Thus the episodes can all be considered to end in the same terminal state, with different rewards for the different outcomes.\\\n",
    "这种方法适用于具有最终time step的自然概念的应用，也就是说，当agent-环境交互作用自然地分解成我们称之为**episode**的子序列时，如玩游戏、穿越迷宫或任何类型的重复交互。每一episode都以一种称为**terminal状态**的特殊状态结束，然后重置到一个标准开始状态或从一个标准开始状态分布的样本结束。即使你认为episode以不同的方式结束，如游戏的胜利和失败，下一episode的开始与前一episode的结束是独立的。因此，这些episode都可以被认为以相同的terminal状态结束，不同的结果会得到不同的reward。\n",
    "\n",
    "Tasks with episodes of this kind are called **episodic tasks**. In episodic tasks we sometimes need to distinguish the set of all nonterminal states, denoted $\\mathcal{S}$, from the set of all states plus the terminal state, denoted $\\mathcal{S}^+$. The time of termination, $T$, is a random variable that normally varies from episode to episode.\\\n",
    "有这种episode的任务叫做**episodic任务**。在episodic任务中，我们有时需要区分所有nonterminal状态的集合（记为$\\mathcal{S}$）和所有状态加上terminal状态的集合（记为$\\mathcal{S}^+$）。termination时间$T$是一个随机变量，通常因episode而异。\n",
    "\n",
    "### Continuing Task\n",
    "\n",
    "On the other hand, in many cases the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. For example, this would be the natural way to formulate an on-going process-control task, or an application to a robot with a long life span. We call these **continuing tasks**. The return formulation $\\ref{Eq 3.7}$ is problematic for continuing tasks because the final time step would be $T = \\infty$, and the return, which is what we are trying to maximize, could itself easily be infinite. (For example, suppose the agent receives a reward of +1 at each time step.) Thus, in this book we usually use a definition of return that is slightly more complex conceptually but much simpler mathematically.\\\n",
    "另一方面，在许多情况下，agent-环境的交互作用不会自然地分解成可识别的episode，而是无限地持续下去。例如，这将是一种自然的方式来制定一个正在进行的过程控制任务，或一个具有较长寿命机器人的一个应用程序。我们称这些为**continuing任务**。return公式$\\ref{Eq 3.7}$对于continuing任务来说是有问题的，因为最后的时间步是$T = \\infty$，而我们试图最大化的return本身很容易是无限的。（例如，假设agent在每个时间步长获得+1的奖励。）因此，在这本书中，我们通常使用一个概念上稍微复杂一点，但数学上要简单得多的return定义。\n",
    "\n",
    "#### Discounting\n",
    "\n",
    "The additional concept that we need is that of **discounting**. According to this approach, the agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses $A_t$ to maximize the expected **discounted return**:\\\n",
    "我们需要的另一个概念是**discounting**。根据这种方法，agent试图选择行动，使其在未来获得的discounted rewards的总和最大化。特别是，它选择$A_t$来最大化expected **discounted return**:\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t \n",
    "= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots\n",
    "&= \\sum_{k=0}^\\infty \\gamma^{k} R_{t+k+1} \\\\\n",
    "&= \\sum_{k=1}^\\infty \\gamma^{k-1} R_{t+k}\n",
    "\\end{align}\n",
    "\\tag{3.8}\n",
    "\\label{Eq 3.8}\n",
    "$$\n",
    "where $\\gamma$ is a parameter, $0 \\leq \\gamma \\leq 1$, called the **discount rate**.\\\n",
    "其中$\\gamma$是一个参数，$0 \\leq \\gamma \\leq 1$称为**discount rate**。\n",
    "\n",
    "The discount rate determines the present value of future rewards: a reward received $k$ time steps in the future is worth only $\\gamma^{k-1}$ times what it would be worth if it were received immediately.\\\n",
    "discount rate决定了未来reward的现值：在未来$k$ time steps收到的reward价值仅为$\\gamma^{k-1}$乘以立即收到的reward价值。\n",
    "\n",
    "___\n",
    "**<font color = purple>Remark</font> Discount Rate**\n",
    "* That is, at time step $k$ in the future, the reward is $\\gamma^{k-1} R_{t+k}$.\\\n",
    "即在未来time step $k$时，reward为$\\gamma^{k-1} R_{t+k}$。\n",
    "* If $\\gamma < 1$, the infinite sum in $\\ref{Eq 3.8}$ has a finite value as long as the reward sequence ${R_k}$ is bounded.\\\n",
    "如果$\\gamma < 1$，只要reward序列${R_k}$有界，$\\ref{Eq 3.8}$中的无限和就有一个有限值。\n",
    "* If $\\gamma = 0$, the agent is “myopic [maɪˈɒpɪk]” in being concerned only with maximizing immediate rewards: its objective in this case is to learn how to choose $A_t$ so as to maximize only $R_{t+1}$.\\\n",
    "如果$\\gamma = 0$，那么agent只关心即时reward最大化是“短视的（myopic）”：在这种情况下，它的目标是学习如何选择$A_t$，从而只使$R_{t+1}$最大化。\n",
    "* As $\\gamma$ approaches 1, the return objective takes future rewards into account more strongly; the agent becomes more farsighted.\\\n",
    "当$\\gamma$接近1时，return目标更强烈地考虑未来的回报；agent变得更有远见了。\n",
    "___\n",
    "\n",
    "If each of the agent’s actions happened to influence only the immediate reward, not future rewards as well, then a myopic agent could maximize $\\ref{Eq 3.8}$ by separately maximizing each immediate reward. But in general, acting to maximize immediate reward can reduce access to future rewards so that the return is reduced.\\\n",
    "如果agent的每一个行为都只影响眼前的reward，而不影响未来的reward，那么一个短视的agent可以通过分别最大化每个即时reward来最大化\\ref{Eq 3.8}。但一般来说，最大化即时reward会减少获得未来reward的机会，从而降低return。\n",
    "\n",
    "#### Properties of Returns at Successive Time Steps\n",
    "\n",
    "Returns at successive time steps are related to each other in a way that is important for the theory and algorithms of reinforcement learning:\\\n",
    "连续的time steps上的return是相互关联的，这对强化学习的理论和算法很重要:\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t \n",
    "&= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + \\cdots \\\\\n",
    "&= R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + \\cdots) \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1}\n",
    "\\end{align}\n",
    "\\tag{3.9}\n",
    "\\label{Eq 3.9}\n",
    "$$\n",
    "Note that this works for all time steps $t < T$, even if termination occurs at $t + 1$, if we define $G_T = 0$. This often makes it easy to compute returns from reward sequences.\\\n",
    "注意，如果我们定义$G_T = 0$，那么这适用于所有time steps $t < T$，即使termination发生在$t + 1$。这通常会使计算reward序列的return变得容易。\n",
    "\n",
    "Note that although the return $\\ref{Eq 3.8}$ is a sum of an infinite number of terms, it is still finite if the reward is nonzero and constant—if $\\gamma < 1$.\\\n",
    "请注意，虽然return$\\ref{Eq 3.8}$是一个无限项的和，但如果reward是非零的且是常量-如果$\\gamma < 1$，它仍然是有限的。\\\n",
    "For example, if the reward is a constant +1, then the return is\\\n",
    "例如，如果奖励是常量+1，那么return就是\n",
    "$$\n",
    "G_t \n",
    "= \\sum_{k=0}^\\infty \\gamma^{k} \n",
    "= \\frac{1}{1 - \\gamma}\n",
    "\\tag{3.10}\n",
    "\\label{Eq 3.10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified Notation for Episodic and Continuing Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies and Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function\n",
    "\n",
    "Almost all reinforcement learning algorithms involve estimating **value functions**—functions of states (or of state–action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of “how good” here is defined in terms of future rewards that can be expected, or, to be precise, in terms of expected return. Of course the rewards the agent can expect to receive in the future depend on what actions it will take. Accordingly, value functions are defined with respect to particular ways of acting, called policies.\\\n",
    "几乎所有的强化学习算法都涉及估计**值函数（value functions）**——状态（或状态-动作对）的函数，它估计agent处于给定状态有多好（或在给定状态下执行给定动作有多好）。这里“有多好”的概念是根据可以预期的未来reward来定义的，或者，准确地说，根据expected return来定义的。当然，agent期望在未来获得的reward取决于它将采取什么行动。因此，值函数是根据特定的行为方式（称为policy）定义的。\n",
    "\n",
    "___\n",
    "**<font color = blue>Definition</font> Value Function (值函数)**\\\n",
    "**Value functions** are functions of states (or of state–action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state).\\\n",
    "**值函数（value function）**是状态（或状态-动作对）的函数，估计agent处于给定状态有多好（或在给定状态下执行给定动作有多好）。\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "\n",
    "Formally, a **policy** is a mapping from states to probabilities of selecting each possible action. If the agent is following policy $\\pi$ at time $t$, then $\\pi(a|s)$ is the probability that $A_t = a$ if $S_t = s$. Like $p$, $\\pi$ is an ordinary function; the “|” in the middle of $\\pi(a|s)$ merely reminds that it defines a probability distribution over $a \\in \\mathcal{A}(s)$ for each $s \\in \\mathcal{S}$. Reinforcement learning methods specify how the agent’s policy is changed as a result of its experience.\\\n",
    "形式上，一个**策略（policy）**是从状态到选择每个可能动作的概率的映射。如果agent在时间$t$遵循策略$\\pi$，则$\\pi(a|s)$为如果$S_t = s$，则$A_t = a$的概率。像$p$一样，$\\pi$是一个普通函数；$\\pi(a|s)$中间的“|”只是提醒我们，它为每个$s \\in \\mathcal{S}$定义了在$a \\in \\mathcal{A}(s)$上的概率分布。强化学习方法指定agent的策略如何因其经验而改变。\n",
    "\n",
    "___\n",
    "**<font color = blue>Definition</font> Policy (策略)**\\\n",
    "Formally, a **policy** is a mapping from states to probabilities of selecting each possible action.\\\n",
    "形式上，一个**策略（policy）**是从状态到选择每个可能动作的概率的一个映射。\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-Value Function\n",
    "The value function of a state $s$ under a policy $\\pi$, denoted $v_\\pi(s)$, is the expected return when starting in $s$ and following $\\pi$ thereafter. For MDPs, we can define $v_\\pi$ formally by\\\n",
    "在策略$\\pi$下状态$s$的值函数，记为$v_\\pi(s)$，是从状态$s$开始，其后遵循策略$\\pi$的期望return。对于MDPs，我们可以正式定义$v_\\pi$\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{\\pi}(s)\n",
    "&= \\mathbb{E}_\\pi[G_t|S_t=s] \\\\\n",
    "&= \\mathbb{E}_\\pi[\\sum_{k=0}^\\infty \\gamma^{k} R_{t+k+1}|S_t=s], \\qquad \\text{for all } s \\in \\mathcal{S} \\\\\n",
    "&= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1}|S_t=s]\n",
    "\\end{align}\n",
    "\\tag{3.12}\n",
    "\\label{Eq 3.12}\n",
    "$$\n",
    "where $\\mathbb{E}_\\pi[\\cdot]$ denotes the expected value of a random variable given that the agent follows policy $\\pi$, and $t$ is any time step.\\\n",
    "其中$\\mathbb{E}_\\pi[\\cdot]$表示假设agent遵循策略$\\pi$时随机变量的期望值，$t$为任意时间步长。\n",
    "\n",
    "Note that the value of the terminal state, if any, is always zero. We call the function $v_\\pi$ the **state-value function** for policy $\\pi$.\\\n",
    "请注意，如果有terminal状态的值，则其始终为零。我们将函数$v_\\pi$称为策略$\\pi$的**状态值函数（state-value function）**。\n",
    "\n",
    "**<font color = purple>Remark</font> State-Value Function**\n",
    "* The state-value function $v_{\\pi}(s)$ of an MDP is the expected return starting from state $s$, and then following policy $\\pi$.\\\n",
    "MDP的state-value函数$v_{\\pi}(s)$是从状态$s$开始，然后遵循策略$\\pi$的期望return。\n",
    "* Note that $S_t$ and $R_t$ etc are random variables. Hence the return $G_t$ is also a random variable. We thus consider the expected return $v_{\\pi}(s)$.\\\n",
    "注意，$S_t$和$R_t$等是随机变量。因此，return $G_t$也是一个随机变量。因此，我们考虑期望return $v_{\\pi}(s)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-Value Function (Q-function)\n",
    "\n",
    "Similarly, we define the value of taking action $a$ in state $s$ under a policy $\\pi$, denoted $q_\\pi(s, a)$, as the expected return starting from state $s$, taking the action $a$, and thereafter following policy $\\pi$:\\\n",
    "同样，我们定义在策略$\\pi$下的状态$s$中执行动作$a$的值，记为$q_\\pi(s, a)$，作为从状态$s$开始，执行动作$a$，然后遵循策略$\\pi$的期望return：\n",
    "$$\n",
    "\\begin{align}\n",
    "q_{\\pi}(s, a) \n",
    "&= \\mathbb{E}_{\\pi}[G_t|S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[\\sum_{k=0}^\\infty \\gamma^{k} R_{t+k+1}|S_t=s, A_t=a]\\\\\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1}|S_t=s, A_t=a]\n",
    "\\end{align}\n",
    "\\tag{3.13}\n",
    "\\label{Eq 3.13}\n",
    "$$\n",
    "We call $q_\\pi$ the **action-value function** for policy $\\pi$. Often called the **Q-function**.\\\n",
    "我们称$q_\\pi$为策略$\\pi$的**动作值函数（action-value function）**。通常称为**Q函数**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bellman Equation\n",
    "\n",
    "A fundamental property of value functions used throughout reinforcement learning and dynamic programming is that they satisfy recursive relationships similar to that which we have already established for the return (3.9). For any policy $\\pi$ and any state $s$, the following consistency condition holds between the value of $s$ and the value of its possible successor states:\\\n",
    "在强化学习和动态规划中使用的value函数的一个基本性质是，它们满足递归关系，类似于我们已经为return(3.9)建立的关系。对于任何策略$\\pi$和任何状态$s$， $s$的值与其可能的后续状态的值之间存在以下一致性条件：\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) \n",
    "&= \\mathbb{E}_\\pi[G_t|S_t=s] \\\\\n",
    "&= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1}|S_t=s], \\qquad \\text{by (3.9)} \\\\\n",
    "&= \\sum_{a} \\pi(a|s) \\sum_{s'} \\sum_{r} p(s',r|s,a) [r + \\gamma \\mathbb{E}_\\pi[G_t|S_t=s']] \\\\\n",
    "&= \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_\\pi(s')], \\qquad \\text{for all } s \\in \\mathcal{S}\n",
    "\\end{align}\n",
    "\\tag{3.14}\n",
    "\\label{Eq 3.14}\n",
    "$$\n",
    "where it is implicit that the actions, $a$, are taken from the set $\\mathcal{A}(s)$, that the next states, $s'$, are taken from the set $\\mathcal{S}$ (or from $\\mathcal{S}^+$ in the case of an episodic problem), and that the rewards, $r$, are taken from the set $\\mathcal{R}$. \n",
    "Note also how in the last equation we have merged the two sums, one over all the values of $s'$ and the other over all the values of $r$, into one sum over all the possible values of both. We use this kind of merged sum often to simplify formulas. Note how the final expression can be read easily as an expected value. It is really a sum over all values of the three variables, $a$, $s'$, and $r$. For each triple, we compute its probability, $\\pi(a|s) p(s',r|s,a)$, weight the quantity in brackets by that probability, then sum over all possibilities to get an expected value.\\\n",
    "其中隐含的意思是，动作$a$来自集合$\\mathcal{A}(s)$，下一个状态$s'$来自集合$\\mathcal{S}$（或在episodic问题的情况下来自$\\mathcal{S}^+$），reward $r$来自集合$\\mathcal{R}$。\n",
    "请注意，在上一个等式中，我们是如何将两个和合并的，一个除以$s'$的所有值，另一个除以$r$的所有值，合并成一个除以两者的所有可能值的和。我们经常用这种合并和来简化公式。请注意，最终表达式可以很容易地读取为一个期望值。它实际上是三个变量，$a$， $s'$和$r$的所有值的总和。对于每个三元组，我们计算它的概率$\\pi(a|s) p(s',r|s,a)$，用该概率对括号中的数量进行加权，然后对所有可能的情况进行求和以得到一个期望值。\n",
    "\n",
    "Equation $\\ref{Eq 3.14}$ is the **Bellman equation** for $v_\\pi$. It expresses a relationship between the value of a state and the values of its successor states. Think of looking ahead from a state to its possible successor states, as suggested by the diagram to the right. Each open circle represents a state and each solid circle represents a state–action pair. Starting from state $s$, the root node at the top, the agent could take any of some set of actions—three are shown in the diagram—based on its policy $\\pi$. From each of these, the environment could respond with one of several next states, $s'$ (two are shown in the figure), along with a reward, $r$, depending on its dynamics given by the function $p$. The Bellman equation $\\ref{Eq 3.14}$ averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way.\\\n",
    "方程$\\ref{Eq 3.14}$是$v_\\pi$的**Bellman方程**。它表示一个状态的值与其后续状态的值之间的关系。如右图所示，从一个状态展望它可能的继承状态。每个开放的圆代表一种状态，每个实心圆代表一种状态-动作对。从状态$s$(顶部的根节点)开始，agent可以采取某些操作集中的任何一个——图中显示了三个基于其策略$\\pi$的操作。从每一个这些，环境可以响应的下几个状态之一，$s'$(两个显示在图中)，以及一个reward，$r$，取决于它的动态函数$p$。Bellman等式$\\ref{Eq 3.14}$对所有的可能性进行平均，并根据其发生的概率对每个可能性进行加权。它规定了开始状态的值必须等于期望的下一个状态的(折现后的)值，加上过程中期望的奖励。\n",
    "\n",
    "The value function $v_\\pi$ is the unique solution to its Bellman equation. We show in subsequent chapters how this Bellman equation forms the basis of a number of ways to compute, approximate, and learn $v_\\pi$. We call diagrams like that above backup diagrams because they diagram relationships that form the basis of the update or backup operations that are at the heart of reinforcement learning methods. These operations transfer value information back to a state (or a state–action pair) from its successor states (or state–action pairs). We use backup diagrams throughout the book to provide graphical summaries of the algorithms we discuss. (Note that, unlike transition graphs, the state nodes of backup diagrams do not necessarily represent distinct states; for example, a state might be its own successor.)\\\n",
    "value函数$v_\\pi$是Bellman方程的唯一解。我们将在后续的章节中展示这个Bellman方程是如何形成计算、近似和学习$v_\\pi$的多种方法的基础的。我们将上面的图称为备份图，因为它们描绘了构成强化学习方法核心的更新或备份操作基础的关系。这些操作将值信息从其后续状态(或状态-操作对)传输回状态(或状态-操作对)。我们在整本书中使用备份图来提供我们讨论的算法的图形摘要。(注意，不像转换图，备份图的状态节点不一定代表不同的状态；例如，一个状态可能是它自己的继承者。)\n",
    "\n",
    "**<font color = purple>Remark</font> The Bellman Equation**\n",
    "* The state-value of $s$ is the expected action-value:\\\n",
    "$s$的状态值是预期的行为值。\n",
    "$$\n",
    "v_{\\pi}(s) = \\sum_{a} \\pi(a|s) q_{\\pi}(s,a)\n",
    "$$\n",
    "\n",
    "\n",
    "**<font color = green>Example</font> The Bellman Equation**\n",
    "<img src = \"The Bellman Equation Example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policies and Optimal Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving a reinforcement learning task means, roughly, finding a policy that achieves a lot of reward over the long run. For finite MDPs, we can precisely define an optimal policy in the following way. Value functions define a partial ordering over policies.\\\n",
    "解决一个强化学习任务大致意味着，找到一个在长期内获得大量reward的策略。对于有限的MDP，我们可以用以下方法精确地定义最优策略。值函数定义策略的部分排序。\n",
    "\n",
    "A policy $\\pi$ is defined to be better than or equal to a policy $\\pi'$ if its expected return is greater than or equal to that of $\\pi'$ for all states. In other words, $\\pi \\geq \\pi'$ if and only if $v_\\pi(s) \\geq v_{\\pi'}(s)$ for all $s \\in \\mathcal{S}$.\\\n",
    "如果一个策略$\\pi$在所有状态的预期return大于或等于$\\pi'$，则该策略$\\pi$被定义为优于或等于策略$\\pi'$。换句话说，对于所有$s \\in \\mathcal{S}$，$\\pi \\geq \\pi'$当且仅当$v_\\pi(s) \\geq v_{\\pi'}(s)$。\n",
    "\n",
    "___\n",
    "**<font color = blue>Definition</font> Optimal Policy (最优策略)**\\\n",
    "There is always at least one policy that is better than or equal to all other policies. This is an **optimal policy**.\\\n",
    "总有至少一个策略优于或等于所有其他策略。这就是**最优策略（optimal policy）**。\n",
    "___\n",
    "\n",
    "___\n",
    "**<font color = blue>Definition</font> Optimal State-Value Function (最优状态值函数)**\\\n",
    "Although there may be more than one, we denote all the optimal policies by $\\pi_*$. They share the same state-value function, called the **optimal state-value function**, denoted $v_*$, and defined as\\\n",
    "虽然可能有多个最优策略，但我们用$\\pi_*$表示所有最优策略。它们共享相同的状态值函数，称为**最优状态值函数（optimal state-value function）**，记为$v_*$，定义为\n",
    "$$\n",
    "v_*(s) = \\underset{\\pi}{\\max} v_\\pi(s)\n",
    "\\tag{3.15}\n",
    "$$\n",
    "for all $s \\in \\mathcal{S}$.\n",
    "对于所有的$s \\in \\mathcal{S}$。\n",
    "___\n",
    "\n",
    "___\n",
    "**<font color = blue>Definition</font> Optimal Action-Value Function (最优动作值函数)**\\\n",
    "Optimal policies also share the same **optimal action-value function**, denoted $q_*$, and defined as\\\n",
    "最优策略也具有相同的**最优动作值函数（optimal action-value function）**，记为$q_*$，定义为\n",
    "$$\n",
    "q_*(s,a) = \\underset{\\pi}{\\max} q_\\pi(s,a)\n",
    "\\tag{3.16}\n",
    "$$\n",
    "for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$.\\\n",
    "对于所有的$s \\in \\mathcal{S}$以及$a \\in \\mathcal{A}$。\n",
    "___\n",
    "\n",
    "For the state–action pair $(s,a)$, this function gives the expected return for taking action $a$ in state $s$ and thereafter following an optimal policy. Thus, we can write $q_*$ in terms of $v_*$ as follows:\\\n",
    "对于状态-动作组合$(s,a)$，该函数给出了在状态$s$中采取行为$a$并遵循最优策略的预期return。因此，我们可以将$q_*$写成$v_*$，如下所示:\n",
    "$$\n",
    "q_*(s,a) = \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma v_*(S_{t+1}) | S_t=s, A_t=a]\n",
    "\\tag{3.17}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": "3",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
