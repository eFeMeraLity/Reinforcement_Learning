{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by considering Monte Carlo methods for learning the state-value function for a given policy. Recall that the value of a state is the expected return—expected cumulative future discounted reward—starting from that state. An obvious way to estimate it from experience, then, is simply to average the returns observed after visits to that state. As more returns are observed, the average should converge to the expected value. This idea underlies all Monte Carlo methods.\\\n",
    "我们首先考虑Monte Carlo方法来学习给定策略的状态值函数。回想一下，一种状态的值是预期return——预期的累积未来折现reward——从该状态开始。因此，根据经验来估算收益的一个显而易见的方法，就是简单地将访问该状态后观察到的收益取平均值。随着更多的回报被观察到，平均值应该收敛到期望值。这个想法是所有Monte Carlo方法的基础。\n",
    "\n",
    "In particular, suppose we wish to estimate $v_\\pi(s)$, the value of a state $s$ under policy $\\pi$, given a set of episodes obtained by following $\\pi$ and passing through $s$. Each occurrence of state $s$ in an episode is called a __visit__ to $s$. Of course, $s$ may be visited multiple times in the same episode; let us call the first time it is visited in an episode the __first visit__ to $s$. The __first-visit MC method__ estimates $v_\\pi(s)$ as the average of the returns following first visits to $s$, whereas the __every-visit MC method__ averages the returns following all visits to $s$. These two Monte Carlo (MC) methods are very similar but have slightly different theoretical properties. First-visit MC has been most widely studied, dating back to the 1940s, and is the one we focus on in this chapter. Every-visit MC extends more naturally to function approximation and eligibility traces, as discussed in Chapters 9 and 12. First-visit MC is shown in procedural form in the box. Every-visit MC would be the same except without the check for Sthaving occurred earlier in the episode.\\\n",
    "特别地，假设我们想要估计$v_\\pi(s)$，即策略$\\pi$下的状态$s$的值，给定一个通过遵循$\\pi$并通过$s$获得的episode集合。每次在一个episode中出现的状态$s$被称为__访问__$s$。当然，$s$在同一个episode中可能会被访问多次；让我们把它在一个episode中的第一次访问称为__首次访问__$s$。__首次访问MC方法__估计$v_\\pi(s)$为首次访问$s$的平均return，而__每次访问MC方法__估计所有访问$s$的平均return。这两种Monte Carlo方法非常相似，但理论性质略有不同。首次访问MC已经被广泛研究，可以追溯到20世纪40年代，这是我们在本章重点讨论的。如第9章和第12章所讨论的，每次访问MC更自然地扩展到函数近似和资格追踪。第一次访问的MC以程序形式显示在方框中。每次访问MC都是一样的，除了没有检查在事件早期发生的事情。\n",
    "\n",
    "__<font color = red> Algorithm </font> First-visit MC prediction, for estimating $V \\approx v_\\pi$__\n",
    "* __Input:__ a policy $\\pi$ to be evaluated\n",
    "* __Initialize:__\n",
    "    * $V(s) \\in \\mathbb{R}$, arbitrarily, for all $s \\in \\mathcal{S}$\n",
    "    * $N(S_{t}) \\leftarrow$ an empty list, for all $s \\in \\mathcal{S}$\n",
    "* __Step:__\n",
    "    * Loop forever (for each episode):\n",
    "        * Generate an episode following $\\pi$: $S_0, R_1, S_1, R_2, S_2, R_3, \\cdots, S_{(T-1)}, R_T, S_T$\n",
    "        * $G \\leftarrow 0$\n",
    "        * Loop for each step of episode, $t = T-1, T-2, \\cdots, 0$:\n",
    "            * $G_t \\leftarrow \\gamma G_{t+1} + R_{t+1}$\n",
    "            * Unless $S_t$ appears in $S_0, \\cdots, S_{t-1}$:\n",
    "                * $N(S_{t}) \\leftarrow N(S_{t}) + 1$\n",
    "                * $\\displaystyle V(S_t) = V(S_{t+1}) + \\frac{1}{N(S_{t})} (G_t - V(S_{t+1}))$\n",
    " \n",
    "Both first-visit MC and every-visit MC converge to $v_\\pi(s)$ as the number of visits (or first visits) to $s$ goes to infinity. This is easy to see for the case of first-visit MC. In this case each return is an independent, identically distributed estimate of $v_\\pi(s)$ with finite variance. By the law of large numbers the sequence of averages of these estimates converges to their expected value. Each average is itself an unbiased estimate, and the standard deviation of its error falls as $1/\\sqrt{n}$, where $n$ is the number of returns averaged. Every-visit MC is less straightforward, but its estimates also converge quadratically to $v_\\pi(s)$ (Singh and Sutton, 1996).\\\n",
    "首次访问的MC和每次访问的MC都收敛到$v_\\pi(s)$，当访问$s$的次数（或首次访问）达到无穷大时。在首次访问MC的情况下，这是很容易看到的。在这种情况下，每个返回都是具有有限方差的$v_\\pi(s)$的独立、同分布估计。根据大数定律，这些估计的平均值序列收敛于它们的期望值。每个平均值本身就是一个无偏估计，其误差的标准差为$1/\\sqrt{n}$，其中$n$是收益的平均数目。每次访问MC不那么直接，但它的估计也二次收敛到$v_\\pi(s)$ (Singh and Sutton, 1996)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": "5",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
