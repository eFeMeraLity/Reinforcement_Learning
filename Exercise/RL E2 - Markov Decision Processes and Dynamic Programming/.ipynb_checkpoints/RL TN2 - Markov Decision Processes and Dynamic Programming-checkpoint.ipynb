{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tinkering notebook 2: Markov Decision Processes and Dynamic Programming\n",
    "\n",
    "In this notebook we will see how some of the content of Lecture 2 - Lecture 3 works in practice. \n",
    "\n",
    "We will start by a repetition of Markov Decision Processes (MDPs) and value functions. After this we will study two GridWorld examples (Example 4.1 in the textbook and FrozenLake). Here we will use dynamic programming to learn both value functions and optimal policies.\\\n",
    "我们将从重复马尔可夫决策过程(MDPs)和价值函数开始。在这之后，我们将学习两个GridWorld的例子(例子4.1在教科书和结冰湖)。这里我们将使用动态规划来学习值函数和最优策略。\n",
    "\n",
    "**Lecture 2 - Markov Decision Processes**\n",
    "* __Recommended Reading:__ Read Chapter 3.\n",
    "\n",
    "**Lecture 3 - Dynamic Programming**\n",
    "* __Recommended Reading:__ Read Chapter 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "* ### [1. Imports](#sec1)\n",
    "* ### [2. Markov Decision Processes and Value Functions](#sec2)\n",
    " * #### [2.1 The Bellman equations](#sec2_1)\n",
    " * #### [2.2 Example: Study or Facebook?](#sec2_2)\n",
    " * #### [2.3 *Analytical solution to the Bellman equation](#sec2_3)\n",
    "* ### [3. Helper functions](#sec3)\n",
    "* ### [4. The environments](#sec4)\n",
    " * #### [4.1 Example 4.1: GridWorld-v0](#sec4_1)\n",
    " * #### [4.2 The Frozen Lake](#sec4_2)\n",
    "* ### [5. MDPs and the Bellman equations](#sec5)\n",
    " * #### [5.1 Test your code on Example 4.1 (GridWorld)](#sec5_1)\n",
    "* ### [6. Policy Evaluation](#sec6)\n",
    " * #### [6.1 In place updates](#sec6_1)\n",
    "* ### [7. Policy Iteration](#sec7)\n",
    "* ### [8. Value iteration](#sec8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports <a id=\"sec1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages needed in this notebook are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "package = \"/Users/lmf/PycharmProjects/MSc_DS/Python/Package/gym-gridworld\"\n",
    "sys.path.append(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Packages needed for this notebook\n",
    "import gym\n",
    "import gym_gridworld\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Markov Decision Processes and Value Functions <a id=\"sec2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MDP consist of a state space $\\mathcal{S}$, an action space $\\mathcal{A}$, a reward set $\\mathcal{R}$ and a transition function $p(s', r | s, a)$. We define the return as\\\n",
    "MDP由一个状态空间 $\\mathcal{S}$、一个动作空间$\\mathcal{A}$、一个reward集$\\mathcal{R}$和一个转换函数$p(s', r | s, a)$组成。我们将return定义为\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n",
    "$$\n",
    "where $\\gamma$ is the discount rate.\\\n",
    "其中，$\\gamma$是贴现率。\n",
    "\n",
    "**State-value function:**\n",
    "$$\n",
    "v_{\\pi}(s) = \\mathbb{E}_{\\pi}[ G_t | S_t = s]\n",
    "$$\n",
    "The expected return starting from state $s \\in \\mathcal{S}$ and following policy $\\pi$.\\\n",
    "从状态$s \\in \\mathcal{S}$开始，然后遵循策略$\\pi$的expected return。\n",
    "\n",
    "**Action-value function ($Q$-value)**:\n",
    "$$\n",
    "q_{\\pi}(s) = \\mathbb{E}_{\\pi}[ G_t | S_t = s, A_t = a]\n",
    "$$\n",
    "The expected return starting from state $s \\in \\mathcal{S}$, then taking action $a \\in \\mathcal{A}$ and then follow policy $\\pi$.\\\n",
    "从状态$s \\in \\mathcal{S}$开始，然后采取动作$a \\in \\mathcal{A}$，然后遵循策略$\\pi$的expected return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 The Bellman equations <a id=\"sec2_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that $v_{\\pi}(s)$ is the solution to the Bellman equation\\\n",
    "我们已经知道$v_{\\pi}(s)$是Bellman方程的解\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) \n",
    "&= \\mathbb{E}_{\\pi}[ R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t = s] \\\\\n",
    "&= \\sum_{a} \\pi(a|s) \\sum_{r} \\sum_{s'} p(s',r|s,a) [r + \\gamma v_\\pi(s')] \\\\\n",
    "&= \\sum_{a} \\pi(a|s) q_{\\pi}(s,a)\n",
    "\\end{align}\n",
    "$$\n",
    "where\\\n",
    "其中\n",
    "$$\n",
    "\\begin{align}\n",
    "q_{\\pi}(s,a) \n",
    "&= \\mathbb{E}_{\\pi}[G_t|S_t=s, A_t=a] \\\\\n",
    "&= \\sum_{r}\\sum_{s'} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Example: Study or Facebook? <a id=\"sec2_2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lecture 2 we looked at the example MDP bellow.\n",
    "<img src = \"example.png\">\n",
    "\n",
    "It has four states, $\\mathcal{S} = \\{ K_0, K_1, K_2, Pass \\}$. The state $Pass$ is a terminal state, so the episode will end if this state is reached (alternatively, if we reach $Pass$ we will stay there forever and receive 0 future return).\\\n",
    "它有四种状态，$\\mathcal{S} = \\{ K_0, K_1, K_2, Pass \\}$。状态$Pass$是一种terminal状态，所以当到达该状态时，事件将结束（或者，如果到达$Pass$，我们将永远停留在那里，并在未来收到0返回）。\n",
    "\n",
    "In each non-terminal state we can choose between `Study` or `Facebook`, $\\mathcal{A} = \\{ Study, Facebook\\}$. The red nodes corresponds to actions, and the labels on the edges gives immediate rewards (green) and transition probabilities (black).\\\n",
    "在每个non-terminal状态中，我们可以选择`Study`或`Facebook`，$\\mathcal{A} = \\{ Study, Facebook\\}$。红色节点对应于行动，边缘上的标签提供即时reward(绿色)和转移概率(黑色)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task:** the Bellman equation\n",
    "\n",
    "**Task:** \n",
    "Assume that we use the policy $\\pi(s|a) = 0.5$ for all states and actions, and the discount factor $\\gamma = 0.9$.\\\n",
    "假设我们对所有状态和动作使用策略$\\pi(s|a) = 0.5$，discount因子$\\gamma = 0.9$。\n",
    "\n",
    "In Lecture 2 we saw that the state-value function (rounded to two decimals) is\\\n",
    "在第二讲中，我们看到state-value函数（四舍五入到两位小数）是\n",
    "$$\n",
    "v_{\\pi}(K_0) = 3.00, \\quad v_{\\pi}(K_1) = 4.78, \\quad v_{\\pi}(K_2) = 7.84.\n",
    "$$\n",
    "1. Verify that this satisfies the Bellman equation for all states! (At least approximately, since we have rounded everything to two decimals).\\\n",
    "对于所有状态，验证此满足Bellman方程(至少是近似的，因为我们把所有的都四舍五入到两个小数)。\n",
    "\n",
    "You can use the code-block bellow to carry out your computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v(K0) = 3.0 v(K1) = 4.78 v(K2) = 7.84\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "K0 = 3.00\n",
    "K1 = 4.78\n",
    "K2 = 7.84\n",
    "P = 0\n",
    "pi_FB = 0.5\n",
    "pi_S = 0.5\n",
    "\n",
    "vK0 = pi_FB * (0 + gamma*K0) + pi_S * (-1 + gamma*K1)\n",
    "vK1 = pi_FB * (0 + gamma*(0.5*K0 + 0.5*K1)) + pi_S * (-1 + gamma*K2)\n",
    "vK2 = pi_FB * (0 + gamma*(0.5*K1 + 0.5*K2)) + pi_S * (10 + gamma*P)\n",
    "print(\"v(K0) =\", round(vK0,2), \"v(K1) =\", round(vK1,2), \"v(K2) =\", round(vK2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 *Analytical solution to the Bellman equation <a id=\"sec2_3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Bellman equation can be written as\\\n",
    "请注意，Bellman方程可以写成\n",
    "$$\n",
    "v_{\\pi}(s) = \\mathbb{E}_{\\pi}[ R_{t+1} | S_{t} = s] + \\gamma\\sum_{a, s'} \\pi(a|s)p(s' | s, a) v_{\\pi}(s').\n",
    "$$\n",
    "So the value of state $s$ is the average immediate reward plus the discounted average value of the next state.\\\n",
    "所以状态$s$的价值是平均即时reward加上下一个状态的平均折现价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: From the state $K_1$**:\n",
    "    \n",
    "* In $s = K_1$ the expected immediate reward is $-0.5$, since we choose `Facebook` with probability 0.5 (reward 0) and `Study` with probability 0.5 (reward -1). \n",
    "* There is a 0.5 probability that the action is `Facebook`, and then there is a 50/50 chance of going either to $K_0$ or $K_1$. Hence the total probability of going to $K_1$ and $K_2$ are both 0.25 ($0.5 \\times 0.5$). There is also a 0.5 probability for the action `Study` which will move us to $K_2$. Finally there is 0 probability of reaching $Pass$. \n",
    "\n",
    "Summarizing this we get\n",
    "$$\n",
    "v_{\\pi}(K_1) = -0.5 + \\gamma [ 0.25 v_{\\pi}(K_0) + 0.25 v_{\\pi}(K_1) + 0.5 v_{\\pi}(K_2) + 0 v_{\\pi}(Pass)]\n",
    "$$\n",
    "If we define a vector with all state-values\n",
    "$$\n",
    "V_{\\pi} = \\begin{bmatrix} v_{\\pi}(K_0) \\\\ v_{\\pi}(K_1) \\\\ v_{\\pi}(K_2) \\\\ v_{\\pi}(Pass) \\end{bmatrix}\n",
    "$$\n",
    "we can write this as\n",
    "$$\n",
    "v_{\\pi}(K_1) = \\underbrace{-0.5}_{r_1} + \\gamma \\underbrace{\\begin{bmatrix} 0.25 & 0.25 & 0.5 & 0 \\end{bmatrix}}_{p_1} V_\\pi\n",
    "$$\n",
    "Note that the elements of $p_1$ are the probability of going from $K_1$ to each of the other states when we follow the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting it all together:**\n",
    "\n",
    "If we do the same for all states, we get an equation on the form \n",
    "$$\n",
    "V_{\\pi} = R + \\gamma P V_{\\pi}\n",
    "$$\n",
    "where $R$ is the vector of expected immediate rewards for each state, and $P \\in \\mathbb{R}^{4 \\times 4}$ where element $P_{i,j}$ is the probability of moving from the $i$th state to the $j$th state when we follow the policy.\n",
    "\n",
    "Assuming that $\\gamma < 1$ there is always a unique solution given by \n",
    "$$\n",
    "V_{\\pi} = (I - \\gamma P)^{-1} R.\n",
    "$$\n",
    "\n",
    "**Task**: Fill in the correct values of $R$ and $P$ in the code below, and see if you find the same solution as in the slides of Lecture 2. You can also play around with the discount rate, and/or try to compute $R$, $P$ and then $V_{\\pi}$ for another policy.\n",
    "\n",
    "**Note**: The state $Pass$ is a bit special, since it is a terminating state. Hence, when we reach \"Pass the exam\" we will not receive anymore rewards, and we will stay in this state forever (the probability of going to any other state is 0). Hence\n",
    "$$\n",
    "v_{\\pi}(Pass) = 0 + \\gamma \\begin{bmatrix} 0 & 0 & 0 & 1 \\end{bmatrix} V_{\\pi}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-cf2a25bb5627>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-cf2a25bb5627>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    R[0] = ? # For K_0\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "discount = 0.9 # gamma\n",
    "R = np.zeros((4,1))\n",
    "P = np.zeros((4,4))\n",
    "\n",
    "# Enter the expected immediate reward for each state\n",
    "# Those computed in the text above are already filled in.\n",
    "R[0] = ? # For K_0\n",
    "R[1] = -.5 # For K_1\n",
    "R[2] = ? # For K_2\n",
    "R[3] = 0 # For \"Pass exam\"\n",
    "\n",
    "# Enter the probabilities going from state i to state j\n",
    "P[0] = [?, ?, ?, ?] # for i=0 (K_0)\n",
    "P[1] = [0.25, 0.25, 0.5, 0] # for i=1 (K_1)\n",
    "P[2] = [?, ?, ?, ?] # for i=2 (K_2)\n",
    "P[3] = [0, 0, 0, 1] # for i=3 (Pass the exam) \n",
    "\n",
    "# Solve the Bellman equation \n",
    "V = np.linalg.inv(np.eye(4) - discount*P)@R # V = (I - discount*P)^-1 * R\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Helper functions <a id=\"sec3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now start to look at two GridWorld examples, but first we define some functions that will be useful in the rest of the notebook.\\\n",
    "现在我们将开始看两个GridWorld示例，但是首先我们定义一些函数，这些函数将在本笔记本的其余部分中有用。\n",
    "\n",
    "The class `RandomAgent` implements an agent with a policy $\\pi(a|s)$. The method `act` will take the state $s$ as input, and then sample an action according to the probabilities $\\pi(s|a)$.\\\n",
    "`RandomAgent`类实现了一个带有策略$\\pi(a|s)$的agent。方法`act`将状态$s$作为输入，然后根据概率$\\pi(s|a)$对动作进行抽样。\n",
    "\n",
    "To implement this we use a table `probs` of size $|\\mathcal{S}| \\times |\\mathcal{A}|$. So `probs[s][a]` $= \\pi(a|s)$. Here we implement an agent that (initially) chooses the action with a uniform probability, so all actions are equally likely in each state.\\\n",
    "为了实现这一点，我们使用了一个大小为$|\\mathcal{S}| \\times |\\mathcal{A}|$的`probs`表。因此，`probs[s][a]` $= \\pi(a|s)$。在这里，我们实现了一个(最初)以均匀概率选择动作的agent，因此在每个状态下所有动作的可能性都是相等的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    \n",
    "    def __init__(self, nA = 4, nS = 16):\n",
    "        self.nA = nA # Number of actions\n",
    "        self.nS = nS # Number of states\n",
    "        \n",
    "        # Uniform probabilites in each state.\n",
    "        # That is, in each of the `nS` states\n",
    "        # each of the `nA` actions has probability 1/nA.\n",
    "        self.probs = np.ones((nS,nA))/nA \n",
    "\n",
    "    def act(self, state, done):\n",
    "        action = np.random.choice(self.nA, p = self.probs[state]) \n",
    "        return action # a random policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement a function that will let the agent run on the environment:\\\n",
    "我们还实现了一个函数，让agent在环境上运行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_agent(env, agent, tsleep = 0.05):\n",
    "    state = env.reset()\n",
    "    time_step = 0\n",
    "    total_reward = 0\n",
    "    reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state, done);\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        time_step += 1\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        print(\"Time step:\", time_step)\n",
    "        print(\"State:\", state)\n",
    "        print(\"Action:\", action)\n",
    "        print(\"Total reward:\", total_reward)\n",
    "        time.sleep(tsleep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The environments <a id=\"sec4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will try the methods on two environments. Both environments are $4 \\times 4$ gridworlds, see figure below.\\\n",
    "在本笔记本中，我们将在两个环境中尝试这些方法。这两个环境都是$4 \\times 4$的网格世界，见下图。\n",
    "\n",
    "<img src=\"grid.png\" width=200>\n",
    "\n",
    "The agent can be in one of the 16 grids, so the state space is\\\n",
    "agent可以在16个网格中的一个中，因此状态空间为\n",
    "$$\n",
    "\\mathcal{S} = \\{0, 1, 2, \\ldots, 15\\}.\n",
    "$$\n",
    "In each state the agent can take one out of four actions:\\\n",
    "在每种状态下，agent可以从四个动作中选择一个:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the action space is\\\n",
    "所以动作空间是\n",
    "$$\n",
    "\\mathcal{A} = \\{ 0, 1, 2, 3\\}\n",
    "$$\n",
    "Lets now look at the two different environments.\\\n",
    "现在让我们看看这两种不同的环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Example 4.1: GridWorld-v0 <a id=\"sec4_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first environment is called `GridWorld-v0`, and is described in Example 4.1 of the textbook. To create the environment and study the state and action spaces, execute:\\\n",
    "第一个环境称为`GridWorld-v0`，在教科书的示例4.1中有描述。创造环境，研究状态和行动空间，执行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Discrete(16)\n",
      "Action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('GridWorld-v0')\n",
    "state = env.reset()\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the state space has 16 states, and there 4 actions (as mentioned above). Let us next visualize the environment:\\\n",
    "我们可以看到状态空间有16个状态，还有4个动作(如上所述)。下面让我们来看看环境:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GFFF\n",
      "FFFF\n",
      "F\u001b[41mS\u001b[0mFF\n",
      "FFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`S` is the starting state.\\\n",
    "`S`是起始状态。\n",
    "\n",
    "`F` is a state where the agent can walk.\\\n",
    "`F`是agent可以行走的状态。\n",
    "\n",
    "`G` are the two goal states. (In Example 4.1 these two states are considered to be one state).\\\n",
    "`G`是两个目标状态。(在例4.1中，这两种状态被认为是一种状态)。\n",
    "\n",
    "**Reward:** The agent receives the reward -1 for each action taken, and the episode ends when a goal state is reached. Hence, the agent should reach a goal state with as few actions as possible in order to maximize the total reward.\\\n",
    "agent会因为每次行动而获得-1的奖励，当达到目标状态时，episode就结束了。因此，为了使总reward最大化，agent应该以尽可能少的行动达到目标状态。\n",
    "\n",
    "**Dynamics:** This is a deterministic environment. So if the agent chooses the action `LEFT = 0` then it will move one step to the left if possible. If it hits a wall it will just stay in the same place.\\\n",
    "这是一个确定性的环境。因此，如果agent选择了动作`LEFT = 0`，那么它将尽可能向左移动一步。如果它撞到墙上，它会停留在相同的地方。\n",
    "\n",
    "We can try to move the agent one step to the left with the following code:\\\n",
    "我们可以尝试用以下代码将代理向左移动一步:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "GFFF\n",
      "FFFF\n",
      "\u001b[41mF\u001b[0mSFF\n",
      "FFFG\n",
      "New state: 8\n",
      "Reward: -1.0\n"
     ]
    }
   ],
   "source": [
    "new_state, reward, done, _ = env.step(LEFT) # Take action LEFT = 0\n",
    "env.render()\n",
    "print(\"New state:\", new_state)\n",
    "print(\"Reward:\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More about dynamics:** \n",
    "The GridWorld environment has the variable `env.P` that encodes the dynamics $p(s',r|s,a)$ of the environment.\\\n",
    "GridWorld环境有一个变量`env.P`，它编码环境的动态$p(s',r|s,a)$。\n",
    "\n",
    "`env.P[s][a]` will give back a list where each element is on the form `(probability, next_state, reward, terminating)`. So, if you take action `a` in state `s`, then with probability `probability` you will move to `next_state` and get the reward `reward`. `terminating` tells us if `next_state` will terminate the episode or not.\\\n",
    "`env.P[s][a]`将返回一个列表，其中每个元素都以`(probability, next_state, reward, terminating)`的形式存在。所以，如果你在状态`s`中采取行动`a`，那么你就会以概率`probability`移动到`next_state`并获得reward `reward`。`terminating`告诉我们`next_state`是否会终止episode。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 8, -1.0, False)]\n",
      "With probability 1.0 you will move to state 8 and get the reward -1.0\n"
     ]
    }
   ],
   "source": [
    "s = 9\n",
    "a = LEFT\n",
    "print(env.P[s][a])\n",
    "for p, next_s, reward, _ in env.P[s][a]: # Go through all possible transitions\n",
    "    print(\"With probability\", p, \n",
    "          \"you will move to state\", next_s, \n",
    "          \"and get the reward\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a deterministic environment, the list only contains one next state that is reached with probability 1.0.\\\n",
    "由于这是一个确定性的环境，列表中只包含一个到达概率为1.0的下一个状态。\n",
    "\n",
    "Finally we can try to run the agent that chooses between all actions with equal probability in all states. Try to run it a few time, and note that you will get different total rewards every time since the agent uses a random policy.\\\n",
    "最后，我们可以尝试运行在所有状态下在所有动作之间以相同概率进行选择的agent。尝试运行它几次，并注意每次您将获得不同的总reward，因为代理使用随机策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mG\u001b[0mFFF\n",
      "FFFF\n",
      "FSFF\n",
      "FFFG\n",
      "Time step: 13\n",
      "State: 0\n",
      "Action: 0\n",
      "Total reward: -13.0\n"
     ]
    }
   ],
   "source": [
    "agent = RandomAgent(env.action_space.n, env.observation_space.n)\n",
    "run_agent(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task:** optimal policy\n",
    "By changing `agent.probs` we can change the policy of the agent. Try to implement an optimal policy (see Figure 4.1 in the textbook).\\\n",
    "通过改变`agent.probs`，我们可以改变agent的策略。尝试实施最优政策（见教科书图4.1）。\n",
    "\n",
    "Note that when two possible directions are shown in the optimal policy, then you can choose between them anyway you want (either pick one with probability 1.0, or maybe pick between them with equal probability).\\\n",
    "请注意，当最优策略中显示了两个可能的方向时，您可以根据自己的意愿在其中进行选择（要么选择概率为1.0的方向，要么以相等概率从它们之中选择一个）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]\n",
      " [0.  0.  0.  1. ]\n",
      " [0.5 0.  0.  0.5]\n",
      " [0.5 0.5 0.  0. ]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.  0.  0.  1. ]\n",
      " [0.  0.  0.5 0.5]\n",
      " [0.  0.5 0.5 0. ]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.  0.  0.5 0.5]\n",
      " [0.  0.  1.  0. ]\n",
      " [0.  0.  1.  0. ]\n",
      " [0.  0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "agent.probs = np.zeros((16, 4))\n",
    "\n",
    "# Note that in each state the total\n",
    "# probability must add upp to 1.0.\n",
    "\n",
    "# Row 1\n",
    "agent.probs[1][LEFT] = 1.0\n",
    "agent.probs[2][LEFT] = 1.0\n",
    "agent.probs[3][[LEFT, DOWN]] = 0.5 # Pick between them with equal probability\n",
    "\n",
    "# Row 2\n",
    "agent.probs[4][UP] = 1.0\n",
    "agent.probs[5][[LEFT, UP]] = 0.5\n",
    "agent.probs[6][[LEFT, DOWN]] = 0.5\n",
    "agent.probs[7][DOWN] = 1.0\n",
    "\n",
    "# Row 3\n",
    "agent.probs[8][UP] = 1.0\n",
    "agent.probs[9][[RIGHT, UP]] = 0.5\n",
    "agent.probs[10][[RIGHT, DOWN]] = 0.5\n",
    "agent.probs[11][DOWN] = 1.0\n",
    "\n",
    "\n",
    "# Row 4 \n",
    "agent.probs[12][[RIGHT, UP]] = 0.5\n",
    "agent.probs[13][RIGHT] = 1.0\n",
    "agent.probs[14][RIGHT] = 1.0\n",
    "\n",
    "print(agent.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "GFFF\n",
      "FFFF\n",
      "FSFF\n",
      "FFF\u001b[41mG\u001b[0m\n",
      "Time step: 3\n",
      "State: 15\n",
      "Action: 1\n",
      "Total reward: -3.0\n"
     ]
    }
   ],
   "source": [
    "run_agent(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 The Frozen Lake <a id=\"sec4_2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment `FrozenLake-v0` [FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0/) is similar to `GridWorld-v0`, but it is stochastic.\\\n",
    "环境`FrozenLake-v0`[FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0/)类似于`GridWorld-v0`，但它是随机的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Discrete(16)\n",
      "Action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "state = env.reset()\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`S` is the starting state.\n",
    "\n",
    "`F` is a frozen surface that the agent can walk on, but it is slippery, so the movement direction only partially depends on the action.\\\n",
    "`F`是一个冻结的表面，agent可以在上面行走，但它很滑，所以移动方向只部分取决于动作。\n",
    "\n",
    "`H` is a hole. If the agent steps here, it will fall in. (The episode terminates with $0$ reward.)\\\n",
    "`H`是一个洞。如果agent进入这里，它就会掉进去。(这一episode以$0$的reward结束。)\n",
    "\n",
    "`G` is the goal. If the agents steps here, the episode terminates with reward $+1$.\\\n",
    "`G`是目标。如果agent到达这里，这一episode将以$+1$的reward结束。\n",
    "\n",
    "**Reward**: All actions not leading to the goal state gives a reward of 0. Hence, to maximize the reward the agent much reach the goal state without falling into a hole.\\\n",
    "所有不指向目标状态的行为都给予0的reward。因此，为了reward励最大化，agent要达到目标状态而不会掉进洞里。\n",
    "\n",
    "**Dynamics:** Here we also have `env.P[s][a]` to see the dynamics of the environment.\\\n",
    "这里我们也有`env.P[s][a]`来观察环境的动态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "With probability 0.3333333333333333 you will move to state 5 and get the reward 0.0\n",
      "With probability 0.3333333333333333 you will move to state 8 and get the reward 0.0\n",
      "With probability 0.3333333333333333 you will move to state 13 and get the reward 0.0\n"
     ]
    }
   ],
   "source": [
    "s = 9\n",
    "a = LEFT \n",
    "print(env.P[s][a])\n",
    "for p, next_s, reward, _ in env.P[s][a]:\n",
    "    print(\"With probability\", p, \n",
    "          \"you will move to state\", next_s, \n",
    "          \"and get the reward\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the environment is stochastic in this case, since the action `LEFT` in state 9 may take the agent either to state 5 (up into a hole!), state 8 (left) or state 13 (down), due to the slippery surface.\\\n",
    "我们可以看到，在这种情况下，环境是随机的，因为状态9中的`LEFT`行动可能会将agent带入状态5(向上进入一个洞!)、状态8(向左)或状态13(向下)，因为表面很滑。\n",
    "\n",
    "We can also try to run this environment using the random policy:\\\n",
    "我们也可以尝试使用随机策略来运行这个环境:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Time step: 7\n",
      "State: 7\n",
      "Action: 0\n",
      "Total reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "agent = RandomAgent(env.action_space.n, env.observation_space.n)\n",
    "run_agent(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that typically the agent ends up in one of the holes, and thus the total reward is typically 0 (but the expected total reward is positive, since there is a non-zero probability that we reach the goal).\\\n",
    "我们可以看到，agent通常会在其中一个洞中结束，因此总reward通常是0（但期望的总reward是正的，因为我们达到目标的概率是非0）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. MDPs and the Bellman equations <a id=\"sec5\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will study the Bellman equations for the value function a bit closer. Remember that we defined the return as\\\n",
    "在本节中，我们将进一步研究值函数的Bellman方程。记住，我们把return定义为\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n",
    "$$\n",
    "and the state-value function as\\\n",
    "状态值函数为\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}[G_t | S_{t} = s].\n",
    "$$\n",
    "The Bellman equation for the state-value function is then\\\n",
    "状态值函数的Bellman方程为\n",
    "$$\n",
    "v_\\pi(s) \n",
    "= \\sum_{a} \\pi(a|s) \\sum_{r} \\sum_{s'} p(s',r|s,a) [r + \\gamma v_\\pi(s')] \n",
    "= \\sum_{a} \\pi(a|s) q_{\\pi}(s,a)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "q_{\\pi}(s,a) \n",
    "= \\mathbb{E}_{\\pi} [G_t | S_t=s, A_t=a] \n",
    "= \\sum_{r}\\sum_{s'} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n",
    "$$\n",
    "is the action-value function.\\\n",
    "是动作值函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task:** RHS of the Bellman equation\n",
    "\n",
    "**Implementation:**\n",
    "In the code, we will represent the state-value function $v_\\pi(s)$ as a vector $v$, with one element for each state in $\\mathcal{S}$.\\\n",
    "在代码中，我们将状态值函数$v_\\pi(s)$表示为一个向量$v$，对于每一个在$\\mathcal{S}$中的状态有一个元素。\n",
    "\n",
    "We will now implement functions for computing the right-hand side of the Bellman equation.\\\n",
    "现在我们将实现计算Bellman方程右边的函数。\n",
    "\n",
    "**Task:**\n",
    "Complete `compute_action_value` and `Bellman_RHS`. Make sure that you understand the code.\\\n",
    "完成`compute_action_value`和`Bellman_RHS`。确保你理解了代码。\n",
    "\n",
    "We start by a function that computes the action values $q_{\\pi}(s,a)$ given the state-value function $v_\\pi(s)$.\\\n",
    "给定状态值函数$v_\\pi(s)$，我们从一个计算动作值$q_{\\pi}(s,a)$的函数开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_action_value(env, discount, s, a, v):\n",
    "    \n",
    "    action_value = 0\n",
    "    \n",
    "    for p, next_s, reward, _ in env.P[s][a]:\n",
    "        # Loop through all possible (s', r) pairs\n",
    "        action_value += p * (reward + discount*v[next_s])\n",
    "    \n",
    "    return action_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the action values, we can now compute $\\sum_{a} \\pi(a|s) q_{\\pi}(s,a)$ (the expected action value) to get the right-hand side (RHS) of the Bellman equation.\\\n",
    "有了这些动作值，我们现在可以计算$\\sum_{a} \\pi(a|s) q_{\\pi}(s,a)$（预期的动作值）来得到Bellman方程的右边(RHS)。\n",
    "\n",
    "For this we use `agent.probs[s][a]` $=\\pi(a|s)$, see discussion in \"Helper Functions\".\\\n",
    "为此我们使用agent。`agent.probs[s][a]` $=\\pi(a|s)$，参见“Helper Functions”中的讨论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Bellman_RHS(env, discount, agent, s, v):\n",
    "    \n",
    "    state_value = 0\n",
    "    \n",
    "    for a in range(env.action_space.n):\n",
    "        # Loop through all possible actions\n",
    "        state_value += agent.probs[s][a] * compute_action_value(env, discount, s, a, v)\n",
    "        \n",
    "    return state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we implement a function that, given a value function, computes the right-hand side of the Bellman equation for all states.\\\n",
    "最后，我们实现一个函数，给定一个值函数，计算所有状态的Bellman方程的右边。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Bellman_RHS_all(env, discount, agent, v0):\n",
    "    # v0 is the given value function\n",
    "    # v will be the right-hand side of the Bellman equation\n",
    "    # If v0 is indeed the value function, then we should get v = v0.\n",
    "    \n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        v[s] = Bellman_RHS(env, discount, agent, s, v0)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Test your code on Example 4.1 (GridWorld) <a id=\"sec5_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Example 4.1 we will consider the state-value $v_{\\pi}(s)$ for the policy when each action is taken with equal probability. The discount rate is\\\n",
    "在例4.1中，我们将考虑策略的状态值$v_{\\pi}(s)$，当每个动作的执行概率相等时。discount rate为\n",
    "$$\n",
    "\\gamma = 1\n",
    "$$\n",
    "The value function for this policy is given in Figure 4.1 in the textbook (the lower left), and it is\\\n",
    "该策略的值函数见教科书的图4.1（左下角），为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v as vector: [  0 -14 -20 -22 -14 -18 -20 -20 -20 -20 -18 -14 -22 -20 -14   0]\n",
      "v as matrix:\n",
      " [[  0 -14 -20 -22]\n",
      " [-14 -18 -20 -20]\n",
      " [-20 -20 -18 -14]\n",
      " [-22 -20 -14   0]]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([[0, -14, -20, -22], \n",
    "             [-14, -18, -20, -20],\n",
    "             [-20, -20, -18, -14], \n",
    "             [-22, -20, -14, 0]]).ravel()\n",
    "\n",
    "print(\"v as vector:\", v)\n",
    "print(\"v as matrix:\\n\", v.reshape(4,4))\n",
    "\n",
    "# ravel turns the matrix into an array,\n",
    "# and with reshape we print it as a matrix again so that it is easier to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our code to see if this value function really satisfy the Bellman equation:\\\n",
    "现在我们可以使用我们的代码来看看这个值函数是否真的满足Bellman方程:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right-hand side of Bellman equation:\n",
      " [[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('GridWorld-v0')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "v_new = Bellman_RHS_all(env, discount, agent, v)\n",
    "print('Right-hand side of Bellman equation:\\n', v_new.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** \n",
    "`v` is the true value-function for the policy $\\pi$ implemented in `agent`. Hence, `v_new` (the right-hand side of the Bellman equation) should be equal to `v`.\\\n",
    "`v`是在`agent`中实现的策略$\\pi$的真正的值函数。因此，`v_new`(Bellman方程的右边)应该等于`v`。\n",
    "\n",
    "If `v_new` is not equal to `v`, go back and fix your code for `compute_action_value` and `Bellman_RHS`. Remember to re-run the code cells after you have changed the code!\\\n",
    "如果`v_new`不等于`v`，返回并修复您的代码`compute_action_value`和`Bellman_RHS`。记住在更改代码后重新运行代码单元格!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Policy Evaluation <a id=\"sec6\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lecture 3 we learned that one way of solving the Bellman equation, is to start from an initial guess and then repeatedly update the value function by applying the right-hand side of the Bellman equation.\\\n",
    "在第三讲中，我们学习解Bellman方程的一种方法，从一个初始猜测开始，然后通过应用Bellman方程的右边来反复更新值函数。\n",
    "\n",
    "Below is one way to implement this. The iteration will stop when the maximum change in $v$ is less than `tol` (tolerance) or the number of iterations are `max_iter`.\\\n",
    "下面是实现这一点的一种方法。当$v$的最大变化小于`tol`（公差）或迭代次数为`max_iter`时，迭代将停止。\n",
    "\n",
    "***Note:*** For this code to work properly, your implementation of `compute_action_value` and `Bellman_RHS` must be correct. So make sure that you have tested your code first!\\\n",
    "为了让这段代码正常工作，你的`compute_action_value`和`Bellman_RHS`的实现必须正确。所以要确保你先测试了你的代码!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task:** Understand the Policy Evaluation Code\n",
    "\n",
    "**Task:** Make sure that you understand the code!\\\n",
    "确保你理解了代码!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(env, discount, agent, v0, max_iter = 1000, tol=1e-6):\n",
    "    \n",
    "    v_old = v0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        v_new = Bellman_RHS_all(env, discount, agent, v_old)\n",
    "        \n",
    "        if np.max(np.abs(v_new - v_old)) < tol:\n",
    "            break\n",
    "            \n",
    "        v_old = v_new\n",
    "        \n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try this on the `GridWorld-v0` example, with the uniformly random policy. We start with an initial guess $v_{\\pi}(s) = 0$ for all $s$.\\\n",
    "让我们在`GridWorld-v0`示例中使用一致随机策略进行尝试。我们从所有$s$的初始猜测$v_{\\pi}(s) = 0$开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.         -13.99998982 -19.99998491 -21.99998311]\n",
      " [-13.99998982 -17.9999867  -19.99998501 -19.99998491]\n",
      " [-19.99998491 -19.99998501 -17.9999867  -13.99998982]\n",
      " [-21.99998311 -19.99998491 -13.99998982   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('GridWorld-v0')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "v0 = np.zeros((env.observation_space.n))\n",
    "\n",
    "v = policy_evaluation(env, discount, agent, v0)\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this (approximately) the same as the true value function in Figure 4.1? $(k=\\infty)$\\\n",
    "这（大致）与图4.1中的真值函数相同吗？$(k=\\infty)$\n",
    "\n",
    "If you do not find the correct value function, make sure that your code in `compute_action_value` and `Bellman_RHS` is correct!\\\n",
    "如果没有找到正确的值函数，请确保`compute_action_value`和`Bellman_RHS`中的代码是正确的!\n",
    "\n",
    "To replicate the other parts of Figure 4.1, you can set `max_iter` in order to see how the value function looks after a few iterations.\\\n",
    "为了重现图4.1的其他部分，可以设置`max_iter`，以便查看值函数在几次迭代后的样子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "v = policy_evaluation(env, discount, agent, v0, max_iter = 1)\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   -1.75 -2.   -2.  ]\n",
      " [-1.75 -2.   -2.   -2.  ]\n",
      " [-2.   -2.   -2.   -1.75]\n",
      " [-2.   -2.   -1.75  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "v = policy_evaluation(env, discount, agent, v0, max_iter = 2)\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.     -2.4375 -2.9375 -3.    ]\n",
      " [-2.4375 -2.875  -3.     -2.9375]\n",
      " [-2.9375 -3.     -2.875  -2.4375]\n",
      " [-3.     -2.9375 -2.4375  0.    ]]\n"
     ]
    }
   ],
   "source": [
    "v = policy_evaluation(env, discount, agent, v0, max_iter = 3)\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -6.13796997 -8.35235596 -8.96731567]\n",
      " [-6.13796997 -7.73739624 -8.42782593 -8.35235596]\n",
      " [-8.35235596 -8.42782593 -7.73739624 -6.13796997]\n",
      " [-8.96731567 -8.35235596 -6.13796997  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "v = policy_evaluation(env, discount, agent, v0, max_iter = 10)\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task:** Compute Value Function by Policy Evaluation\n",
    "**Task:** Use `policy_evaluation` to compute the value function for `FrozenLake-v0` when the uniformly random policy is used. Use $\\gamma = 1$.\\\n",
    "在使用均匀随机策略时，使用`policy_evaluation`计算`FrozenLake-v0`的值函数。使用$\\gamma = 1$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[[0.0139352  0.01162767 0.02095011 0.01047427]\n",
      " [0.01624597 0.         0.04075041 0.        ]\n",
      " [0.03480461 0.08816898 0.14205233 0.        ]\n",
      " [0.         0.17581966 0.4392905  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "env.render()\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "\n",
    "# Write code for computing the state-value function\n",
    "v0 = np.zeros((env.observation_space.n))\n",
    "v = policy_evaluation(env, discount, agent, v0, max_iter = 1000)\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 In place updates <a id=\"sec6_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in Lecture 3 and in the textbook, the policy evaluation is often implemented using in place updates.\\\n",
    "正如第三节课和教材中所提到的，策略评估通常是使用适当的更新来实施的。\n",
    "\n",
    "This can both simplify implementation, since we do not keep two separate arrays, and it can also speed up convergence.\\\n",
    "这既可以简化实现，因为我们不保留两个单独的数组，也可以加快收敛速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task:** In-place policy evaluation\n",
    "\n",
    "**Task:** Complete the code in `policy_evaluation_ip`. Pseudo-code can be found in the textbook in the box on page 75. Then test your code on `GridWorld-v0` with the uniform policy to see that you still get the correct value function.\\\n",
    "完成`policy_evaluation_ip`中的代码。伪代码可以在课本第75页的方框中找到。然后使用均匀策略在`GridWorld`上测试代码，看看是否仍然得到了正确的值函数。\n",
    "\n",
    "***Note:*** You have already written a function that computes the right-hand side of the Bellman equation!\\\n",
    "您已经编写了一个计算Bellman方程右边的函数!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def policy_evaluation_ip(env, discount, agent, v0, max_iter = 1000, tol = 1e-6):\n",
    "    \n",
    "    v = v0\n",
    "    \n",
    "    for i in range(max_iter): # Loop\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            vs = v[s]\n",
    "            \n",
    "            # Code for updating v[s]\n",
    "            v[s] = Bellman_RHS(env, discount, agent, s, v)\n",
    "            \n",
    "            delta = np.max([delta, np.abs(vs - v[s])])\n",
    "            \n",
    "        if (delta < tol): # Until delta < tol\n",
    "            break\n",
    "            \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.         -13.99999335 -19.99999044 -21.99998948]\n",
      " [-13.99999335 -17.99999184 -19.99999114 -19.99999125]\n",
      " [-19.99999044 -19.99999114 -17.99999253 -13.99999442]\n",
      " [-21.99998948 -19.99999125 -13.99999442   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('GridWorld-v0')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "\n",
    "v0 = np.zeros(16)\n",
    "v = policy_evaluation_ip(env, discount, agent, v0)\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Policy Iteration <a id=\"sec7\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we have code for evaluating a policy, it is time to see how it can be improved. Remember that the idea is to act greedily with respect to $v_{\\pi}(s)$. That is, given $v_{\\pi}(s)$ we can compute $q_{\\pi}(s,a)$, and then the greedy (improved) policy is\\\n",
    "现在，当我们有了评估策略的代码后，就该看看如何改进它了。记住，这个想法是对$v_{\\pi}(s)$的贪婪行为。即，给定$v_{\\pi}(s)$，我们可以计算$q_{\\pi}(s,a)$，则贪婪（改进）策略为\n",
    "$$\n",
    "\\pi'(s) = \\text{argmax}_{a} q_{\\pi}(s,a)\n",
    "$$\n",
    "We have already written code for computing $q_{\\pi}(s,a)$ for a given $v_{\\pi}(s)$, so the only thing we have to do now is to implement the maximization.\\\n",
    "我们已经为给定的$v_{\\pi}(s)$编写了计算$q_{\\pi}(s,a)$的代码，所以我们现在唯一要做的就是实现最大化。\n",
    "\n",
    "`greedy_policy` will return `a_probs` which encode a policy that is greedy with respect to `v`. That is `a_probs[s][a]` $= \\pi'(a|s)$. Make sure that you understand the code.\\\n",
    "`greedy_policy`将返回`a_probs`，它将编码一个相对于`v`是贪婪的策略。这是`a_probs[s][a]` $= \\pi'(a|s)$。确保你理解了代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def greedy_policy(env, discount, agent, v):\n",
    "    \n",
    "    # The new policy will be a_probs\n",
    "    # We start by setting all probabilities to 0\n",
    "    # Then when we have found the greedy action in a state, \n",
    "    # we change the probability for that action to 1.0.\n",
    "    \n",
    "    a_probs = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        \n",
    "        action_values = np.zeros(env.action_space.n)\n",
    "        \n",
    "        for a in range(env.action_space.n):\n",
    "            # Compute action value for all actions\n",
    "            action_values[a] = compute_action_value(env, discount, s, a, v)\n",
    "            \n",
    "        a_max = np.argmax(action_values) # A greedy action\n",
    "        a_probs[s][a_max] = 1.0 # Always choose the greedy action!\n",
    "        \n",
    "    return a_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to improve the policy on `GridWorld-v0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('GridWorld-v0')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "\n",
    "# We first evaluate the policy\n",
    "v = np.zeros(env.observation_space.n)\n",
    "v = policy_evaluation(env, discount, agent, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of initial policy:\n",
      "[[  0.         -13.99998982 -19.99998491 -21.99998311]\n",
      " [-13.99998982 -17.9999867  -19.99998501 -19.99998491]\n",
      " [-19.99998491 -19.99998501 -17.9999867  -13.99998982]\n",
      " [-21.99998311 -19.99998491 -13.99998982   0.        ]]\n",
      "\n",
      "Value of improved policy:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "v_old = v\n",
    "\n",
    "# And then we improve the policy (act greedy w.r.t v)\n",
    "agent.probs = greedy_policy(env, discount, agent, v)\n",
    "\n",
    "# We can also evaluate the new policy \n",
    "v = policy_evaluation(env, discount, agent, v)\n",
    "\n",
    "print(\"Value of initial policy:\")\n",
    "print(v_old.reshape(4,4))\n",
    "print(\"\\nValue of improved policy:\")\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of initial policy:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "Value of improved policy:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "v_old = v\n",
    "\n",
    "# And then we improve the policy (act greedy w.r.t v)\n",
    "agent.probs = greedy_policy(env, discount, agent, v)\n",
    "\n",
    "# We can also evaluate the new policy \n",
    "v = policy_evaluation(env, discount, agent, v)\n",
    "\n",
    "print(\"Value of initial policy:\")\n",
    "print(v_old.reshape(4,4))\n",
    "print(\"\\nValue of improved policy:\")\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that your implementation of `compute_action_value` is correct, we can clearly see that the improved policy has higher value in every state. In fact, the policy is now an optimal policy. To see this, you can try to rerun the second cell above and note that the policy does not improve anymore.\\\n",
    "假设`compute_action_value`的实现是正确的，我们可以清楚地看到改进的策略在每个状态下都有更高的值。事实上，现在的政策是最优政策。要查看这一点，您可以尝试重新运行上面的第二个单元格，并注意策略不再改进。\n",
    "\n",
    "**Policy iteration:** However, it is not the case for all environments that the policy will converge in just one improvement. In this case we may have to improve the policy several times until it finally converge to the optimal policy.\\\n",
    "__策略迭代__：然而，并不是在所有的环境中，策略都会在一次改进中收敛。在这种情况下，我们可能需要多次改进策略，直到它最终收敛到最优策略。\n",
    "\n",
    "Finally, we can try to run the agent with the improved policy.\\\n",
    "最后，我们可以尝试使用改进的策略运行代理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "\u001b[41mG\u001b[0mFFF\n",
      "FFFF\n",
      "FSFF\n",
      "FFFG\n",
      "Time step: 3\n",
      "State: 0\n",
      "Action: 3\n",
      "Total reward: -3.0\n"
     ]
    }
   ],
   "source": [
    "run_agent(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task:** Find an Optimal Policy for `FrozenLake-v0`\n",
    "\n",
    "**Task:** Find an optimal policy for `FrozenLake-v0`. (Note again that you may have to improve several times to reach an optimal policy!)\\\n",
    "找到`FrozenLake-v0`的最佳策略。（再次注意，您可能需要多次改进才能达到最佳策略！）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "\n",
    "# Enter code here\n",
    "# We first evaluate the policy\n",
    "v = np.zeros(env.observation_space.n)\n",
    "v = policy_evaluation(env, discount, agent, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of initial policy:\n",
      "[[0.0139352  0.01162767 0.02095011 0.01047427]\n",
      " [0.01624597 0.         0.04075041 0.        ]\n",
      " [0.03480461 0.08816898 0.14205233 0.        ]\n",
      " [0.         0.17581966 0.4392905  0.        ]]\n",
      "\n",
      "Value of improved policy:\n",
      "[[0.78045617 0.65850977 0.53656581 0.53656386]\n",
      " [0.78045904 0.         0.41462363 0.        ]\n",
      " [0.78046452 0.78047211 0.70730602 0.        ]\n",
      " [0.         0.85364721 0.92682333 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "v_old = v\n",
    "\n",
    "# And then we improve the policy (act greedy w.r.t v)\n",
    "agent.probs = greedy_policy(env, discount, agent, v)\n",
    "\n",
    "# We can also evaluate the new policy \n",
    "v = policy_evaluation(env, discount, agent, v)\n",
    "\n",
    "print(\"Value of initial policy:\")\n",
    "print(v_old.reshape(4,4))\n",
    "print(\"\\nValue of improved policy:\")\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Time step: 30\n",
      "State: 15\n",
      "Action: 1\n",
      "Total reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "# run the agent with the improved policy\n",
    "run_agent(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Value iteration <a id=\"sec8\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the value iteration we instead start from the Bellman optimality equation\\\n",
    "在值迭代中，我们从Bellman最优方程开始\n",
    "\n",
    "$$\n",
    "v_{*}(s) = \\max_{a} q_{\\pi_*}(s,a) = \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_{*}(s')]\n",
    "$$\n",
    "\n",
    "We start with an initial guess $v_0$ and then we repeatedly compute the right-hand side of this equation, until we converge to the optimal state-value function. When we have the optimal state-value function $v_*$, we can take any policy that is greedy w.r.t $v_*$ and this will give us an optimal policy.\\\n",
    "我们从初始猜测$v_0$开始，然后重复计算这个方程的右边，直到我们收敛到最优状态值函数。当我们有最优状态值函数$v_*$时，我们可以取任何对$v_*$贪婪的策略，这将给我们一个最优策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 1:** Value Iteration\n",
    "\n",
    "**Task 1:** Complete the code below. Pseudo-code for the algorithm can be found on page 83 in the textbook. Note that the code for computing the action-value given $v_{\\pi}$ has already been implemented above.\\\n",
    "完成以下代码。算法的伪代码可以在课本的83页找到。请注意，计算给定$v_{\\pi}$的动作值的代码已经在上面实现了。\n",
    "\n",
    "The `value_iteration` function will (if implemented correctly) give back the optimal value function.\\\n",
    "`value_iteration`函数将（如果正确实现）返回最优值函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 2:** Compute the Optimal Policy\n",
    "\n",
    "**Task 2:** Also add some code for computing the optimal policy given this, and try it on `FrozenLake-v0` and/or `GridWorld-v0`.\\\n",
    "还添加一些代码来计算给定的最优策略，并在`FrozenLake-v0`和/或`GridWorld-v0`上尝试它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, discount, agent, v0, max_iter = 1000, tol=1e-6):\n",
    "    \n",
    "    v = v0\n",
    "    optimal_policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for i in range(max_iter): # Loop\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            vs = v[s]\n",
    "            \n",
    "            # Code for updating v[s]\n",
    "            q = np.zeros(env.action_space.n)\n",
    "            for a in range(env.action_space.n):\n",
    "                q[a] = compute_action_value(env, discount, s, a, v)\n",
    "            \n",
    "            v[s] = np.max(q)\n",
    "            optimal_policy[s] = np.argmax(q)\n",
    "            \n",
    "            delta = np.max([delta, np.abs(vs - v[s])])\n",
    "            \n",
    "        if (delta < tol): # Until delta < tol\n",
    "            break\n",
    "    \n",
    "    return v, optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal value: \n",
      " [[0.82351232 0.82350671 0.82350281 0.82350083]\n",
      " [0.82351404 0.         0.52940011 0.        ]\n",
      " [0.82351673 0.82352018 0.76469779 0.        ]\n",
      " [0.         0.88234653 0.94117321 0.        ]]\n",
      "optimal policy: \n",
      " [[0. 3. 3. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "\n",
    "v0 = np.zeros(env.observation_space.n)\n",
    "v, op = value_iteration(env, discount, agent, v0)\n",
    "print(f\"optimal value: \\n {v.reshape(4,4)}\")\n",
    "print(f\"optimal policy: \\n {op.reshape(4,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BA 1.4 `FrozenLake8x8-v0`\n",
    "\n",
    "Consider the `FrozenLake8x8-v0` environment. It is similar to the `FrozenLake-v0` that was studied in Tinkering Notebook 2, but it consist of an $8 \\times 8$ grid and thus have 64 states.\\\n",
    "考虑`FrozenLake8x8-v0`环境。它类似于Tinkering Notebook 2中学习的`FrozenLake-v0`，但它由一个$8 \\times 8$ 的网格组成，因此有64个状态。\n",
    "\n",
    "Write a code that find an optimal policy $\\pi_*(s)$ and the corresponding value function $v_*(s)$.\\\n",
    "编写代码，查找最佳策略$\\pi_*(s)$和相应的值函数$v_*(s)$。\n",
    "\n",
    "In the quizz on you will be asked for example \"Which of these are optimal actions in state $s = 26$?\" or \"What is $v_*(26)$?\". So make sure that you can easily run code that can answer these types of questions for different states.\\\n",
    "在quizz中，你会被问到“在状态$s = 26$时，哪些是最优行为?”或者“$v_*(26)$是什么?”因此，请确保您可以轻松地运行可以回答不同状态下的这类问题的代码。\n",
    "\n",
    "__Hint:__ You can check that your code seems to be working by ensuring that you get to correct answer to the following:\\\n",
    "你可以检查你的代码似乎工作通过确保你得到以下的正确答案：\n",
    "* For the optimal policy $v_*(26) = 0.80$ (rounded to two decimals).\n",
    "* In $s = 26$ the optimal action is 0 (left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal value: \n",
      " [[0.99998912 0.99998973 0.99999042 0.99999114 0.99999184 0.9999925\n",
      "  0.99999308 0.99999351]\n",
      " [0.99998918 0.99998967 0.99999029 0.99999098 0.99999168 0.99999237\n",
      "  0.99999308 0.99999391]\n",
      " [0.99997942 0.97818294 0.92641478 0.         0.85660997 0.94622427\n",
      "  0.98207059 0.99999454]\n",
      " [0.99997068 0.93458025 0.80107148 0.4748952  0.62361428 0.\n",
      "  0.94467234 0.99999539]\n",
      " [0.99996331 0.82558766 0.54221971 0.         0.53933775 0.61118505\n",
      "  0.85195124 0.9999964 ]\n",
      " [0.99995756 0.         0.         0.16803827 0.38321409 0.44226632\n",
      "  0.         0.99999754]\n",
      " [0.99995363 0.         0.19466435 0.12090087 0.         0.33239983\n",
      "  0.         0.99999876]\n",
      " [0.99995164 0.73152158 0.46309273 0.         0.27746659 0.5549332\n",
      "  0.7774666  0.        ]]\n",
      "optimal policy: \n",
      " [[1. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [0. 0. 0. 0. 2. 3. 3. 2.]\n",
      " [0. 0. 0. 1. 0. 0. 2. 2.]\n",
      " [0. 3. 0. 0. 2. 1. 3. 2.]\n",
      " [0. 0. 0. 1. 3. 0. 0. 2.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 2.]\n",
      " [0. 1. 0. 0. 1. 2. 1. 0.]]\n",
      "v(26): 0.8\n",
      "op(26): 0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')\n",
    "agent = RandomAgent(nA = env.action_space.n, nS = env.observation_space.n)\n",
    "discount = 1\n",
    "\n",
    "v0 = np.zeros(env.observation_space.n)\n",
    "v, op = value_iteration(env, discount, agent, v0)\n",
    "d = int(math.sqrt(env.observation_space.n))\n",
    "print(f\"optimal value: \\n {v.reshape(d,d)}\")\n",
    "print(f\"optimal policy: \\n {op.reshape(d,d)}\")\n",
    "print(f\"v(26): {round(v[26],2)}\")\n",
    "print(f\"op(26): {op[26]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
