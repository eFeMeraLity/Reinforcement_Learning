{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tinkering notebook 2: Markov Decision Processes and Dynamic Programming\n",
    "\n",
    "In this notebook we will see how some of the content of Lecture 2 - Lecture 3 works in practice. \n",
    "\n",
    "We will start by a repetition of Markov Decision Processes (MDPs) and value functions. After this we will study two GridWorld examples (Example 4.1 in the textbook and FrozenLake). Here we will use dynamic programming to learn both value functions and optimal policies.\\\n",
    "我们将从重复马尔可夫决策过程(MDPs)和价值函数开始。在这之后，我们将学习两个GridWorld的例子(例子4.1在教科书和结冰湖)。这里我们将使用动态规划来学习值函数和最优策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "* ### [1. Imports](#sec1)\n",
    "* ### [2. Markov Decision Processes and Value Functions](#sec2)\n",
    " * #### [2.1 The Bellman equations](#sec2_1)\n",
    " * #### [2.2 Example: Study or Facebook?](#sec2_2)\n",
    " * #### [2.3 *Analytical solution to the Bellman equation](#sec2_3)\n",
    "* ### [3. Helper functions](#sec3)\n",
    "* ### [4. The environments](#sec4)\n",
    " * #### [4.1 Example 4.1: GridWorld-v0](#sec4_1)\n",
    " * #### [4.2 The Frozen Lake](#sec4_2)\n",
    "* ### [5. MDPs and the Bellman equations](#sec5)\n",
    " * #### [5.1 Test your code on Example 4.1 (GridWorld)](#sec5_1)\n",
    "* ### [6. Policy Evaluation](#sec6)\n",
    " * #### [6.1 In place updates](#sec6_1)\n",
    "* ### [7. Policy Iteration](#sec7)\n",
    "* ### [8. Value iteration](#sec8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports <a id=\"sec1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages needed in this notebook are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"gym-gridworld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Packages needed for this notebook\n",
    "import gym\n",
    "import gym_gridworld\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Markov Decision Processes and Value Functions <a id=\"sec2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MDP consist of a state space $\\mathcal{S}$, an action space $\\mathcal{A}$, a reward set $\\mathcal{R}$ and a transition function $p(s', r | s, a)$. We define the return as\\\n",
    "MDP由一个状态空间 $\\mathcal{S}$、一个动作空间$\\mathcal{A}$、一个reward集$\\mathcal{R}$和一个转换函数$p(s', r | s, a)$组成。我们将return定义为\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n",
    "$$\n",
    "where $\\gamma$ is the discount rate.\\\n",
    "其中，$\\gamma$是贴现率。\n",
    "\n",
    "**State-value function:**\n",
    "$$\n",
    "v_{\\pi}(s) = \\mathbb{E}_{\\pi}[ G_t | S_t = s]\n",
    "$$\n",
    "The expected return starting from state $s \\in \\mathcal{S}$ and following policy $\\pi$.\\\n",
    "从状态$s \\in \\mathcal{S}$开始，然后遵循策略$\\pi$的expected return。\n",
    "\n",
    "**Action-value function ($Q$-value)**:\n",
    "$$\n",
    "q_{\\pi}(s) = \\mathbb{E}_{\\pi}[ G_t | S_t = s, A_t = a]\n",
    "$$\n",
    "The expected return starting from state $s \\in \\mathcal{S}$, then taking action $a \\in \\mathcal{A}$ and then follow policy $\\pi$.\\\n",
    "从状态$s \\in \\mathcal{S}$开始，然后采取动作$a \\in \\mathcal{A}$，然后遵循策略$\\pi$的expected return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 The Bellman equations <a id=\"sec2_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that $v_{\\pi}(s)$ is the solution to the Bellman equation\\\n",
    "我们已经知道$v_{\\pi}(s)$是Bellman方程的解\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) \n",
    "&= \\mathbb{E}_{\\pi}[ R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t = s] \\\\\n",
    "&= \\sum_{a} \\pi(a|s) \\sum_{r} \\sum_{s'} p(s',r|s,a) [r + \\gamma v_\\pi(s')] \\\\\n",
    "&= \\sum_{a} \\pi(a|s) q_{\\pi}(s,a)\n",
    "\\end{align}\n",
    "$$\n",
    "where\\\n",
    "其中\n",
    "$$\n",
    "\\begin{align}\n",
    "q_{\\pi}(s,a) \n",
    "&= \\mathbb{E}_{\\pi}[G_t|S_t=s, A_t=a] \\\\\n",
    "&= \\sum_{r}\\sum_{s'} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Example: Study or Facebook? <a id=\"sec2_2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lecture 2 we looked at the example MDP bellow.\n",
    "<img src = \"example.png\">\n",
    "\n",
    "It has four states, $\\mathcal{S} = \\{ K_0, K_1, K_2, Pass \\}$. The state $Pass$ is a terminal state, so the episode will end if this state is reached (alternatively, if we reach $Pass$ we will stay there forever and receive 0 future return).\\\n",
    "它有四种状态，$\\mathcal{S} = \\{ K_0, K_1, K_2, Pass \\}$。状态$Pass$是一种terminal状态，所以当到达该状态时，事件将结束（或者，如果到达$Pass$，我们将永远停留在那里，并在未来收到0返回）。\n",
    "\n",
    "In each non-terminal state we can choose between `Study` or `Facebook`, $\\mathcal{A} = \\{ Study, Facebook\\}$. The red nodes corresponds to actions, and the labels on the edges gives immediate rewards (green) and transition probabilities (black).\\\n",
    "在每个non-terminal状态中，我们可以选择`Study`或`Facebook`，$\\mathcal{A} = \\{ Study, Facebook\\}$。红色节点对应于行动，边缘上的标签提供即时reward(绿色)和转移概率(黑色)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** \n",
    "Assume that we use the policy $\\pi(s|a) = 0.5$ for all states and actions, and the discount factor $\\gamma = 0.9$.\\\n",
    "假设我们对所有状态和动作使用策略$\\pi(s|a) = 0.5$，discount因子$\\gamma = 0.9$。\n",
    "\n",
    "In Lecture 2 we saw that the state-value function (rounded to two decimals) is\\\n",
    "在第二讲中，我们看到state-value函数（四舍五入到两位小数）是\n",
    "$$\n",
    "v_{\\pi}(K_0) = 3.00, \\quad v_{\\pi}(K_1) = 4.78, \\quad v_{\\pi}(K_2) = 7.84.\n",
    "$$\n",
    "1. Verify that this satisfies the Bellman equation for all states! (At least approximately, since we have rounded everything to two decimals).\\\n",
    "对于所有状态，验证此满足Bellman方程(至少是近似的，因为我们把所有的都四舍五入到两个小数)。\n",
    "\n",
    "You can use the code-block bellow to carry out your computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v(K0) = 3.0 v(K1) = 4.78 v(K2) = 7.84\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "K0 = 3.00\n",
    "K1 = 4.78\n",
    "K2 = 7.84\n",
    "P = 0\n",
    "pi_FB = 0.5\n",
    "pi_S = 0.5\n",
    "\n",
    "vK0 = pi_FB * (0 + gamma*K0) + pi_S * (-1 + gamma*K1)\n",
    "vK1 = pi_FB * (0 + gamma*(0.5*K0 + 0.5*K1)) + pi_S * (-1 + gamma*K2)\n",
    "vK2 = pi_FB * (0 + gamma*(0.5*K1 + 0.5*K2)) + pi_S * (10 + gamma*P)\n",
    "print(\"v(K0) =\", round(vK0,2), \"v(K1) =\", round(vK1,2), \"v(K2) =\", round(vK2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 *Analytical solution to the Bellman equation <a id=\"sec2_3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Bellman equation can be written as\\\n",
    "请注意，Bellman方程可以写成\n",
    "$$\n",
    "v_{\\pi}(s) = \\mathbb{E}_{\\pi}[ R_{t+1} | S_{t} = s] + \\gamma\\sum_{a, s'} \\pi(a|s)p(s' | s, a) v_{\\pi}(s').\n",
    "$$\n",
    "So the value of state $s$ is the average immediate reward plus the discounted average value of the next state.\\\n",
    "所以状态$s的价值是平均即时reward加上下一个状态的平均贴现值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: From the state $K_1$**:\n",
    "    \n",
    "* In $s = K_1$ the expected immediate reward is $-0.5$, since we choose `Facebook` with probability 0.5 (reward 0) and `Study` with probability 0.5 (reward -1). \n",
    "* There is a 0.5 probability that the action is `Facebook`, and then there is a 50/50 chance of going either to $K_0$ or $K_1$. Hence the total probability of going to $K_1$ and $K_2$ are both 0.25 ($0.5 \\times 0.5$). There is also a 0.5 probability for the action `Study` which will move us to $K_2$. Finally there is 0 probability of reaching $Pass$. \n",
    "\n",
    "Summarizing this we get\n",
    "$$\n",
    "v_{\\pi}(K_1) = -0.5 + \\gamma [ 0.25 v_{\\pi}(K_0) + 0.25 v_{\\pi}(K_1) + 0.5 v_{\\pi}(K_2) + 0 v_{\\pi}(Pass)]\n",
    "$$\n",
    "If we define a vector with all state-values\n",
    "$$\n",
    "V_{\\pi} = \\begin{bmatrix} v_{\\pi}(K_0) \\\\ v_{\\pi}(K_1) \\\\ v_{\\pi}(K_2) \\\\ v_{\\pi}(Pass) \\end{bmatrix}\n",
    "$$\n",
    "we can write this as\n",
    "$$\n",
    "v_{\\pi}(K_1) = \\underbrace{-0.5}_{r_1} + \\gamma \\underbrace{\\begin{bmatrix} 0.25 & 0.25 & 0.5 & 0 \\end{bmatrix}}_{p_1} V_\\pi\n",
    "$$\n",
    "Note that the elements of $p_1$ are the probability of going from $K_1$ to each of the other states when we follow the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting it all together:**\n",
    "\n",
    "If we do the same for all states, we get an equation on the form \n",
    "$$\n",
    "V_{\\pi} = R + \\gamma P V_{\\pi}\n",
    "$$\n",
    "where $R$ is the vector of expected immediate rewards for each state, and $P \\in \\mathbb{R}^{4 \\times 4}$ where element $P_{i,j}$ is the probability of moving from the $i$th state to the $j$th state when we follow the policy.\n",
    "\n",
    "Assuming that $\\gamma < 1$ there is always a unique solution given by \n",
    "$$\n",
    "V_{\\pi} = (I - \\gamma P)^{-1} R.\n",
    "$$\n",
    "\n",
    "**Task**: Fill in the correct values of $R$ and $P$ in the code below, and see if you find the same solution as in the slides of Lecture 2. You can also play around with the discount rate, and/or try to compute $R$, $P$ and then $V_{\\pi}$ for another policy.\n",
    "\n",
    "**Note**: The state $Pass$ is a bit special, since it is a terminating state. Hence, when we reach \"Pass the exam\" we will not receive anymore rewards, and we will stay in this state forever (the probability of going to any other state is 0). Hence\n",
    "$$\n",
    "v_{\\pi}(Pass) = 0 + \\gamma \\begin{bmatrix} 0 & 0 & 0 & 1 \\end{bmatrix} V_{\\pi}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "discount = 0.9 # gamma\n",
    "R = np.zeros((4,1))\n",
    "P = np.zeros((4,4))\n",
    "\n",
    "# Enter the expected immediate reward for each state\n",
    "# Those computed in the text above are already filled in.\n",
    "R[0] = ? # For K_0\n",
    "R[1] = -.5 # For K_1\n",
    "R[2] = ? # For K_2\n",
    "R[3] = 0 # For \"Pass exam\"\n",
    "\n",
    "# Enter the probabilities going from state i to state j\n",
    "P[0] = [?, ?, ?, ?] # for i=0 (K_0)\n",
    "P[1] = [0.25, 0.25, 0.5, 0] # for i=1 (K_1)\n",
    "P[2] = [?, ?, ?, ?] # for i=2 (K_2)\n",
    "P[3] = [0, 0, 0, 1] # for i=3 (Pass the exam) \n",
    "\n",
    "# Solve the Bellman equation \n",
    "V = np.linalg.inv(np.eye(4) - discount*P)@R # V = (I - discount*P)^-1 * R\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Helper functions <a id=\"sec3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now start to look at two GridWorld examples, but first we define some functions that will be useful in the rest of the notebook.\\\n",
    "现在我们将开始看两个GridWorld示例，但是首先我们定义一些函数，这些函数将在本笔记本的其余部分中有用。\n",
    "\n",
    "The class `RandomAgent` implements an agent with a policy $\\pi(a|s)$. The method `act` will take the state $s$ as input, and then sample an action according to the probabilities $\\pi(s|a)$.\\\n",
    "`RandomAgent`类实现了一个带有策略$\\pi(a|s)$的agent。方法`act`将状态$s$作为输入，然后根据概率$\\pi(s|a)$对动作进行抽样。\n",
    "\n",
    "To implement this we use a table `probs` of size $|\\mathcal{S}| \\times |\\mathcal{A}|$. So `probs[s][a]` $= \\pi(a|s)$. Here we implement an agent that (initially) chooses the action with a uniform probability, so all actions are equally likely in each state.\\\n",
    "为了实现这一点，我们使用了一个大小为$|\\mathcal{S}| \\times |\\mathcal{A}|$的`probs`表。因此，`probs[s][a]` $= \\pi(a|s)$。在这里，我们实现了一个(最初)以均匀概率选择动作的agent，因此在每个状态下所有动作的可能性都是相等的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    \n",
    "    def __init__(self, nA = 4, nS = 16):\n",
    "        self.nA = nA # Number of actions\n",
    "        self.nS = nS # Number of states\n",
    "        \n",
    "        # Uniform probabilites in each state.\n",
    "        # That is, in each of the `nS` states\n",
    "        # each of the `nA` actions has probability 1/nA.\n",
    "        self.probs = np.ones((nS,nA))/nA \n",
    "\n",
    "    def act(self, state, done):\n",
    "        action = np.random.choice(self.nA, p = self.probs[state]) \n",
    "        return action # a random policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement a function that will let the agent run on the environment:\\\n",
    "我们还实现了一个函数，让agent在环境上运行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_agent(env, agent, tsleep = 0.05):\n",
    "    state = env.reset()\n",
    "    time_step = 0\n",
    "    total_reward = 0\n",
    "    reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state, done);\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        time_step += 1\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        print(\"Time step:\", time_step)\n",
    "        print(\"State:\", state)\n",
    "        print(\"Action:\", action)\n",
    "        print(\"Total reward:\", total_reward)\n",
    "        time.sleep(tsleep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The environments <a id=\"sec4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will try the methods on two environments. Both environments are $4 \\times 4$ gridworlds, see figure below.\\\n",
    "在本笔记本中，我们将在两个环境中尝试这些方法。这两个环境都是$4 \\times 4$的网格世界，见下图。\n",
    "\n",
    "<img src=\"grid.png\" width=200>\n",
    "\n",
    "The agent can be in one of the 16 grids, so the state space is\\\n",
    "agent可以在16个网格中的一个中，因此状态空间为\n",
    "$$\n",
    "\\mathcal{S} = \\{0, 1, 2, \\ldots, 15\\}.\n",
    "$$\n",
    "In each state the agent can take one out of four actions:\\\n",
    "在每种状态下，agent可以从四个动作中选择一个:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the action space is\\\n",
    "所以动作空间是\n",
    "$$\n",
    "\\mathcal{A} = \\{ 0, 1, 2, 3\\}\n",
    "$$\n",
    "Lets now look at the two different environments.\\\n",
    "现在让我们看看这两种不同的环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Example 4.1: GridWorld-v0 <a id=\"sec4_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first environment is called `GridWorld-v0`, and is described in Example 4.1 of the textbook. To create the environment and study the state and action spaces, execute:\\\n",
    "第一个环境称为`GridWorld-v0`，在教科书的示例4.1中有描述。创造环境，研究状态和行动空间，执行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Discrete(16)\n",
      "Action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('GridWorld-v0')\n",
    "state = env.reset()\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the state space has 16 states, and there 4 actions (as mentioned above). Let us next visualize the environment:\\\n",
    "我们可以看到状态空间有16个状态，还有4个动作(如上所述)。下面让我们来看看环境:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GFFF\n",
      "FFFF\n",
      "F\u001b[41mS\u001b[0mFF\n",
      "FFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`S` is the starting state.\\\n",
    "`S`是起始状态。\n",
    "\n",
    "`F` is a state where the agent can walk.\\\n",
    "`F`是agent可以行走的状态。\n",
    "\n",
    "`G` are the two goal states. (In Example 4.1 these two states are considered to be one state).\\\n",
    "`G`是两个目标状态。(在例4.1中，这两种状态被认为是一种状态)。\n",
    "\n",
    "**Reward:** The agent receives the reward -1 for each action taken, and the episode ends when a goal state is reached. Hence, the agent should reach a goal state with as few actions as possible in order to maximize the total reward.\\\n",
    "agent会因为每次行动而获得-1的奖励，当达到目标状态时，episode就结束了。因此，为了使总reward最大化，agent应该以尽可能少的行动达到目标状态。\n",
    "\n",
    "**Dynamics:** This is a deterministic environment. So if the agent chooses the action `LEFT = 0` then it will move one step to the left if possible. If it hits a wall it will just stay in the same place.\\\n",
    "这是一个确定性的环境。因此，如果agent选择了动作`LEFT = 0`，那么它将尽可能向左移动一步。如果它撞到墙上，它会停留在相同的地方。\n",
    "\n",
    "We can try to move the agent one step to the left with the following code:\\\n",
    "我们可以尝试用以下代码将代理向左移动一步:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "GFFF\n",
      "FFFF\n",
      "\u001b[41mF\u001b[0mSFF\n",
      "FFFG\n",
      "New state: 8\n",
      "Reward: -1.0\n"
     ]
    }
   ],
   "source": [
    "new_state, reward, done, _ = env.step(LEFT) # Take action LEFT = 0\n",
    "env.render()\n",
    "print(\"New state:\", new_state)\n",
    "print(\"Reward:\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More about dynamics:** \n",
    "The GridWorld environment has the variable `env.P` that encodes the dynamics $p(s',r|s,a)$ of the environment.\\\n",
    "GridWorld环境有一个变量`env.P`，它编码环境的动态$p(s',r|s,a)$。\n",
    "\n",
    "`env.P[s][a]` will give back a list where each element is on the form `(probability, next_state, reward, terminating)`. So, if you take action `a` in state `s`, then with probability `probability` you will move to `next_state` and get the reward `reward`. `terminating` tells us if `next_state` will terminate the episode or not.\\\n",
    "`env.P[s][a]`将返回一个列表，其中每个元素都以`(probability, next_state, reward, terminating)`的形式存在。所以，如果你在状态`s`中采取行动`a`，那么你就会以概率`probability`移动到`next_state`并获得reward `reward`。`terminating`告诉我们`next_state`是否会终止episode。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 8, -1.0, False)]\n",
      "With probability 1.0 you will move to state 8 and get the reward -1.0\n"
     ]
    }
   ],
   "source": [
    "s = 9\n",
    "a = LEFT\n",
    "print(env.P[s][a])\n",
    "for p, next_s, reward, _ in env.P[s][a]: # Go through all possible transitions\n",
    "    print(\"With probability\", p, \n",
    "          \"you will move to state\", next_s, \n",
    "          \"and get the reward\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a deterministic environment, the list only contains one next state that is reached with probability 1.0.\\\n",
    "由于这是一个确定性的环境，列表中只包含一个到达概率为1.0的下一个状态。\n",
    "\n",
    "Finally we can try to run the agent that chooses between all actions with equal probability in all states. Try to run it a few time, and note that you will get different total rewards every time since the agent uses a random policy.\\\n",
    "最后，我们可以尝试运行在所有状态下在所有动作之间以相同概率进行选择的agent。尝试运行它几次，并注意每次您将获得不同的总reward，因为代理使用随机策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "GFFF\n",
      "FFFF\n",
      "FSFF\n",
      "FFF\u001b[41mG\u001b[0m\n",
      "Time step: 9\n",
      "State: 15\n",
      "Action: 2\n",
      "Total reward: -9.0\n"
     ]
    }
   ],
   "source": [
    "agent = RandomAgent(env.action_space.n, env.observation_space.n)\n",
    "run_agent(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Try to implement an optimal policy (see Figure 4.1 in the textbook).\n",
    "By changing `agent.probs` we can change the policy of the agent. Try to implement an optimal policy (see Figure 4.1 in the textbook).\\\n",
    "通过改变`agent.probs`，我们可以改变agent的策略。尝试实施最优政策（见教科书图4.1）。\n",
    "\n",
    "Note that when two possible directions are shown in the optimal policy, then you can choose between them anyway you want (either pick one with probability 1.0, or maybe pick between them with equal probability).\\\n",
    "请注意，当最优策略中显示了两个可能的方向时，您可以根据自己的意愿在其中进行选择（要么选择概率为1.0的方向，要么以相等概率从它们之中选择一个）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]\n",
      " [0.  0.  0.  1. ]\n",
      " [0.5 0.  0.  0.5]\n",
      " [0.5 0.5 0.  0. ]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.  0.  0.  1. ]\n",
      " [0.  0.  0.5 0.5]\n",
      " [0.  0.5 0.5 0. ]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.  0.  0.5 0.5]\n",
      " [0.  0.  1.  0. ]\n",
      " [0.  0.  1.  0. ]\n",
      " [0.  0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "agent.probs = np.zeros((16, 4))\n",
    "\n",
    "# Note that in each state the total\n",
    "# probability must add upp to 1.0.\n",
    "\n",
    "# Row 1\n",
    "agent.probs[1][LEFT] = 1.0\n",
    "agent.probs[2][LEFT] = 1.0\n",
    "agent.probs[3][[LEFT, DOWN]] = 0.5 # Pick between them with equal probability\n",
    "\n",
    "# Row 2\n",
    "agent.probs[4][UP] = 1.0\n",
    "agent.probs[5][[LEFT, UP]] = 0.5\n",
    "agent.probs[6][[LEFT, DOWN]] = 0.5\n",
    "agent.probs[7][DOWN] = 1.0\n",
    "\n",
    "# Row 3\n",
    "agent.probs[8][UP] = 1.0\n",
    "agent.probs[9][[RIGHT, UP]] = 0.5\n",
    "agent.probs[10][[RIGHT, DOWN]] = 0.5\n",
    "agent.probs[11][DOWN] = 1.0\n",
    "\n",
    "\n",
    "# Row 4 \n",
    "agent.probs[12][[RIGHT, UP]] = 0.5\n",
    "agent.probs[13][RIGHT] = 1.0\n",
    "agent.probs[14][RIGHT] = 1.0\n",
    "\n",
    "print(agent.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "\u001b[41mG\u001b[0mFFF\n",
      "FFFF\n",
      "FSFF\n",
      "FFFG\n",
      "Time step: 3\n",
      "State: 0\n",
      "Action: 3\n",
      "Total reward: -3.0\n"
     ]
    }
   ],
   "source": [
    "run_agent(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 The Frozen Lake <a id=\"sec4_2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment `FrozenLake-v0` [FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0/) is similar to `GridWorld-v0`, but it is stochastic.\\\n",
    "环境`FrozenLake-v0`[FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0/)类似于`GridWorld-v0`，但它是随机的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Discrete(16)\n",
      "Action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "state = env.reset()\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`S` is the starting state.\n",
    "\n",
    "`F` is a frozen surface that the agent can walk on, but it is slippery, so the movement direction only partially depends on the action.\\\n",
    "`F`是一个冻结的表面，agent可以在上面行走，但它很滑，所以移动方向只部分取决于动作。\n",
    "\n",
    "`H` is a hole. If the agent steps here, it will fall in. (The episode terminates with $0$ reward.)\\\n",
    "`H`是一个洞。如果agent进入这里，它就会掉进去。(这一episode以$0$的reward结束。)\n",
    "\n",
    "`G` is the goal. If the agents steps here, the episode terminates with reward $+1$.\\\n",
    "`G`是目标。如果agent到达这里，这一episode将以$+1$的reward结束。\n",
    "\n",
    "**Reward**: All actions not leading to the goal state gives a reward of 0. Hence, to maximize the reward the agent much reach the goal state without falling into a hole.\\\n",
    "所有不指向目标状态的行为都给予0的reward。因此，为了reward励最大化，agent要达到目标状态而不会掉进洞里。\n",
    "\n",
    "**Dynamics:** Here we also have `env.P[s][a]` to see the dynamics of the environment.\\\n",
    "这里我们也有`env.P[s][a]`来观察环境的动态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "With probability 0.3333333333333333 you will move to state 5 and get the reward 0.0\n",
      "With probability 0.3333333333333333 you will move to state 8 and get the reward 0.0\n",
      "With probability 0.3333333333333333 you will move to state 13 and get the reward 0.0\n"
     ]
    }
   ],
   "source": [
    "s = 9\n",
    "a = LEFT \n",
    "print(env.P[s][a])\n",
    "for p, next_s, reward, _ in env.P[s][a]:\n",
    "    print(\"With probability\", p, \n",
    "          \"you will move to state\", next_s, \n",
    "          \"and get the reward\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the environment is stochastic in this case, since the action `LEFT` in state 9 may take the agent either to state 5 (up into a hole!), state 8 (left) or state 13 (down), due to the slippery surface.\\\n",
    "我们可以看到，在这种情况下，环境是随机的，因为状态9中的`LEFT`行动可能会将agent带入状态5(向上进入一个洞!)、状态8(向左)或状态13(向下)，因为表面很滑。\n",
    "\n",
    "We can also try to run this environment using the random policy:\\\n",
    "我们也可以尝试使用随机策略来运行这个环境:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Time step: 2\n",
      "State: 5\n",
      "Action: 1\n",
      "Total reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "agent = RandomAgent(env.action_space.n, env.observation_space.n)\n",
    "run_agent(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that typically the agent ends up in one of the holes, and thus the total reward is typically 0 (but the expected total reward is positive, since there is a non-zero probability that we reach the goal).\\\n",
    "我们可以看到，agent通常会在其中一个洞中结束，因此总reward通常是0（但期望的总reward是正的，因为我们达到目标的概率是非0）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. MDPs and the Bellman equations <a id=\"sec5\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will study the Bellman equations for the value function a bit closer. Remember that we defined the return as\\\n",
    "在本节中，我们将进一步研究值函数的Bellman方程。记住，我们把return定义为\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n",
    "$$\n",
    "and the state-value function as\\\n",
    "状态值函数为\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}[G_t | S_{t} = s].\n",
    "$$\n",
    "The Bellman equation for the state-value function is then\\\n",
    "状态值函数的Bellman方程为\n",
    "$$\n",
    "v_\\pi(s) \n",
    "= \\sum_{a} \\pi(a|s) \\sum_{r} \\sum_{s'} p(s',r|s,a) [r + \\gamma v_\\pi(s')] \n",
    "= \\sum_{a} \\pi(a|s) q_{\\pi}(s,a)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "q_{\\pi}(s,a) \n",
    "= \\mathbb{E}_{\\pi} [G_t | S_t=s, A_t=a] \n",
    "= \\sum_{r}\\sum_{s'} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n",
    "$$\n",
    "is the action-value function.\\\n",
    "是动作值函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation:**\n",
    "In the code we will represent the state-value function $v_\\pi(s)$ as a vector $v$ with one element for each state in $\\mathcal{S}$.\n",
    "\n",
    "We will now implement functions for computing the right-hand side of the Bellman equation. \n",
    "\n",
    "**Task:**\n",
    "Complete `compute_action_value` and `Bellman_RHS`. Make sure that you understand the code.\n",
    "\n",
    "We start by a function that computes the action values $q_{\\pi}(s,a)$ given the state-value function $v_\\pi(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_action_value(env, discount, s, a, v):\n",
    "    \n",
    "    action_value = 0\n",
    "    \n",
    "    for p, next_s, reward, _ in env.P[s][a]:\n",
    "        # Loop through all possible (s', r) pairs\n",
    "        action_value += ?\n",
    "    \n",
    "    return action_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the action values, we can now compute $\\sum_{a} \\pi(a|s) q_{\\pi}(s,a)$ (the expected action value) to get the right-hand side (RHS) of the Bellman equation.\n",
    "\n",
    "For this we use `agent.probs[s][a]` $=\\pi(a|s)$, see discussion in \"Helper Functions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Bellman_RHS(env, discount, agent, s, v):\n",
    "    \n",
    "    state_value = 0\n",
    "    \n",
    "    for a in range(env.action_space.n):\n",
    "        # Loop through all possible actions\n",
    "        state_value += ?\n",
    "    \n",
    "    return state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we implement a function that, given a value function, computes the right-hand side of the Bellman equation for all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Bellman_RHS_all(env, discount, agent, v0):\n",
    "    # v0 is the given value function\n",
    "    # v will be the right-hand side of the Bellman equation\n",
    "    # If v0 is indeed the value function, then we should get v = v0.\n",
    "    \n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        v[s] = Bellman_RHS(env, discount, agent, s, v0)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Test your code on Example 4.1 (GridWorld) <a id=\"sec5_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Example 4.1 we will consider the state-value $v_{\\pi}(s)$ for the policy when each action is taken with equal probability. The discount rate is\n",
    "$$\n",
    "\\gamma = 1\n",
    "$$\n",
    "The value function for this policy is given in Figure 4.1 in the textbook (the lower left), and it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "v = np.array([[0, -14, -20, -22], \n",
    "             [-14, -18, -20, -20],\n",
    "             [-20, -20, -18, -14], \n",
    "             [-22, -20, -14, 0]]).ravel()\n",
    "\n",
    "print(\"v as vector:\", v)\n",
    "print(\"v as matrix:\\n\", v.reshape(4,4))\n",
    "\n",
    "# ravel turns the matrix into an array,\n",
    "# and with reshape we print it as a matrix again so that it is easier to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our code to see if this value function really satisfy the Bellman equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('GridWorld-v0')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "v_new = Bellman_RHS_all(env, discount, agent, v)\n",
    "print('Right-hand side of Bellman equation:\\n', v_new.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** `v` is the true value-function for the policy $\\pi$ implemented in `agent`. Hence, `v_new` (the right-hand side of the Bellman equation) should be equal to `v`. \n",
    "\n",
    "If `v_new` is not equal to `v`, go back and fix your code for `compute_action_value` and `Bellman_RHS`. Remember to re-run the code cells after you have changed the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Policy Evaluation <a id=\"sec6\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lecture 3 we learned that one way of solving the Bellman equation, is to start from an initial guess and then repeatedly update the value function by applying the right-hand side of the Bellman equation. \n",
    "\n",
    "Below is one way to implement this. The iteration will stop when the maximum change in $v$ is less than `tol` (tolerance) or the number of iterations are `max_iter`.\n",
    "\n",
    "***Note:*** For this code to work properly, your implementation of `compute_action_value` and `Bellman_RHS` must be correct. So make sure that you have tested your code first!\n",
    "\n",
    "**Task:** Make sure that you understand the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(env, discount, agent, v0, max_iter=1000, tol=1e-6):\n",
    "    \n",
    "    v_old = v0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        v_new = Bellman_RHS_all(env, discount, agent, v_old)\n",
    "        \n",
    "        if np.max(np.abs(v_new-v_old)) < tol:\n",
    "            break\n",
    "            \n",
    "        v_old = v_new\n",
    "        \n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try this on the `GridWorld-v0` example, with the uniformly random policy. We start with an initial guess $v_{\\pi}(s) = 0$ for all $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('GridWorld-v0')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "v0 = np.zeros((env.observation_space.n))\n",
    "\n",
    "v = policy_evaluation(env, discount, agent, v0)\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this (approximately) the same as the true value function in Figure 4.1? $(k=\\infty)$\n",
    "If you do not find the correct value function, make sure that your code in `compute_action_value` and `Bellman_RHS` is correct!\n",
    "\n",
    "To replicate the other parts of Figure 4.1, you can set `max_iter` in order to see how the value function looks after a few iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "v = policy_evaluation(env, discount, agent, v0, max_iter=1)\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Use `policy_evaluation` to compute the value function for `FrozenLake-v0` when the uniformly random policy is used. Use $\\gamma = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "env.render()\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "\n",
    "# Write code for computing the state-value function\n",
    "\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 In place updates <a id=\"sec6_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in Lecture 3 and in the textbook, the policy evaluation is often implemented using in place updates.\n",
    "\n",
    "This can both simplify implementation, since we do not keep two separate arrays, and it can also speed up convergence.\n",
    "\n",
    "**Task:** Complete the code in `policy_evaluation_ip`. Pseudo-code can be found in the textbook in the box on page 75. Then test your code on `GridWorld-v0` with the uniform policy to see that you still get the correct value function.\n",
    "\n",
    "***Note:*** You have already written a function that computes the right-hand side of the Bellman equation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def policy_evaluation_ip(env, discount, agent, v0, max_iter=1000, tol=1e-6):\n",
    "    \n",
    "    v = v0\n",
    "    \n",
    "    for i in range(max_iter): # Loop\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            vs = v[s]\n",
    "            \n",
    "            # Code for updating v[s]\n",
    "            \n",
    "            delta = np.max([delta, np.abs(vs-v[s])])\n",
    "            \n",
    "        if (delta < tol): # Until delta < tol\n",
    "            break\n",
    "            \n",
    "    return v    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('GridWorld-v0')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "\n",
    "v0 = np.zeros(16)\n",
    "v = policy_evaluation_ip(env, discount, agent, v0)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Policy Iteration <a id=\"sec7\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we have code for evaluating a policy, it is time to see how it can be improved. Remember that the idea is to act greedily with respect to $v_{\\pi}(s)$. That is, given $v_{\\pi}(s)$ we can compute $q_{\\pi}(s,a)$, and then the greedy (improved) policy is\n",
    "$$\n",
    "\\pi'(s) = \\text{argmax}_{a} q_{\\pi}(s,a)\n",
    "$$\n",
    "We have already written code for computing $q_{\\pi}(s,a)$ for a given $v_{\\pi}(s)$, so the only thing we have to do now is to implement the maximization.\n",
    "\n",
    "`greedy_policy` will return `a_probs` which encode a policy that is greedy with respect to `v`. That is `a_probs[s][a]` $= \\pi'(a|s)$. Make sure that you understand the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def greedy_policy(env, discount, agent, v):\n",
    "    \n",
    "    # The new policy will be a_probs\n",
    "    # We start by setting all probabilities to 0\n",
    "    # Then when we have found the greedy action in a state, \n",
    "    # we change the probability for that action to 1.0.\n",
    "    \n",
    "    a_probs = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        \n",
    "        action_values = np.zeros(env.action_space.n)\n",
    "        \n",
    "        for a in range(env.action_space.n):\n",
    "            # Compute action value for all actions\n",
    "            action_values[a] = compute_action_value(env, discount, s, a, v)\n",
    "            \n",
    "        a_max = np.argmax(action_values) # A greedy action\n",
    "        a_probs[s][a_max] = 1.0 # Always choose the greedy action!\n",
    "        \n",
    "    return a_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to improve the policy on `GridWorld-v0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('GridWorld-v0')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "\n",
    "# We first evaluate the policy\n",
    "v = np.zeros(env.observation_space.n)\n",
    "v = policy_evaluation(env, discount, agent, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "v_old = v\n",
    "\n",
    "# And then we improve the policy (act greedy w.r.t v)\n",
    "agent.probs = greedy_policy(env, discount, agent, v)\n",
    "\n",
    "# We can also evaluate the new policy \n",
    "v = policy_evaluation(env, discount, agent, v)\n",
    "\n",
    "print(\"Value of initial policy:\")\n",
    "print(v_old.reshape(4,4))\n",
    "print(\"\\nValue of improved policy:\")\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that your implementation of `compute_action_value` is correct, \n",
    "we can clearly see that the improved policy has higher value in every state. In fact, the policy is now an optimal policy. To see this, you can try to rerun the second cell above and note that the policy does not improve anymore.\n",
    "\n",
    "**Policy iteration:** However, it is not the case for all environments that the policy will converge in just one improvement. In this case we may have to improve the policy several times until it finally converge to the optimal policy. \n",
    "\n",
    "Finally, we can try to run the agent with the improved policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run_agent(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Find an optimal policy for `FrozenLake-v0`. (Note again that you may have to improve several times to reach an optimal policy!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "\n",
    "# Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Value iteration <a id=\"sec8\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the value iteration we instead start from the Bellman optimality equation\n",
    "\n",
    "$$\n",
    "v_{*}(s) = \\max_{a} q_{\\pi_*}(s,a) = \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_{*}(s')]\n",
    "$$\n",
    "\n",
    "We start with an initial guess $v_0$ and then we repeatedly compute the right-hand side of this equation, until we converge to the optimal state-value function. When we have the optimal state-value function $v_*$, we can take any policy that is greedy w.r.t $v_*$ and this will give us an optimal policy. \n",
    "\n",
    "**Task 1:** Complete the code below. Pseudo-code for the algorithm can be found on page 83 in the textbook. Note that the code for computing the action-value given $v_{\\pi}$ has already been implemented above.\n",
    "\n",
    "The `value_iteration` function will (if implemented correctly) give back the optimal value function. \n",
    "\n",
    "**Task 2:** Also add some code for computing the optimal policy given this, and try it on `FrozenLake-v0` and/or `GridWorld-v0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, discount, agent, v0, max_iter=1000, tol=1e-6):\n",
    "    \n",
    "    v = v0\n",
    "    \n",
    "    for i in range(max_iter): # Loop\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            vs = v[s]\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Code for updating v[s]\n",
    "                \n",
    "            \n",
    "            ##\n",
    "            \n",
    "            delta = np.max([delta, np.abs(vs-v[s])])\n",
    "            \n",
    "        if (delta < tol): # Until delta < tol\n",
    "            break\n",
    "            \n",
    "    return v    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "\n",
    "v0 = np.zeros(env.observation_space.n)\n",
    "v = value_iteration(env, discount, agent, v0)\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Code for finding the greedy policy w.r.t v, and to run it on FrozenLake-v0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
