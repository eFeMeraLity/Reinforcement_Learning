{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tinkering Notebook 3b: Model-free control\n",
    "\n",
    "In this notebook we will test model-free control. We will implement and use SARSA and Q-learning. If you are interested you can also try to implement Monte Carlo-control, but this method will not work very well on the environments we study in this notebook.\\\n",
    "在本笔记本中，我们将测试无模型控制。我们将实施和使用SARSA和Q-learning。如果你有兴趣，你也可以尝试实现Monte Carlo控制，但这种方法将不能很好地工作在我们在本笔记本中研究的环境。\n",
    "\n",
    "**Lecture 5 - Model-free control**\n",
    "__Recommended Reading:__ 5.2-5.4, 6.4-6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "* ### [1. Imports](#sec1)\n",
    "* ### [2. A note on time limits in OpenAI gym](#sec2)\n",
    " * #### [2.1 \\*How to handle actual time limits](#sec2_1)\n",
    "* ### [3. Helper functions](#sec3)\n",
    "* ### [4. SARSA](#sec4)\n",
    " * #### [4.1 SARSA on Example 3.5](#sec4_1)\n",
    " * #### [4.2 Example 6.5: Windy Grid World](#sec4_2)\n",
    "* ### [5. Q-learning](#sec5)\n",
    " * #### [5.1 Example 6.6.](#sec5_1)\n",
    "* ### [6. A note on exploration](#sec6)\n",
    "* ### [7. \\* MountainCar-v0](#sec7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports <a id=\"sec1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "package = \"/Users/lmf/PycharmProjects/MSc_DS/Python/Package/gym-gridworld\"\n",
    "sys.path.append(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "import gym_gridworld\n",
    "import gymgrid\n",
    "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. A note on time limits in OpenAI gym <a id=\"sec2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an episodic MDP, the episode will end when (and only when) a terminal state is reached. In a continuing environment, there are no terminal states so it will never stop.\\\n",
    "在episodic MDP中，当（且仅当）到达一个终止状态时，该episode将结束。在一个持续的环境中，没有终止状态，所以它永远不会停止。\n",
    "\n",
    "However, some of the OpenAI gym environments we will study in this notebook stops after a time limit (typically 200 time steps) even if we do not reach a terminal state. It should be noted that this does not mean that the episode has ended in the MDP-sense, since we did not reach a terminal state!\\\n",
    "然而，我们将在本笔记本中学习的一些OpenAI gym环境在一个时间限制（通常是200个时间步）后停止，即使我们没有达到一个终止状态。需要指出的是，这并不意味着这一事件已经在MDP意义上结束了，因为我们没有达到一个终止状态！\n",
    "\n",
    "This is mainly a problem in Monte-Carlo control. Here we first collect the data of a full episode before we do any learning. If the OpenAI gym environment stops due to the time limit, we did not actually get a full episode trajectory!\\\n",
    "这主要是Monte-Carlo控制中的一个问题。在这里，在我们进行任何学习之前，我们首先收集一个完整的episode的数据。如果OpenAI gym的环境由于时间限制而停止，我们实际上并没有得到完整的episode轨迹！\n",
    "\n",
    "In this notebook we will look at SARSA and $Q$-learning. In these cases it is not a problem that the episode terminates prematurely, since we only use 1-step transitions in the updates.\\\n",
    "在本笔记中，我们将学习SARSA和$Q$-learning。在这些情况下，episode提前终止并不是问题，因为我们在更新中只使用了1步转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 \\*How to handle actual time limits <a id=\"sec2_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the environments we will study in this notebook it makes sense to treat the time limits in OpenAI gym as artificially stopping the environment before the episode was done.\\\n",
    "在我们将在本笔记本中学习的环境中，将OpenAI gym的时间限制视为在episode结束前人为地停止环境是有意义的。\n",
    "\n",
    "But there are tasks where the time-limit itself is important. Lets say the task is for a humanoid robot to run as far as possible in 60s. Optimizing this would lead to different behavior depending on the time: In the beginning it is important for the robot to not fall down, but towards the end of the 60s it may be a good idea to be more aggressive even if this would mean that the robot falls down after the 60s are over (throwing itself over the finish line).\\\n",
    "但有些任务的时限本身很重要。假设任务是让一个人形机器人在60秒内跑得尽可能远。优化这将导致取决于时间的不同行为：一开始机器人不跌倒是很重要的，但到60秒末，变得更积极可能是一个好主意，即使这将意味着机器人在60s结束后跌倒（将其自身扔超过终点线）。\n",
    "\n",
    "To encode this into an MDP, we would have to include e.g. \"time left\" in the state. Then any state with \"time left = 0\" is a terminating state. Without this the state would not have the Markov-property, and a policy $\\pi(a|s)$ could not know if we are in the beginning of the race or if we are close to the end of the 60s.\\\n",
    "要将其编码为MDP，我们必须包括例如在某状态的“剩余时间”。那么任何“剩余时间 = 0”的状态都是终止状态。没有这一点，状态就没有Markov性质，而且一个策略$\\pi(一个|s)$就不能知道我们是在比赛的开始还是接近60秒末。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Helper functions <a id=\"sec3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here define a function `test_policy` that can be used to see the policy we have found in action. It takes an agent and an environment, and runs the agents policy on the environment for one episode while rendering (no learning). It renders the environment and returns the total reward received. If you do not want to render the environment, add `render = False` as an argument. It will run until the episode is done or `max_steps` (default 40) has been taken.\\\n",
    "我们在这里定义了一个函数`test_policy`，它可以用来查看我们发现的实际策略。它需要一个agent和一个环境，并在渲染（不学习）的情况下，在环境中运行一次episode的agent策略。它呈现环境并返回所获得的全部reward。如果你不想渲染环境，可以添加`render = False`作为参数。它将一直运行，直到该episode完成或执行了`max_steps`（默认为40）次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Code:__ Funtion `test_policy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_policy(agent, env, wait = 0.1, max_steps = 40, render = True):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done and step < max_steps:\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        \n",
    "        if render:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            # Show some information\n",
    "            print(\"Time step:\", step)\n",
    "            print(\"Reward:\", reward)\n",
    "            print(\"Total reward:\", total_reward)\n",
    "            time.sleep(wait)\n",
    "    env.close()\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SARSA <a id=\"sec4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will implement and try out SARSA-control.\\\n",
    "在本节中，我们将实现并尝试SARSA-控制。\n",
    "\n",
    "We first implement the function `train_sarsa` with the arguments\\\n",
    "我们首先实现函数`train_sarsa`，其具有下述参数\n",
    "* `agent` - Should be an object with the methods `act` that implements the current policy, and `learn` that is used to update the estimated `Q`.\\\n",
    "应该是一个对象，其具有实现当前策略的`act`方法，和用于更新估计`Q`的`learn`方法。\n",
    "* `env` - The environment\\\n",
    "环境\n",
    "* `n_episodes` - The number of episodes we should use to train the agent\n",
    "我们应该用来训练agent的episode数目\n",
    "* `max_steps` - If the total number of steps taken is larger than `max_steps` we will stop the training even if we have not finished `n_episodes`.\\\n",
    "如果总步骤数大于`max_steps`，即使没有完成`n_episodes`，我们也会停止训练。\n",
    "\n",
    "The function also computes an array `steps` that shows the total number of time steps that have been used after each episode, and `total_rewards` that gives the total reward for each episode.\\\n",
    "该函数还计算了一个数组`steps`，它显示了每一个episode之后使用的总时间步长，以及`total_rewards`，它给出了每一个episode的总奖励。\n",
    "\n",
    "**Task:** Read the code, and compare it with the pseudo-code for SARSA seen in the slides of Lecture 3. Make sure that you understand it.\\\n",
    "阅读代码，并将其与第三节课幻灯片中看到的SARSA伪代码进行比较。确保你理解了它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_sarsa(agent, env, n_episodes, max_steps = 50000):\n",
    "    step = 0\n",
    "    steps = np.zeros(n_episodes) # Steps after each episode\n",
    "    total_rewards = np.zeros(n_episodes)\n",
    "    for i in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        action = agent.act(state)\n",
    "        rewards = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_next, reward, done, info = env.step(action)\n",
    "            action_next = agent.act(state_next)\n",
    "            agent.learn(state, action, reward, state_next, action_next)\n",
    "            state = state_next\n",
    "            action = action_next\n",
    "            step += 1\n",
    "            rewards += reward\n",
    "            \n",
    "            if step > max_steps:\n",
    "                return total_rewards, steps\n",
    "            \n",
    "        steps[i] = step\n",
    "        total_rewards[i] = rewards\n",
    "    return total_rewards, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define the class `SARSA` that implements the agent.\\\n",
    "接下来我们定义实现agent的类`SARSA`。\n",
    "\n",
    "## **Task:** Implement SARSA\n",
    "1. Implement a policy in `act` that is $\\varepsilon$-greedy w.r.t `self.Q`. ($\\varepsilon$ = `self.epsilon`)\\\n",
    "在`act`中实现一个对`self.Q`的$\\varepsilon$贪婪的策略。（$\\varepsilon$ = `self.epsilon`）\n",
    "\n",
    "2. Implement the SARSA-update of $Q$ in `learn`.\\\n",
    "在`learn`中实现$Q$的SARSA更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SARSA():\n",
    "    def __init__(self, n_states, n_actions, gamma, alpha, epsilon):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "        \n",
    "    def act(self, state):\n",
    "        # You can use np.random.choice(self.n_actions) to get a random action\n",
    "        # Implement epsilon-greedy policy\n",
    "        if np.random.random() > self.epsilon:\n",
    "            action = np.argmax(self.Q[state])\n",
    "        else:\n",
    "            action = np.random.choice(self.n_actions) # Random action\n",
    "        return action\n",
    "            \n",
    "    def learn(self, s, a, r, s_next, a_next):\n",
    "        # Implement the TD(0) update of Q (see equation (6.7) in textbook)\n",
    "        self.Q[s][a] += self.alpha * (r + self.gamma*self.Q[s_next][a_next] - self.Q[s][a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 SARSA on Example 3.5 <a id=\"sec4_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will try SARSA on the `GridWorld-5x5-AB-v0`. The environment is described in Example 3.5 in the textbook, and the optimal policy is shown in Figure 3.5. This is a continuing environment, so there is no terminal state. However, as discussed above, the OpenAI Gym environment will still stop after 200 time steps due to a time limit.\\\n",
    "这里，我们将在`GridWorld-5x5-AB-v0`上尝试SARSA。环境的描述如教材中的例3.5所示，最优策略如图3.5所示。这是一个持续的环境，所以没有终止状态。但是，如上所述，由于时间限制，OpenAI Gym的环境在200个时间步后仍然会停止。\n",
    "\n",
    "**Note on exploration:** When a relatively good policy is found, the agent will keep moving from $A'$ to $A$, with the occasional random action due to the $\\varepsilon$-greedy policy. Hence, we will mainly learn about these states. However, when we reset the environment we restart at a random state, and thus resetting the environment helps with exploration. Here we will reset the environment after 200 time steps.\\\n",
    "当发现一个相对好的策略时，agent将不断从$A'$移动到$A$，由于$\\varepsilon$贪婪粗略，偶尔会有随机动作。因此，我们将主要学习这些状态。然而，当我们重置环境时，我们会以随机状态重新开始，因此重置环境有助于探索。在这里，我们将在200个时间步后重置环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment the discount is $\\gamma = 0.9$. Lets try to the step size $\\alpha = 0.1$ and exploration rate $\\varepsilon = 0.2$.\\\n",
    "在这个环境中，折现率是$\\gamma = 0.9$。让我们尝试步长$\\alpha = 0.1$和探索率$\\varepsilon = 0.2$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('GridWorld-5x5-AB-v0') # the same as in Example 3.5\n",
    "agent = SARSA(env.observation_space.n, env.action_space.n, gamma = 0.9, alpha = 0.1, epsilon = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train train the agent. \n",
    "\n",
    "**Note:** If you run the cell below again without resetting the agent, you will continue from your already estimated $Q$. That is, running the cell below two times effectively doubles the number of time steps.\\\n",
    "如果您再次运行下面的单元格而不重新设置agent，则将从已经估算的$Q$继续。也就是说，运行下面的单元格两次，有效地增加了一倍的时间步长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_resets = 1000 # Train with n_resets * 200 time steps\n",
    "rewards, _ = train_sarsa(agent, env, n_resets, max_steps = n_resets*200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next plot the total reward for each 200 times steps (before we reset). This should hopefully show that the total reward may start at a very small level, but that it will increase as the agent learns more.\\\n",
    "接下来我们将绘制每个200次步骤的总reward（在我们重置之前）。这将有望表明，总reward可能从一个非常小的水平开始，但它将随着agent学习的更多而增加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, n_resets+1), rewards)\n",
    "plt.xlabel(\"Number of resets\")\n",
    "plt.ylabel(\"Total reward over 200 time steps\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next test to see how the policy works in practice. Note that we are still using the $\\varepsilon$-greedy policy in `agent`, so it will choose a random action with probability $\\varepsilon$.\\\n",
    "我们接下来要测试一下该策略在实践中是如何运作的。注意，我们仍然在`agent`中使用$\\varepsilon$贪婪策略，因此它将以概率$\\varepsilon$选择一个随机行为。\n",
    "\n",
    "The main reason for using an $\\varepsilon$-greedy policy is to ensure that the agent explores while it trains. Here we want to evaluate the policy that the agent has learned, and it thus makes sense to set the exploration to zero ($\\varepsilon = 0$) and hence use the greedy policy w.r.t the estimated $Q$. (However, if you will continue to train after this, you should add exploration again)\\\n",
    "使用$\\varepsilon$贪婪策略的主要原因是确保agent在训练时进行探索。在这里，我们希望评估agent已经学习到的策略，因此将探索设置为零（$\\varepsilon = 0$）是有意义的，因此使用关于估计$Q$的贪婪策略。（然而，如果你想在此之后继续训练，你应该再次添加探索）\n",
    "\n",
    "**Task:** Test your policy both with the $\\varepsilon$-greedy policy you trained your policy with, and the greedy policy ($\\varepsilon = 0$).\\\n",
    "测试您的策略，使用$\\varepsilon$贪婪策略和贪婪策略($\\varepsilon = 0$)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "FAFBF\n",
      "FFFFF\n",
      "FFFbF\n",
      "F\u001b[41mF\u001b[0mFFF\n",
      "FaFFF\n",
      "Time step: 40\n",
      "Reward: 0.0\n",
      "Total reward: 80.0\n"
     ]
    }
   ],
   "source": [
    "agent.epsilon = 0\n",
    "test_policy(agent, env, max_steps=40)\n",
    "agent.epsilon = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only trained with $100 \\times 200$ time steps, it is common that SARSA find a greedy policy that sometimes ends up going from b to B over and over again. This is not optimal, since going from $a$ to $A$ gives a higher reward. Try to increase the number of time steps you use in training to see if you can get a better result!\\\n",
    "如果你只用$100 \\times 200$的时间步来训练，SARSA通常会发现一个贪婪的策略，有时会一遍又一遍地从b到B。这不是最理想的，因为从$a$到$A$会带来更高的奖励。试着增加你在训练中使用的时间步数，看看你是否能得到更好的结果!\n",
    "\n",
    "We can also visualize the greedy policy w.r.t $Q$ with the following function.\\\n",
    "我们还可以用下面的函数来可视化关于$Q$的贪婪策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Code__: Function `render_greedy_policy` to visualize the greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def render_greedy_policy(Q):\n",
    "    # Prints an illustration of the greedy policy with respect to Q\n",
    "    n_states = Q.shape[0]\n",
    "    greedy = np.full(n_states, 'L') \n",
    "    for s in range(n_states):\n",
    "            a = np.argmax(Q[s,:])\n",
    "            if a == 0:\n",
    "                greedy[s] = 'L'\n",
    "            elif a == 1:\n",
    "                greedy[s] = 'D'\n",
    "            elif a == 2:\n",
    "                greedy[s] = 'R'\n",
    "            elif a == 3:\n",
    "                greedy[s] = 'U'\n",
    "\n",
    "    print(greedy.reshape(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['D' 'D' 'L' 'L' 'L']\n",
      " ['R' 'U' 'L' 'L' 'L']\n",
      " ['R' 'U' 'U' 'U' 'L']\n",
      " ['R' 'U' 'U' 'U' 'U']\n",
      " ['U' 'U' 'U' 'L' 'L']]\n"
     ]
    }
   ],
   "source": [
    "render_greedy_policy(agent.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Compare the greedy policy found by SARSA with the optimal policy found in Figure 3.5 in the textbook. If the policy you have found is not optimal, then train it some more!\\\n",
    "比较SARSA发现的贪心策略与教材图3.5中发现的最优策略。如果你发现的策略不是最优的，那么再多训练一下!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Example 6.5: Windy Grid World <a id=\"sec4_2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will try out Example 6.5 in the textbook. To do so, we use the `gymgrid` package.\\\n",
    "本节我们将试用教材中的例6.5。为此，我们使用`gymgrid`包。\n",
    "    \n",
    "In this environment we have a $7 \\times 10$ grid, with a starting point and a goal. It is a windy grid world, since in some of the columns the agent will get pushed up when it takes an action. See Example 6.5 for a detailed explanation.\\\n",
    "在这个环境中，我们有一个$7 \\times 10$的网格，有一个起点和一个目标。这是一个有风的网格世界，因为在一些列中，当agent采取行动时，它将被推up。详细解释请参见示例6.5。\n",
    "\n",
    "Lets look at the state and action spaces of the environment.\\\n",
    "让我们来看看环境的状态和行动空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  Discrete(70)\n",
      "Action space:  Discrete(8)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('windy-grid-world-v0')\n",
    "print(\"State space: \", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **State space**: The 70 states corresponds to the 70 different possible positions of the agent.\\\n",
    "70个状态对应于70个agent可能的不同位置。\n",
    "* **Action space**: This environment implements the version in Exercise 6.9. The actions corresponds to\\\n",
    "这个环境实现了示例6.9中的版本。动作对应于\n",
    "0 - West<br>\n",
    "1 - East<br>\n",
    "2 - North<br>\n",
    "3 - South<br>\n",
    "4 - North west<br>\n",
    "5 - North east<br>\n",
    "6 - South west<br>\n",
    "7 - South east<br>\n",
    "In order to implement Example 6.5 you should only allow the agent to use the first four actions. To try out Exercise 6.9 you allow the agent to use all actions. This means that we should use four possible actions in SARSA to get Example 6.5.\\\n",
    "为了实现示例6.5，您应该只允许agent使用前四个操作。要尝试示例6.9，您允许agent使用所有操作。这意味着我们应该在SARSA中使用四种可能的操作来获得示例6.5。\n",
    "\n",
    "* **Reward**: The agent gets a reward of -1 for each action, and we consider this to be an undiscounted task ($\\gamma = 1$), so the objective of the agent is just to reach the goal in as few steps as possible. For Example 6.5 (four actions) the optimal policy uses 15 steps. This environment do not have any time-limits implemented, so it will continue until the terminal state (goal) is reached!\\\n",
    "agent每次行动都会得到-1的奖励，我们认为这是一个未折现的任务($\\gamma = 1$)，所以agent的目标是尽可能少的步骤达到目标。例如，6.5（四个动作）最优策略使用了15个步骤。这个环境没有实现任何时间限制，所以它会一直运行，直到达到终止状态（目标）！\n",
    "\n",
    "We will first try out the agent using the exact same setting as in Example 6.5 ($\\alpha = 0.5$ and $\\varepsilon=0.1$), training the agent using 170 episodes. However note that, due to the use of random actions, you will not get exactly the same result as in Example 6.5 every time. If you are unlucky, you may even get a policy that have a hard time reaching the goal.\\\n",
    "我们将首先使用示例6.5中完全相同的设置（$\\alpha = 0.5$和$\\varepsilon=0.1$）来测试agent，使用170个episode来训练agent。但是请注意，由于使用了随机操作，您不会每次都得到与示例6.5完全相同的结果。如果你不幸，你甚至可能会得到一份很难达到目标的政策。\n",
    "Note that we set the number of actions to 4, to ensure that the agent only use West, East, North or South.\\\n",
    "注意，我们将操作数设置为4，以确保agent只使用西、东、北或南。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 6.5\n",
    "agent = SARSA(env.observation_space.n, 4, gamma = 1, alpha = 0.5, epsilon = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6.9\n",
    "agent = SARSA(env.observation_space.n, env.action_space.n, gamma = 1, alpha = 0.5, epsilon = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train the agent. \n",
    "\n",
    "**Note:** If you rerun the cell below, without resetting (creating a new) agent, you will start from the Q-table you previously trained. That is, you will effectively double the number of training episodes.\\\n",
    "如果您重新运行下面的单元格，而没有重新设置（创建一个新的）agent，那么您将从以前训练过的Q表开始。也就是说，你将有效地将培训集的数量翻倍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_episodes = 200\n",
    "rewards, steps = train_sarsa(agent, env, n_episodes); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a plot similar to the one in Example 6.5, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(steps, range(steps.shape[0]))\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('Episodes');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the policy in action we can use `test_policy`. \n",
    "\n",
    "As in the previous example, `test_policy` will use the $\\varepsilon$-greedy policy. To also see the greedy policy w.r.t $Q$ in action, first set `agent.epsilon = 0`.\\\n",
    "在前面的例子中，test_policy将使用$\\varepsilon$贪婪策略。为了也要看关于$Q$的贪婪策略在行动，首先设置`agent.epsilon = 0`。\n",
    "\n",
    "**Task:** Run the $\\varepsilon$-greedy you trained with a few times, and then try the greedy policy ($\\varepsilon = 0$). If your policy did not succeed, try to train it again (or just increase the number of training episodes).\\\n",
    "运行您多次训练过的$\\varepsilon$贪婪策略，然后尝试贪婪策略（$\\varepsilon = 0$）。如果你的策略没有成功，试着再训练一次（或者增加训练的次数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step: 7\n",
      "Reward: -1\n",
      "Total reward: -7\n"
     ]
    }
   ],
   "source": [
    "agent.epsilon = 0\n",
    "test_policy(agent, env, max_steps=40)\n",
    "agent.epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "\n",
    "Do Exercise 6.9 (i.e. let the agent use all possible 8 actions instead of just the first 4). Before you try this out, look at the example and see if you can find a better policy when all actions are allowed. How many steps are needed with all actions available? Then try to train an agent using SARSA to find this policy.\\\n",
    "执行练习6.9（即让agent使用所有可能的8个动作，而不只是前4个）。在你尝试之前，看看这个例子，看看你是否能找到一个更好的策略，当所有动作都被允许时。所有可用的操作需要多少步骤？然后试着训练一个使用SARSA的代理来找到这个策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Q-learning <a id=\"sec5\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement $Q$-learning. We first define the function `train_q`. It is very similar to `train_sarsa`, but for $Q$-learning we do not need `action_next` when we update the $Q$-function.\\\n",
    "现在我们实现$Q$-learning。首先定义函数`train_q`。它非常类似于`train_sarsa`，但是是关于$Q$-learning的，我们在更新$Q$-函数时不需要`action_next`。\n",
    "\n",
    "**Task:** Look through the code, and compare it with the pseudo-code in the slides of Lecture 3.\\\n",
    "浏览代码，并与第三节课幻灯片中的伪代码进行比较。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Code:__ Function `train_q`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_q(agent, env, n_episodes, max_steps = 50000):\n",
    "    step = 0\n",
    "    steps = np.zeros(n_episodes) # Steps after each episode\n",
    "    total_rewards = np.zeros(n_episodes)\n",
    "    for i in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        rewards=0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            state_next, reward, done, info = env.step(action)\n",
    "            agent.learn(state, action, reward, state_next)\n",
    "            state = state_next\n",
    "            step += 1\n",
    "            rewards += reward\n",
    "            \n",
    "            if step > max_steps:\n",
    "                return steps, rewards\n",
    "            \n",
    "        steps[i] = step\n",
    "        total_rewards[i] = rewards\n",
    "    return total_rewards, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the class `QAgent`. Note that the goal of $Q$-learning is to estimate the optimal $Q$-function while running a different behavioral policy. Here we implement the behavioral policy ($\\varepsilon$-greedy w.r.t to current estimate of $Q$) in `act`.\\\n",
    "接下来我们定义类`QAgent`。注意，$Q$-learning的目标是在运行不同的行为策略时估计最优的$Q$-函数。这里我们在`act`中实现了行为策略（关于$Q$当前估计$\\varepsilon$贪婪策略）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task:** Implement Q-learning (Class `QAgent`)\n",
    "\n",
    "1. Implement the behavior policy in `act`. That is $\\varepsilon$-greedy w.r.t `self.Q`. ($\\varepsilon$ = `self.epsilon`)\n",
    "\n",
    "2. Implement the Q-learning update in `learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    def __init__(self, n_states, n_actions, gamma, alpha, epsilon):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "        \n",
    "        \n",
    "    def act(self, state):\n",
    "        # Implement the self.epsilon-greedy policy\n",
    "        if np.random.random() > self.epsilon:\n",
    "            action = np.argmax(self.Q[state])\n",
    "        else:\n",
    "            action = np.random.choice(self.n_actions) # Random action\n",
    "        return action\n",
    "            \n",
    "            \n",
    "    def learn(self, s, a, r, s_next):\n",
    "        # Implement the Q-learning update\n",
    "        maxQ = max(self.Q[s_next])\n",
    "        self.Q[s][a] += self.alpha * (r + self.gamma*maxQ - self.Q[s][a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Example 6.6. <a id=\"sec5_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try out $Q$-learning and SARSA on the environment described in Example 6.6. Here the agent should go from start to goal, while avoiding the cliff region. If the agent steps into the cliff region it gets a reward of -100 and is returned to the start.\\\n",
    "现在我们将在示例6.6中描述的环境中尝试$Q$-learning和SARSA。在这里，agent应该从起点到目标，同时避免悬崖区域。如果代理进入悬崖区域，它将获得-100的奖励，并返回开始。\n",
    "        \n",
    "Lets look at the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  Discrete(48)\n",
      "Action space:  Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('cliff-v0')\n",
    "print(\"State space: \", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **State space:** One state for each of the possible agent positions.\n",
    "* **Action space:** The actions are defined as in the windy grid world. That is <br>\n",
    "0 - West<br>\n",
    "1 - East<br>\n",
    "2 - North<br>\n",
    "3 - South<br>\n",
    "* **Reward:** -1 for each step, except when the agent enters the cliff region resulting in a -100 reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train one agent using $Q$-learning and one agent using SARSA.\\\n",
    "我们将使用$Q$-learning训练一个agent，使用SARSA训练一个agent。\n",
    "\n",
    "Both uses $\\gamma = 1$, $\\alpha = 0.3$ and $\\varepsilon = 0.1$.\\\n",
    "两者都使用$\\gamma = 1$， $\\alpha = 0.3$和$\\varepsilon = 0.1$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agentQ = QAgent(env.observation_space.n, env.action_space.n, gamma=1, alpha=0.3, epsilon=0.1)\n",
    "agentSARSA = SARSA(env.observation_space.n, env.action_space.n, gamma=1, alpha=0.3, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train both agent for e.g. 1000 episodes\\\n",
    "训练两个agent，如1000个episode。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_q(agentQ, env, n_episodes = 1000)\n",
    "train_sarsa(agentSARSA, env, n_episodes = 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now test to run your two policies. \n",
    "\n",
    "**Note:** In both cases you use the `act`-method that is implemented with an $\\varepsilon$-greedy policy. To use the greedy policy w.r.t $Q$ we first have to set $\\varepsilon = 0$.\\\n",
    "在这两种情况下，您都使用了通过$\\varepsilon$贪婪策略实现的`act`方法。要使用关于$Q$的贪婪策略，我们首先必须设置$\\varepsilon = 0$。\n",
    "\n",
    "**Task:** Try to run both with $\\varepsilon$-greedy and greedy policies ($\\varepsilon = 0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step: 17\n",
      "Reward: 0\n",
      "Total reward: -16\n"
     ]
    }
   ],
   "source": [
    "#agentQ.epsilon = 0\n",
    "total_reward = test_policy(agentQ, env, max_steps=100)\n",
    "#agentQ.epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step: 20\n",
      "Reward: 0\n",
      "Total reward: -19\n"
     ]
    }
   ],
   "source": [
    "#agentSARSA.epsilon = 0\n",
    "total_reward = test_policy(agentSARSA, env, max_steps=100)\n",
    "#agentSARSA.epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the two policies behaves quite differently. SARSA tries to learn the best $\\varepsilon$-soft policy while $Q$-learning tries to learn the optimal $q_*$.\\\n",
    "我们可以看到，这两种策略的表现相当不同。SARSA尝试学习最好的$\\varepsilon$-soft策略，而$Q$-learning尝试学习最优的$q_*$策略。\n",
    "\n",
    "In this example, the $Q$-learning algorithm will thus try to take as short path to the goal as possible. However, the SARSA-algorithm looks at $\\varepsilon$-soft policies, and thus also has to take into account that there is a non-zero probability that a non-greedy action is taken. Since the cost of moving into the cliff region is so large, it will take a safer path to ensure that a non-optimal action will not lead into the cliff.\\\n",
    "在这个例子中，$Q$-learning算法将尽可能地采用最短的路径达到目标。然而，SARSA算法考虑$\\varepsilon$-软策略，因此也必须考虑采取非贪婪行为的概率是非零。由于移动到悬崖区域的成本是如此之大，它将采取更安全的路径，以确保非最佳行动不会导致悬崖。\n",
    "\n",
    "### **Task:** Comparing $Q$-learning and SARSA\n",
    "**Task:** \n",
    "Below is a code that will run both the policies learned from $Q$-learning and SARSA 1000 times. It then prints the mean reward.\\\n",
    "下面的代码将运行从$Q$-learning和SARSA中学到的策略1000次。然后它会打印出平均reward。\n",
    "\n",
    "Try to run the code both using the $\\varepsilon$-greedy policies and the greedy policy ($\\varepsilon = 0$). We can see that if the policy is $\\varepsilon$-soft then the $Q$-learning version does worse because it more often moves into the cliff region. However, when we set $\\varepsilon=0$ then $Q$-learning is best because it takes the shortest path.\\\n",
    "尝试使用$\\varepsilon$贪婪策略和贪婪策略（$\\varepsilon = 0$）运行代码。我们可以看到，如果策略是$\\varepsilon$-soft，那么$Q$-learning版本的表现更差，因为它更经常地移动到悬崖区域。然而，当我们设置$\\varepsilon=0$时，$Q$-learning是最好的，因为它选择了最短的路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward Q:  -49.882\n",
      "Mean reward SARSA:  -21.434\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 1000\n",
    "reward_Q = np.zeros(n_episodes)\n",
    "reward_SARSA = np.zeros(n_episodes)\n",
    "#agentQ.epsilon = 0\n",
    "#agentSARSA.epsilon = 0\n",
    "for k in range(n_episodes):\n",
    "    reward_Q[k] = test_policy(agentQ, env, max_steps=100, render=False)\n",
    "    reward_SARSA[k] = test_policy(agentSARSA, env, max_steps=100, render=False)\n",
    "\n",
    "#agentQ.epsilon = 0.1\n",
    "#agentSARSA.epsilon = 0.1\n",
    "    \n",
    "print(\"Mean reward Q: \", np.mean(reward_Q))\n",
    "print(\"Mean reward SARSA: \", np.mean(reward_SARSA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. A note on exploration <a id=\"sec6\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have used $\\varepsilon$-greedy policies to ensure that we always continue to explore during training. However, there are other ways to get exploration and we will discuss them later in the course.\\\n",
    "在本笔记本中，我们使用了$\\varepsilon$贪婪策略，以确保我们总是在培训期间继续探索。然而，还有其他方法可以进行探索，我们将在稍后的课程中讨论它们。\n",
    "\n",
    "An easy way to at get at least temporary exploration would be to initialize your $Q$-function with large values compared to the best possible return (except for terminating states which should be initialized to 0).\\\n",
    "一种简单的方法来获得至少是暂时的探索，那就是初始化你的$Q$-函数，使用大的值，而不是最好的返回值(除了应该初始化为0的终止状态)。\n",
    "\n",
    "This would mean that state/action pairs that we have never seen before will have a large estimated value, and therefor the agent is encouraged to visit them.\\\n",
    "这意味着我们以前从未见过的状态/操作对将有一个很大的估计值，因此鼓励agent访问它们。\n",
    "\n",
    "For example, in the Windy GridWorld, the rewards are always negative. We initialize $Q$ with zeros. When we now are in a state and try an action, we will update the $Q$-value for this state/action pair to something negative. Next time we come to this state, the actions we have not tried yet all have estimated value 0, so they will look better!\\\n",
    "例如，在风大的网格世界中，奖励总是消极的。我们用0初始化$Q$。当我们现在处于一个状态并尝试一个操作时，我们将更新这个状态/操作对的$Q$-value为负值。下次我们来到这个状态的时候，我们还没有尝试过的动作都估计值为0，所以它们看起来会更好!\n",
    "\n",
    "\n",
    "**Task:** Try to train both `windy-grid-world-v0` and `GridWorld-5x5-AB-v0` with $\\varepsilon = 0$. Use the discussion above to explain the results.\\\n",
    "尝试用$\\varepsilon = 0$训练`windy-grid-world-v0`和`GridWorld-5x5-AB-v0`。使用上面的讨论来解释结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. \\* MountainCar-v0 <a id=\"sec7\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in the course we have looked at environment with a finite number of states and actions. Hence we can represent the $Q$-function as a matrix where each element corresponds to a state/action-pair. \n",
    "\n",
    "If the state (or action) space is continuous (infinitely many states) this is not possible. We will start looking in to these types of problems in Lecture 6. \n",
    "\n",
    "However, already now we can mention one simple trick that can sometimes work in these cases - namely discretization of the state space. \n",
    "\n",
    "As an example we look at the `MountainCar-v0` environment studied in Tinkering Notebook 1. Here the state contains two elements: the position and velocity of the car. \n",
    "\n",
    "**Idea:** We divide the state space into a number of discrete states as in the image below.\n",
    "<center><img src=\"./grid.png\"></center>\n",
    "Hence, every position/velocity pair that ends up in tile 4 is considered to be in state 4. An implication of this is that we learn the same $Q$-value for any position/velocity pair in tile 4 etc. Hence, the $Q$-values are now only approximations of the true $Q$-values, but if the grid is fine enough we may still be able to learn a good policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `toDiscreteState` can be used to divide the 2-dimensional state space of MountainCar into a tiles[0]  ×  tiles[1] grid. Given a 2-dimensional state of MountainCar it will return an integer representing the discretized state. It will be enough to use a  30×30  grid, i.e., 900 states. (It may also work well with fewer states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def toDiscreteState(state, tiles=np.array([30,30])):\n",
    "    max_range = np.array([1.2, 0.07]) #  -1.2 <= pos <= 1.2, -0.07 <= vel <= 0.07\n",
    "    \n",
    "    state_tile = np.floor((tiles)*(state+max_range)/(2*max_range))\n",
    "    \n",
    "    if state_tile[0] >= tiles[0]:\n",
    "        state_tile[0] = tiles[0]-1\n",
    "    if state_tile[1] >= tiles[1]:\n",
    "        state_tile[1] = tiles[1]-1\n",
    "        \n",
    "    return int(state_tile[0]+state_tile[1]*tiles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `train_car` below can be used to train a SARSA-agent using the discretized states. It is implemented just as `train_sarsa` but uses the discreteized states instead of the true continuous states.\n",
    "\n",
    "We also define `test_car` that is similar to `test_policy` above, but uses `toDiscreteState`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_car(agent, env, n_episodes):\n",
    "    total_rewards = np.zeros(n_episodes)\n",
    "    for i in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        state = toDiscreteState(state) # To discrete state!\n",
    "        action = agent.act(state)\n",
    "        rewards=0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_next, reward, done, info = env.step(action)\n",
    "            state_next = toDiscreteState(state_next) # To discrete state!\n",
    "            action_next = agent.act(state_next)\n",
    "            agent.learn(state, action, reward, state_next, action_next)\n",
    "            state = state_next\n",
    "            action = action_next\n",
    "            rewards += reward\n",
    "            \n",
    "        total_rewards[i] = rewards\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Episode\", i)\n",
    "            \n",
    "    clear_output(wait=True)\n",
    "    print(\"Done\")\n",
    "    return total_rewards\n",
    "\n",
    "def test_car(agent, env, wait=0.01, render=True): \n",
    "    state = toDiscreteState(env.reset())\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        state = toDiscreteState(state)\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        \n",
    "        if render:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            # Show some information\n",
    "            print(\"Time step:\", step)\n",
    "            print(\"Reward:\", reward)\n",
    "            print(\"Total reward:\", total_reward)\n",
    "            time.sleep(wait)\n",
    "    env.close()\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the SARSA-agent implemented previously. We use discount $\\gamma = 0.99$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "agent = SARSA(30*30, 3, gamma=0.99, alpha=0.1, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a relatively hard problem to solve. The reward signal gives $-1$ for each step until the flag is reached. Hence, before the agent has reached the flag the first time all actions will look equally bad. For this reason you will probably not see any improvement at all for about 1000 episodes.  \n",
    "\n",
    "The environment has a time limit of 200 time steps. So if an episode goes on for longer than 200 time steps, it will be ended prematurely.\n",
    "\n",
    "Try to train it for 2000 episodes to start with (this will take some time). To find a policy that is near optimal, you will probably have to increase the number of episodes to at least 10 000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_episodes = 2000\n",
    "rewards = train_car(agent, env, n_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the total rewards given after each episode. Note that, since the episode stops if the episode goes on for more than 200 time steps, to worst possible total reward we can get is -200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we test to see how the car behaves in practice. If it does not manage to reach the flag every time you run it, try to train it for more episodes. \n",
    "\n",
    "*Note:* If you re-run the code cell with `train_car` without resetting the agent, it will continue to train from your current estimated `Q`-function instead of restarting from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent.epsilon = 0\n",
    "total_reward = test_car(agent, env)\n",
    "agent.epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on exploration:** Here we again have a case where we initialize $Q$ to zero, but all rewards are negative. Hence, this will aid with the exploration, since $Q$ for actions not tested before will always look better than for actions we have tested previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL BA2.4 - Q-learning, optimal policy, `Taxi-v3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Q-learning to find the optimal policy for the `Taxi-v3` environment. This is an undiscounted problem, i.e. $\\gamma = 1$. You can use $\\alpha = 0.1$, $\\varepsilon = 0.1$ and train on at least 10 000 episodes.\\\n",
    "使用Q-learning寻找`Taxi-v3`环境的最佳策略。这是一个未贴现的问题，即$\\gamma = 1$。你可以使用$\\alpha = 0.1$， $\\varepsilon = 0.1$，至少训练10000个episode。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  Discrete(500)\n",
      "Action space:  Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "print(\"State space: \", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentQ = QAgent(env.observation_space.n, env.action_space.n, gamma = 1, alpha = 0.1, epsilon = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([15., 27., 43., ...,  0.,  0.,  0.]), -1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_q(agentQ, env, n_episodes = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Time step: 16\n",
      "Reward: 20\n",
      "Total reward: 5\n"
     ]
    }
   ],
   "source": [
    "agentQ.epsilon = 0\n",
    "total_reward = test_policy(agentQ, env, max_steps = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(env, Q):\n",
    "    n_states = env.observation_space.n\n",
    "    optimal_policy = np.zeros(n_states)\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        optimal_policy[s] = np.argmax(Q[s,:])\n",
    "    \n",
    "    return optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4., 4., 4., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 5.,\n",
       "       0., 0., 0., 0., 3., 3., 3., 0., 0., 0., 0., 0., 3., 0., 3., 0., 0.,\n",
       "       0., 0., 3., 0., 1., 0., 0., 0., 0., 0., 2., 0., 2., 2., 0., 0., 0.,\n",
       "       0., 2., 0., 2., 0., 0., 1., 0., 1., 0., 0., 0., 0., 2., 0., 2., 2.,\n",
       "       3., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 3., 0., 4.,\n",
       "       0., 4., 4., 0., 3., 0., 0., 3., 0., 3., 0., 0., 5., 3., 0., 0., 1.,\n",
       "       1., 1., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 0.,\n",
       "       0., 0., 3., 1., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       3., 0., 2., 0., 0., 0., 0., 0., 2., 0., 2., 2., 0., 0., 0., 0., 2.,\n",
       "       0., 2., 0., 0., 2., 0., 3., 0., 0., 0., 0., 2., 0., 2., 1., 0., 0.,\n",
       "       0., 3., 0., 0., 0., 0., 0., 2., 0., 0., 0., 3., 3., 3., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 3., 3., 3., 0., 0., 3., 1., 3., 3., 0., 1., 1., 1.,\n",
       "       2., 0., 2., 2., 0., 0., 0., 0., 2., 2., 2., 0., 1., 2., 0., 2., 0.,\n",
       "       1., 1., 1., 2., 0., 2., 2., 3., 3., 0., 3., 2., 2., 2., 0., 3., 2.,\n",
       "       3., 2., 0., 3., 3., 3., 2., 0., 2., 1., 3., 3., 0., 3., 2., 2., 2.,\n",
       "       0., 3., 2., 3., 2., 0., 3., 3., 3., 2., 0., 1., 1., 3., 3., 0., 3.,\n",
       "       0., 0., 0., 0., 3., 1., 3., 0., 0., 3., 3., 3., 1., 0., 1., 1., 3.,\n",
       "       3., 0., 3., 0., 0., 3., 0., 3., 1., 3., 3., 0., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 2., 0., 1., 2., 1., 1., 0., 1., 1., 2., 1., 0., 3., 1., 1., 3.,\n",
       "       0., 1., 1., 3., 1., 0., 1., 1., 3., 1., 0., 3., 1., 1., 1., 0., 1.,\n",
       "       2., 2., 2., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 1., 1., 0., 0., 3., 3., 3., 1., 0., 1., 1., 3., 3., 0.,\n",
       "       1., 3., 3., 3., 0., 0., 1., 1., 3., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       4., 4., 0., 4., 1., 1., 1., 0., 1., 1., 5., 1., 0., 1., 1., 1., 1.,\n",
       "       0., 1., 2., 1., 1., 0., 1., 1., 2., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 3., 1., 0., 1., 1., 3., 1., 0., 1., 1., 1., 1., 0., 3., 0., 1.,\n",
       "       2., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 4., 4., 4., 0.,\n",
       "       1., 1., 1., 5., 0., 3., 3., 1., 1., 0., 1., 1., 3., 3., 0., 1., 3.,\n",
       "       3., 3., 0., 3., 0., 2., 3.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op = optimal_policy(env, agentQ.Q)\n",
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op[174]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op[186]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op[422]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (PycharmProjects)",
   "language": "python",
   "name": "pycharm-53e69571"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "327px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
