{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tinkering Notebook for Lecture 1 - Introduction\n",
    "\n",
    "In this notebook you will see that your Jupyter environment is working, and also try to use reinforcement learning on a simple problem.\\\n",
    "在这个笔记本里，你会看到你的Jupyter环境正在起作用，并且尝试在一个简单的问题上使用强化学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "* ### [1. Imports](#sec1)\n",
    "* ### [2. A very short introduction to Reinforcement Learning (RL)](#sec2)\n",
    "* ### [3. Getting started with Open AI gym](#sec3)\n",
    " * #### [3.1 Taxi driver](#sec3_1)\n",
    " * #### [3.2 MountainCar environment](#sec3_2)\n",
    "* ### [4. *Multi-armed bandits](#sec4)\n",
    " * #### [4.1 Learn](#sec4_1)\n",
    " * #### [4.2 Act](#sec4_2)\n",
    " * #### [4.3 Testing MultiarmedBandits](#sec4_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports <a id=\"sec1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook you have to import the following packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Packages needed for this assignment\n",
    "import gym\n",
    "import gym_bandits # Implements 10-armed bandits from Chapter 2 in the textbook\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. A very short introduction to Reinforcement Learning (RL) <a id=\"sec2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<font color = blue> Definition </font> Reinforcement Learning**\n",
    "Reinforcement Learning (RL) is a family of modern machine learning techniques for learning how to make sequential decisions using feedback from real and/or simulated environments.\\\n",
    "强化学习(RL)是一组现代机器学习技术，用于学习如何利用真实和/或模拟环境的反馈做出序贯决策。\n",
    "\n",
    "In RL the agent (e.g. computer program) interacts with an environment and gets rewards.\n",
    "The environment could be a physical or chemical system, resource management, traffic light control, advertisement network, a computer game or many other things.\n",
    "The goal of the agent is typically to maximize the cumulative sum of rewards over some number of sequential actions.\n",
    "In order to do so, the agent learns from observations in order to improve its future actions.\\\n",
    "在RL中，agent(如计算机程序)与环境交互并获得奖励。\n",
    "环境可以是物理或化学系统、资源管理、交通信号灯控制、广告网络、电脑游戏或许多其他东西。\n",
    "agent的目标通常是在一定数量的连续行动中最大化累积奖励的总和。\n",
    "为了做到这一点，agent从观察中学习，以改进其未来的行为。\n",
    "\n",
    "Some important concepts in RL:\n",
    "* __Agent__: The learner and decision maker.\n",
    "* __Environment__: What the agent interacts with.\n",
    "* __State__: A state $s \\in \\mathcal{S}$ is a succint [səkˈsɪŋkt] representation of the environments current state.\\\n",
    "状态$s \\in \\mathcal{S}$是环境当前状态的简洁（succint）表示。\n",
    "* __Action__: The agent can take actions $a \\in \\mathcal{A}$ in order to change the state of the environment.\\\n",
    "agent可以采取行动$a \\in \\mathcal{A}$来改变环境的状态。\n",
    "* __Observation__: After each action the agent recieves an observation of the environment. For most of the course we will assume that the agent observs the state $s$.\\\n",
    "在每个动作之后，代理接收到对环境的一个观察。在课程的大部分时间里，我们将假定代理观察状态$s$。\n",
    "* __Policy__: Rules for how the agent choses the next action given the current state, $a = \\pi(s)$.\\\n",
    "给定当前状态，agent如何选择下一个操作的规则$a = \\pi(s)$。\n",
    "* __Reward__: An immediate reward $R(s,a)$ that the agent gets for taking action $a$ in state $s$.\\\n",
    "agent在状态$s$时采取行动$a$所得到的即时报酬$R(s,a)$。\n",
    "\n",
    "For more discussion on the meaning of these concepts, see Lecture 1 and the course book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Getting started with Open AI gym <a id=\"sec3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning, different methods can be evaluated on static data sets. In RL, however, the algorithms must be tested on interactive (dynamic) environments. This is where OpenAI Gym comes in.\\\n",
    "在监督学习中，不同的方法可以在静态数据集上进行评估。然而，在RL中，算法必须在交互式（动态）环境中进行测试。这就是OpenAI健身房的作用所在。\n",
    "\n",
    "[OpenAI Gym](http://gym.openai.com) is a toolkit (a set of software tools 配套软件；软件包；工具箱) for comparing RL-algorithms. It contains a wide variety of environments that you can train your agents on, and it is often used for benchmarking new methods in the RL research litterature.\\\n",
    "[OpenAI Gym](http://gym.openai.com)是一个比较RL算法的工具包（toolkit）。它包含了各种各样的环境，您可以在这些环境中培训代理，并且它经常被用于RL研究文献中的新方法的基准测试。\n",
    "\n",
    "There are also [leaderboards](https://github.com/openai/gym/wiki/Leaderboard) for different gym-environments, showing which methods has been most successful so far.\\\n",
    "还有针对不同gym环境的[排行榜](https://github.com/openai/gym/wiki/Leaderboard)，显示到目前为止哪种方法最成功。\n",
    "\n",
    "In the exercises, \"Tinkering Notebooks\", we will make use of OpenAI Gym.\\\n",
    "在“Tinkering笔记本”的练习中，我们将使用OpenAI Gym。\n",
    "\n",
    "To test your installation of OpenAI gym, and learn about basic usage, we will look at the relatively simple Taxi-environment.\\\n",
    "为了测试OpenAI gym的安装，并了解基本用法，我们将查看相对简单的出租车环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Taxi driver <a id=\"sec3_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment there are four locations. \n",
    "Your job is to pick up a passenger at one of these locations, and then drop her of at another location.\\\n",
    "在这个环境中有四个地点。你的工作是在其中一个地点接一名乘客，然后把她送到另一个地点。\n",
    "\n",
    "To test this environment, run the following lines of code.\\\n",
    "要测试此环境，请运行以下代码行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: 74\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "state = env.reset()\n",
    "print('Initial state:', state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods used above are:\n",
    "* `make()`: Creates a gym environment object. In this case we use the Taxi-environment.\\\n",
    "创建一个gym环境对象。在本例中，我们使用出租车环境。\n",
    "* `reset()`: Resets the environment to an initial state, and returns the initial state.\\\n",
    "将环境重置为初始状态，并返回初始状态。\n",
    "\n",
    "In the case of the Taxi-environment, the initial state is chosen randomly, so it will be different every time you run `env.reset()`.\\\n",
    "在出租车环境中，初始状态是随机选择的，因此每次运行`env.reset()`时，初始状态都会不同。\n",
    "\n",
    "To visualize the current state of the environment, you can use the function `render()`. In the Taxi-environment the visualization is text-based.\\\n",
    "要可视化环境的当前状态，可以使用`render()`函数。在出租车环境中，可视化是基于文本的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | :\u001B[43m \u001B[0m:G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001B[35mY\u001B[0m| : |\u001B[34;1mB\u001B[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filled square represents the taxi, the letters (R, G, Y and B) represents possible pickup and destination locations, and | represents a wall. The blue letter is the passenger, and the purple is the destination.\\\n",
    "填满的正方形代表出租车，字母(R, G, Y和B)代表可能的载客地点和目的地，|代表一堵墙。蓝色的字母是乘客，紫色的是目的地。\n",
    "\n",
    "Next we take a look at the state space $\\mathcal{S}$ (all possible states) and action space $\\mathcal{A}$ (all possible actions).\\\n",
    "接下来，我们看看状态空间$\\mathcal{S}$（所有可能的状态）和操作空间$\\mathcal{A}$（所有可能的操作）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Discrete(500)\n",
      "Action space: Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__State space__: Contains all possible states of the environment. We see that the state space here contains 500 discrete states. In this case each state corresponds to a position of the taxi (25 possibilities), the passengers position (5 possibilities, including picked up) and the destination (4 possibilities). Hence, there are $25\\times5 \\times 4 = 500$ possible states.\\\n",
    "包含环境的所有可能状态。我们看到状态空间包含500个离散状态。在这种情况下，每个状态对应出租车的一个位置(25种可能)、乘客的位置(5种可能，包括被搭载)和目的地(4种可能)。因此，有$25 × 5 × 4 = 500$可能的状态。\n",
    "\n",
    "__Action space__: Contains all possible actions the agent can take. In this case the six discrete actions corresponds to: 0 - south, 1 - north, 2 - east, 3 - west, 4 - pickup, 5 - dropoff.\\\n",
    "包含agent可以采取的所有可能操作。在这种情况下，六个离散的行动对应于:0-南，1-北，2-东，3-西，4-接客，5-落客。\n",
    "\n",
    "***Remark***: Note that we actually asked for the `observation_space` and not the state space. As mentioned above, we will for most of the course assume that the observation space and the state space are the same. However, in some RL-problems the full state cannot be observed, so the space of possible states may not be the same as the space of possible observations. For example, the complete state of an inverted pendulum consists of both the angle and velocity, but maybe only the angle is meausred directly.\\\n",
    "注意，我们实际上请求的是`observation_space`而不是状态空间。如上所述，我们将在大部分课程中假定观测空间和状态空间是相同的。然而，在一些RL问题中，完全状态是无法观测到的，所以可能状态的空间可能与可能观测的空间不相同。例如，一个倒立摆的完整状态包括角度和速度，但可能只有角度是直接测量的。\n",
    "\n",
    "We next see how the agent can interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | :\u001B[43m \u001B[0m:G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001B[35mY\u001B[0m| : |\u001B[34;1mB\u001B[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "New state: 74\n",
      "Reward: -1\n",
      "Done: False\n",
      "Info: {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "new_state, reward, done, info = env.step(1) # Take action 1 (north)\n",
    "env.render()\n",
    "print(\"New state:\", new_state)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Done:\", done)\n",
    "print(\"Info:\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it was possible, the taxi should now have moved one step north (if the taxi started at the top row then it will not move). The step-function returns the following information:\\\n",
    "如果可能的话，出租车现在应该向北移动了一步(如果出租车从第一排开始，那么它就不会移动)。step函数返回以下信息:\n",
    "* __New state__: The state after the action is taken.\\\n",
    "动作执行后的状态。\n",
    "* __Reward__: The immediate reward. In the taxi-environment the reward for illegal \"pickup\" or \"dropoff\" is -10, successfully delivering the passenger gives +20, and any other action gives -1.\\\n",
    "即刻奖励。在出租车环境中，非法“搭车”或“搭车”的奖励为-10，成功搭车的奖励为+20，其他行为的奖励为-1。\n",
    "* __Done__: Is the environment done? In the Taxi-environment this will be false until the passenger is successfully dropped at her destination, or the number of actions taken gets larger than 200.\\\n",
    "环境解决了吗?在出租车环境中，这将为假，直到乘客成功降落到目的地，或者采取的操作数大于200。\n",
    "* __info__: Additional information mainly used for debugging.\\\n",
    "附加信息主要用于调试。\n",
    "\n",
    "The goal of the agent is to maximize the total reward. In this environment this means to deliver the passenger to her destination in as few steps as possible. If more than 200 actions are taken we will stop even if the passenger is not at her destination.\\\n",
    "agent的目标是使总报酬最大化。在这种环境中，这意味着以尽可能少的步骤将乘客送到目的地。如果采取超过200项措施，即使乘客没有到达目的地，我们也会停止。\n",
    "\n",
    "One (quite bad) strategy for the taxi problem is to take a random action every time. Inside a gym-environment this can be done using `env.action_space.sample()`, which samples a random action from the action space. Look through the following loop and make sure that you understand whats going on. (We use `clear_output()` to clear the output of the Jupyter cell, and `time.sleep()` to pause between each action)\\\n",
    "解决出租车问题的一个(非常糟糕的)策略是每次都采取随机行动。在一个gym环境中，可以使用`env.action_space.sample()`来实现这一点，它从动作空间中采样随机动作。检查下面的循环，确保你理解发生了什么。(我们使用`clear_output()`来清除Jupyter单元格的输出，并使用`time.sleep()`在每个操作之间暂停)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001B[35mR\u001B[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001B[34;1mY\u001B[0m| :\u001B[43m \u001B[0m|B: |\n",
      "+---------+\n",
      "  (East)\n",
      "Time step: 200\n",
      "Reward: -1\n",
      "Total reward: -749\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "time_step = 0\n",
    "total_reward = 0\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample() # Choose random action\n",
    "    state, reward, done, info = env.step(action) \n",
    "    total_reward += reward\n",
    "    time_step += 1\n",
    "    clear_output(wait = True)\n",
    "    env.render()\n",
    "    print(\"Time step:\", time_step)\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"Total reward:\", total_reward)\n",
    "    time.sleep(.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, random actions is, unsurprisingly, not a good policy. However, if the agent has no prior information about the environment or the goal, what else could it do?\\\n",
    "如您所见，随机行动并不是个好政策。然而，如果代理事先没有关于环境或目标的信息，它还能做什么呢?\n",
    "\n",
    "If we know everything about the environment, we could create an array with 500 entries, where each entry tells us what the optimal action is in the corresponding state. One way of finding this array is to use dynamic programming. This will be discussed in Lecture 3.\\\n",
    "如果我们知道关于环境的一切，我们可以创建一个包含500个条目的数组，其中每个条目告诉我们在相应状态下的最佳操作是什么。找到这个数组的一种方法是使用动态编程。这将在第三讲中讨论。\n",
    "\n",
    "In Lecture 4 and Lecture 5 we will see how the agent can learn the array without prior knowledge, by just observing the reward received for taking different actions in different states.\\\n",
    "在第四讲和第五讲中，我们将看到agent如何在没有先验知识的情况下，通过观察在不同状态下采取不同行动所获得的奖励来学习数组。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 MountainCar environment <a id=\"sec3_2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the course we will use some environments where `render()` uses more advanced graphics. In order to make sure that this works on your installation, we also test the *MountainCar*-environment.\\\n",
    "在课程中，我们将使用一些环境，其中`render()`使用了更高级的图形。为了确保这在您的安装上工作，我们还测试*MountainCar*环境。\n",
    "\n",
    "In this case, `render()` opens a new window where the current state of the environment is visualized.\\\n",
    "在本例中，`render()`将打开一个新窗口，其中显示环境的当前状态。\n",
    "\n",
    "***Remark:*** We call render two times. This should not be needed, but sometimes on Windows the first call only opens the window without actually rendering the frame.\\\n",
    "我们调用渲染两次。这是不需要的，但是在Windows上，第一次调用只会打开窗口而没有真正渲染帧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Box(-1.2000000476837158, 0.6000000238418579, (2,), float32) Low: [-1.2  -0.07] High: [0.6  0.07]\n",
      "Action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "env.render()\n",
    "env.render() \n",
    "print(\"State space:\", env.observation_space, \n",
    "      \"Low:\", env.observation_space.low, \n",
    "      \"High:\", env.observation_space.high)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem encountered while running `env.render()`:\n",
    "```python\n",
    "ImportError: Can't find framework /System/Library/Frameworks/OpenGL.framework.\n",
    "ImportError: Error occurred while running `from pyglet.gl import *`\n",
    "```\n",
    "Solution:\n",
    "https://github.com/pyglet/pyglet/issues/274#issuecomment-727584254"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to close this window you call `env.close()`. If you have overwritten __env__ for some reason, you will have to restart your Jupyter kernel in order to close the window.\\\n",
    "如果你想关闭这个窗口，你可以调用`env.close()`。如果你因为某些原因覆盖了__env__，你将不得不重新启动你的Jupyter内核以关闭窗口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Action space__: We have three discrete actions, 0 - push left, 1 - no push, 2 - push right\\\n",
    "我们有三个独立的动作，0-推左，1-不推，2-推右\n",
    "* __State space__: `Box(2,)` represents a two dimensional box, so it is a continuous space. Each state is a vector with two elements. In MouintainCar the first element is the position of the car, and can go between -1.2 and +0.6. The second element is the velocity of the car and can go between -0.07 and +0.07.\\\n",
    "`Box(2,)`表示一个二维的Box，所以它是一个连续空间。每个状态都是一个带有两个元素的向量。在MouintainCar中，第一个元素是汽车的位置，可以在-1.2到+0.6之间变化。第二个元素是车速可以在-0.07到+0.07之间变化。\n",
    "* __Reward__: The reward given by the environment is -1 for each action.\\\n",
    "环境给予每个行动的奖励是-1。\n",
    "* __Done__: The environment is done when the flag is reached (position = 0.5), or 200 actions have been used. The goal is thus to reach the flag in as few steps as possible.\\\n",
    "当到达flag(position = 0.5)或使用了200个操作时，环境就完成了。因此，我们的目标是以尽可能少的步骤到达flag。\n",
    "\n",
    "Lets first try to solve this environment by the simple policy: Always push the car right (`action = 2`).\\\n",
    "让我们首先尝试通过简单的政策来解决这个问题:总是把车往右推(`action = 2`)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    \n",
    "    action = 2\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [-0.25743769 -0.00647591]\n",
      "Action: 2\n",
      "Total reward: -200.0\n",
      "Done: True\n",
      "Info: {'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "time_step = 0\n",
    "total_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = policy(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"State:\", state)\n",
    "    print(\"Action:\", action)\n",
    "    print(\"Total reward:\", total_reward)\n",
    "    print(\"Done:\", done)\n",
    "    print(\"Info:\", info)\n",
    "    \n",
    "    time.sleep(.02)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the car does not have the momentum [məˈmentəm] to overcome gravity, and thus it never reaches the flag. The agent must learn that the car needs to gain momentum by going up the left hill.\\\n",
    "不幸的是，赛车没有冲力(momentum)来克服重力，因此它永远达不到flag。代理人必须知道汽车需要通过爬左坡来获得冲力。\n",
    "\n",
    "This environment is harder than the Taxi-enivronment for RL-agents. There are two reasons for this:\\\n",
    "对于RL-agent来说，这种环境比出租车环境更困难。这有两个原因:\n",
    "\n",
    "1. The state-space is continuous (infinitely many states). Hence, it is not possible to build up a table with information about all states. In RL this can be solved using function approximations. This will be discussed in the second half of the course.\\\n",
    "状态空间是连续的(无限多个状态)。因此，不可能建立一个包含所有状态信息的表。在RL中，这个问题可以用函数近似来解决。这将在课程的后半部分进行讨论。\n",
    "\n",
    "2. The agent always gets the immediate reward -1 for each action until it reaches the flag. So all actions looks just as bad if we do not reach the flag within 200 steps, and it is very unlikely that the car reaches the flag by only using e.g. random actions. So how can it learn about the goal? We will see two possible solutions in the course: We can try to re-engineer the reward so that the agent is encouraged to go up the left slope first. Or more generally, we can encourage the agent to try to reach states that has not been seen before.\\\n",
    "agent总是为每个操作获得即时奖励-1，直到它到达flag。所以，如果我们不能在200步内到达flag，那么所有的动作看起来都一样糟糕，汽车不太可能只通过随机动作到达flag。那么它如何了解目标呢?我们将在课程中看到两种可能的解决方案:我们可以尝试重新设计奖励，鼓励代理先沿着左边的斜坡往上走。或者更普遍地说，我们可以鼓励agent试图达到以前从未见过的状态。\n",
    "\n",
    "Even though the problem is hard for an RL-agent that knows nothing about the environment, it is relatively easy for an engineer that knows the basics of the environment to find a policy that reaches the flag within 200 steps. The strength of RL is that the agent can learn these only by interacting with the environment. However, when we know more about the environment it can still be much more efficient to let an engineer figure out a good policy, and then potentially use RL to improve this policy.\\\n",
    "尽管对于一个对环境一无所知的RL-agent来说，这个问题很难解决，但是对于一个了解环境基础知识的工程师来说，找到一个在200步内到达flag的策略是相对容易的。RL的优点是agent只需通过与环境交互来学习这些。然而，当我们对环境了解得更多时，让工程师想出一个好的政策，然后使用RL来改进这个政策，仍然会更有效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: MountainCar environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Construct a policy only using current velocity\n",
    "Try to construct a policy that only uses the current velocity (`state[1]`) to determine the action. The goal is to reach the flag within 200 steps. Implement your policy in the function `policy` above and try it. Remember to execute the cell with the function after you have made your changes.\\\n",
    "尝试构建一个只使用当前速度(`state[1]`)来确定操作的策略。目标是在200步内到达flag。在上面的`policy`函数中执行您的策略并尝试它。记住，在进行更改之后，要执行包含函数的单元格。\n",
    "\n",
    "*Hint:* It is actually enough to check in which direction the car is currently moving.\\\n",
    "*提示：*实际上，检查汽车当前行驶的方向就足够了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: construct a policy that only uses the current velocity (`state[1]`)\n",
    "def policy(state):\n",
    "    \n",
    "    if state[1] > 0:\n",
    "        action = 2\n",
    "    else:\n",
    "        action = 1\n",
    "    \n",
    "    return action  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [0.4168187 0.0031269]\n",
      "Action: 2\n",
      "Total reward: -200.0\n",
      "Done: True\n",
      "Info: {'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "time_step = 0\n",
    "total_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = policy(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"State:\", state)\n",
    "    print(\"Action:\", action)\n",
    "    print(\"Total reward:\", total_reward)\n",
    "    print(\"Done:\", done)\n",
    "    print(\"Info:\", info)\n",
    "    \n",
    "    time.sleep(.02)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Implement the policy on the leaderboard\n",
    "Implement the policy that is on the leaderboard for `MountainCar-v0`: ($p$ is position,  $v$ is velocity, and the state is $s = \\begin{bmatrix} p & v \\end{bmatrix}^\\top$)\\\n",
    "实现`MountainCar-v0`排行榜上的策略:($p$为位置，$v$为速度，状态为$s = \\begin{bmatrix} p & v \\end{bmatrix}^\\top$)\n",
    "$$\n",
    "\\pi(s) = \\begin{cases} \\text{Right} & \\text{if } \\min\\{-0.09( p + 0.25)^2 + 0.03, 0.3(p + 0.9)^4 - 0.008\\} \\leq v \\leq -0.07(p+0.38)^2 + 0.07 \\\\ \\text{Left} & \\text{otherwise} \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Implement the policy that is on the leaderboard for MountainCar-v0\n",
    "def policy(state):\n",
    "    \n",
    "    p = state[0]\n",
    "    v = state[1]\n",
    "    \n",
    "    if (v >= min(-0.09*(p+0.25)**2 + 0.03, 0.3*(p+0.9)**4 - 0.008)) & \\\n",
    "       (v <= -0.07*(p+0.38)**2 + 0.07): # priority of & !!!\n",
    "        action = 2\n",
    "    else:\n",
    "        action = 1\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [-0.97729709  0.03421257]\n",
      "Action: 2\n",
      "Total reward: -200.0\n",
      "Done: True\n",
      "Info: {'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "time_step = 0\n",
    "total_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = policy(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"State:\", state)\n",
    "    print(\"Action:\", action)\n",
    "    print(\"Total reward:\", total_reward)\n",
    "    print(\"Done:\", done)\n",
    "    print(\"Info:\", info)\n",
    "    \n",
    "    time.sleep(.02)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. *Multi-armed bandits <a id=\"sec4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the assignment is based on Chapter 2 in the textbook. It will give you a test of many of the ideas in RL.\\\n",
    "这部分作业是基于课本第二章。它将给你一个关于RL的许多想法的测试。\n",
    "\n",
    "To get a more complete discussion you can read through Section 2.1-2.6. Below a short summary of these sections is given. After this you can implement and test some relatively simple RL-algorithm.\\\n",
    "要获得更完整的讨论，请阅读第2.1-2.6节。下面给出了这些部分的简短摘要。在此之后，您可以实现和测试一些相对简单的RL算法。\n",
    "\n",
    "Assume that you have 10 slot machines (one-armed bandits) in front of you. Each slot machine has an expected reward that is unknown to you. You will get to choose between the slot machines 1000 times, and your goal is to get as large total reward as possible. \\\n",
    "假设你面前有10台老虎机（单臂老虎机）。每个老虎机都有一个你不知道的预期奖励。你将在老虎机之间选择1000次，你的目标是获得尽可能大的总奖励。\n",
    "\n",
    "> **fruit machine**\\\n",
    "(*BrE*) (*NAmE* *BrE* also ˌ**one-armed ˈbanditˈslot machine**) a gambling machine that you put coins into and that gives money back if particular pictures appear together on the screen 吃角子老虎赌博机；老虎机\\\n",
    "一种赌博机器，你往里面投硬币，如果屏幕上同时出现特定的图片，它就会把钱退还给你\n",
    "\n",
    "This problem is called a multi-armed bandit problem. Now we formulate this as an RL-problem:\\\n",
    "这个问题叫做多臂老虎机问题。现在我们将其表述为RL问题:\n",
    "\n",
    "* __Action space__: $\\mathcal{A} = \\{ 0, 1, \\ldots, 9\\}$, so action $a=0$ means that we try slot machine $0$, $a=1$ means we try slot machine $1$ etc.\\\n",
    "$\\mathcal{A} = \\{ 0, 1, \\ldots, 9\\}$，所以动作$a=0$意味着我们尝试老虎机$0$，$a=1$意味着我们尝试老虎机$1$等等。\n",
    "* __State space__: Since the slot machines do not change between each action, there is just one state. This simplifies the problem: The environment does not change when an action is taken. Hence, the agent can concentrate on the immediate reward  it gets for each action, and it does not have to worry about how the action changes the state of the environment.\\\n",
    "因为老虎机不会在每个动作之间改变，所以只有一种状态。这简化了问题:当采取行动时，环境不会改变。因此，agent可以专注于它从每个行动中获得的即时奖励，而不必担心行动如何改变环境的状态。\n",
    "* __Reward__: The return $R$ we get from the slot machine we picked.\\\n",
    "我们从老虎机中得到的回报$R$。\n",
    "\n",
    "In order to formalize our goal, we define the value of an action, $q_*(a)$, to be the expected reward when $a$ is chosen, i.e., $q_*(a) = \\mathbb{E}[ R | a]$. Note that $q_*(a)$ can be represented by an array with 10 elements, where each element gives the value for the corresponding action.\\\n",
    "为了规范我们的目标，我们定义了一个动作的值$q_*(a)$，作为选择$a$时的预期回报，即$q_*(a) = \\mathbb{E}[ R | a]$。注意，$q_*(a)$可以由一个包含10个元素的数组表示，其中每个元素给出相应操作的值。\n",
    "\n",
    "If we know the value function $q_*(a)$, the optimal policy is to always choose the action\\\n",
    "如果我们知道值函数$q_*(a)$，那么最优策略就是始终选择动作\n",
    "\n",
    "$$\n",
    "A = \\underset{a}{\\operatorname{argmax}} q_*(a).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Learn <a id=\"sec4_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we do not know what $q_*(a)$ is. An option is to try $n-1$ actions in order to get an estimate $Q_n(a)$ of the expected reward for each slot machine. A reasonable estimate of the the expected reward for a specific action $a$ is the average reward seen so far when we have taken action $a$. That is, when $n-1$ actions have been taken, we can compute an estimate of $q_*(a)$ as\\\n",
    "然而，我们不知道$q_*(a)$是什么。一种选择是尝试$n-1$次动作，以获得每台老虎机预期的$Q_n(a)$奖励。对特定行动$a$的预期奖励的合理估计，是我们采取行动$a$时，到目前为止所看到的平均奖励。也就是说，当执行$n-1$动作时，我们可以计算$q_*(a)$的估计值\n",
    "\n",
    "$$\n",
    "Q_n(a) = \\frac{ \\text{ sum of rewards when $a$ taken prior to $n$} }{\\text{number of times $a$  taken prior to $n$}} \\quad \\text{(see eq 2.1 in textbook)},\n",
    "$$\n",
    "\n",
    "where we let $Q_n(a) = 0$ if the action $a$ was not taken prior to the $n$th action. In particular, $Q_1(a) = 0$ for all $a$.\\\n",
    "其中，如果动作$a$在第$n$之前没有被执行，则让$Q_n(a) = 0$。特别地，对于所有的$a$， $Q_1(a) = 0$。\n",
    "\n",
    "Instead of recomputing the sums every time a new action is taken, we can update this sums incrementally. Equation (2.3) in the textbook shows how to do this. That is, if the $n$th action taken is $A_n$, and the received reward is $R_n$, then $Q_{n+1}(A_n)$ can be computed from $Q_n(A_n)$ as\\\n",
    "我们可以增量地更新这些总和，而不是每次执行新操作时都重新计算这些总和。课本中的公式(2.3)说明了如何做这个。即，如果第$n$动作为$A_n$，而收到的奖励为$R_n$，则$Q_{n+1}(A_n)$可由$Q_n(A_n)$计算得到\n",
    "\n",
    "$$\n",
    "Q_{n+1}(A_n) = Q_{n}(A_n) + \\frac{1}{N(A_n)}(R_n - Q_n(A_n))\n",
    "$$\n",
    "where $N(A_n)$ is the number of times that $A_n$ has been taken so far.\\\n",
    "其中$N(A_n)$是到目前为止，$A_n$被执行的次数。\n",
    "\n",
    "If we initialize the estimate to $Q_1(a) = 0$ for all $a \\in \\mathcal{A}$, then both expressions for $Q_n(a)$ are equivalent. For implementation, the second expression is quite useful. Here we only need to create an array with 10 elements (one for each possible action) and whenever action $a$ is taken and the reward is observed, we update element $a$ in the array.\\\n",
    "如果我们对所有的$a \\in \\mathcal{A}$，将估计初始化为$Q_1(a) = 0$，那么$Q_n(a)$的两个表达式是等价的。对于实现，第二个表达式非常有用。这里我们只需要创建一个包含10个元素的数组(每个元素对应一个可能的动作)，当执行动作$a$并观察到奖励时，我们就更新数组中的元素$a$。\n",
    "\n",
    "Pseudo-code for this can be written as\n",
    "\n",
    "*Initialize:*\n",
    "\n",
    "For all $a \\in \\mathcal{A}$, \n",
    "  * $\\quad Q(a) \\leftarrow 0$\n",
    "  * $\\quad N(a) \\leftarrow 0$\n",
    "  \n",
    "*Learn:*\n",
    "\n",
    "When action $A$ is taken with received reward $R$, update the estimates:\\\n",
    "当行动$A$与收到的奖励$R$同时进行时，更新估算:\n",
    "  * $N(A) \\leftarrow N(A) + 1$\n",
    "  * $Q(A) \\leftarrow Q(A) + \\frac{1}{N(A)} (R - Q(A))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Act <a id=\"sec4_2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an estimate $Q(a)$, how should the agent choose the next action?\\\n",
    "给定一个估计$Q(a)$，代理应如何选择下一步行动?\n",
    "\n",
    "#### **<font color = blue> Definition </font> Greedy**\n",
    "A straightforward answer is to pick the one that maximizes $Q(a)$. This is in RL called the *greedy* choice, picking the action that according to the current estimates seems best.\\\n",
    "一个简单的答案是选择一个最大化$Q(A)$的。这在RL中被称为“贪婪的”选择，根据当前的估计选择最好的行动。\n",
    "\n",
    "The problem with the greedy choice is that the estimates may be incorrect. If the current estimate of the optimal action's value is too low then the greedy agent will not take that action, and thus never update the estimated value of it. In other word, the agent is not exploring all possibilities. A simple, but often quite effective, way of adding exploration is that the agent use the greedy option most of the time, but with probability $\\varepsilon$ it takes a random action.\\\n",
    "这种贪婪选择的问题在于，估计可能是不正确的。如果对最优操作的当前估值过低，那么贪婪的代理将不会采取该操作，因此永远不会更新该操作的估值。换句话说，代理并没有探索所有的可能性。添加探索的一种简单但通常非常有效的方法是，代理在大多数情况下使用贪婪选项，但有可能$\\varepsilon$它采取随机操作。\n",
    "\n",
    "This is called an $\\varepsilon$-greedy policy. In this way the agent will always continue to explore different possibilities. The $\\varepsilon$-greedy policy given an estimate $Q(a)$ can be written as\\\n",
    "这被称为$\\varepsilon$-greedy策略。这样代理就会一直不断地探索不同的可能性。给定估计$Q(a)$的$\\varepsilon$-greedy策略可以写成\n",
    "\n",
    "$$\n",
    "\\begin{cases} \n",
    "\\underset{a}{\\operatorname{argmax}} Q(a) & \\text{with probability } 1-\\varepsilon \\\\ \n",
    "\\text{random action} & \\text{with probability } \\varepsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "When several actions have an estimate equal to the maximum, the agent can break ties any way it wants (e.g. randomly).\\\n",
    "当几个动作的估计值等于最大值时，代理可以以任何它想要的方式(例如随机)打破联系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Testing MultiarmedBandits <a id=\"sec4_3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OpenAI gym, the multi-armed bandits are implemented in the `MultiarmedBandits-v0` environment. This environment is not in the standard gym package, so to use it you have to  `import gym_bandits`.\\\n",
    "OpenAI gym在`MultiarmedBandits-v0`环境中实现了多臂老虎机。这个环境不在标准的gym包中，所以要使用它，你必须`import gym_bandits`。\n",
    "\n",
    "Below you can see an example of how to use this environment. A class `Agent` is used to implement an agent that does not learn anything, and always chooses a random action.\\\n",
    "下面您可以看到如何使用这个环境的示例。类`Agent`用于实现一个不学习任何东西的代理，总是选择一个随机动作。\n",
    "\n",
    "In the exercise below you will change the code in this class in order to implement the methods described above (and in Chapter 2 of the textbook).\\\n",
    "在下面的练习中，您将更改这个类中的代码，以实现上面描述的方法(以及教科书的第2章)。\n",
    "\n",
    "*Remark:* The environment is by default a 10-armed bandit. If you want e.g. a 15-armed bandit, you can use `env = gym.make('MultiarmedBandits-v0', nr_arms=15)`. If you want to see what the true $q_*(a)$ is, you can use `env.values` (this will of course change everytime you call `env.reset()`). This can be interesting if you want to compare it to the estimated $Q$ that your agent has learned.\\\n",
    "*备注:*环境默认是10臂老虎机。如果你想要一个15臂老虎机，你可以使用`env = gym.make('MultiarmedBandits-v0', nr_arms=15)`。如果你想知道真正的$q_*(a)$是什么，你可以使用`env.values`(这当然会在每次你调用`env.reset()`时发生变化)。如果你想将其与你的代理所了解到的$Q$进行比较，这可能会很有趣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Discrete(1)\n",
      "Action space: Discrete(10)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MultiarmedBandits-v0\")\n",
    "print('State space:', env.observation_space)\n",
    "print('Action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, epsilon = 0, nr_arms = 10):\n",
    "        self.epsilon = epsilon\n",
    "        self.nr_arms = nr_arms\n",
    "        self.N = np.zeros(nr_arms)\n",
    "        self.Q = np.zeros(nr_arms)\n",
    "        \n",
    "    def learn(self, action, reward):\n",
    "        self.N[action] += 1\n",
    "        self.Q[action] += 1/self.N[action] * (reward - self.Q[action])\n",
    "    \n",
    "    def act(self):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            action = np.argmax(self.Q)\n",
    "        else:\n",
    "            action = np.random.choice(self.nr_arms) # Random action\n",
    "                    \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 388.11\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "agent = Agent(epsilon = 0.1)\n",
    "rewards = np.empty(1000)\n",
    "for t in range(1000):\n",
    "    action = agent.act()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    rewards[t] = reward\n",
    "    agent.learn(action, reward)\n",
    "\n",
    "print('Total reward: %.2f' % np.sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the reward given for each action\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Reward');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Multi-armed bandits\n",
    "\n",
    "[【强化学习】多臂老虎机——E_greedy、UCB、Gradient Bandit 算法 代码实现](https://blog.csdn.net/weixin_45839693/article/details/111307638)\n",
    "\n",
    "a) Change the `Agent`-class in order to implement an $\\varepsilon$-greedy agent. To do this, change the `learn`-method to implement the update of $Q$ given an action with corresponding reward. Then implement the $\\varepsilon$-greedy policy in the `act`-function.\\\n",
    "更改`Agent`类以实现$\\varepsilon$-greedy agent。为此，改变`learn`方法来实现更新$Q$，并给予相应的奖励。然后在`act`函数中实现$\\varepsilon$-greedy策略。\n",
    "\n",
    "b) Try to run your agent with e.g. $\\varepsilon = 0.1$.\\\n",
    "尝试使用$\\varepsilon = 0.1$运行你的agent。\n",
    "\n",
    "d) When you have tested that your implementation, you can run the code below to reproduce plots like in Figure 2.2 in the textbook. Here we do 2000 runs using the agent, and then take the average reward at different time steps. The code may take some minute to run!\\\n",
    "当您测试了您的实现之后，您可以运行下面的代码来重现教科书中如图2.2所示的图。这里我们使用代理进行2000次运行，然后在不同的时间步骤中获取平均奖励。代码可能需要几分钟才能运行！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epsilon = 0.1 # Change this to try different values\n",
    "rewards = np.zeros((2000, 1000)) \n",
    "\n",
    "for i in range(2000):\n",
    "    agent = Agent(epsilon = epsilon)\n",
    "    env.reset()\n",
    "    \n",
    "    for t in range(1000):\n",
    "        action = agent.act()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        rewards[i,t] = reward\n",
    "        agent.learn(action, reward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x119791eb0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_rewards = np.mean(rewards, 0)\n",
    "plt.plot(mean_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}