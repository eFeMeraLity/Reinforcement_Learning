{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RL BA1 - MDPs and Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Exercise 3.4 in the textbook (page 53).\n",
    "\n",
    "Give a table analogous to that in Example 3.3, but for $p(s',r|s,a)$. It should have columns for $s$, $a$, $s'$, $r$, and $p(s',r|s,a)$, and a row for every 4-tuple for which $p(s',r|s,a) > 0$.\\\n",
    "给出一个类似于示例3.3中的表，但是是对于$p(s',r|s,a)$的。它应该有$s$，$a$，$s'$，$r$，和$p(s',r|s,a)$的列，和每一个对应于4元组的行，其中，$p(s',r|s,a) > 0$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__\n",
    "* [Reinforcement Learning (RL) how to obtain $p(s',r|s,a)$](https://ai.stackexchange.com/questions/8554/reinforcement-learning-rl-how-to-obtain-ps-rs-a)\n",
    "\n",
    "The reward set is actually $\\mathcal{R} = \\{0,1,-3\\}$.\n",
    "\n",
    "|$s$|$a$|$s'$|$r$|$p(s',r|s,a)$|\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|high|search|high|$1$|$\\alpha r_{search}$|\n",
    "|high|search|high|$0$|$\\alpha (1 - r_{search})$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4.1 `GridWorld-v0`\n",
    "\n",
    "Consider the `GridWorld-v0` environment studied in Tinkering Notebook 2 with discount rate $\\gamma = 1$. The environment is also described in Example 4.1 in the textbook.\n",
    "考虑在Tinkering Notebook 2中学习的discount率为$\\gamma = 1$的`GridWorld-v0`环境。教材中的示例4.1也描述了环境。\n",
    "\n",
    "Let $\\pi(a|s)$ be a uniformly random policy (in all states all actions have the same probability). The state-value function $v_\\pi(s)$ for this policy is given in Figure 4.1 (lower left) on page 77 of the textbook. Given a state $s$ and action $a$, make sure that you understand how to compute $q_\\pi(s; a)$ for this environment. This can easily be done by hand (you do not need to write any code, since $v_\\pi(s)$ is given in Figure 4.1).\\\n",
    "设$\\pi(a|s)$为一致随机策略（在所有状态下，所有行为的概率都相同）。该策略的状态值函数$v_\\pi(s)$在课本第77页的图4.1（左下角）中给出。给定一个状态$s$和动作$a$，确保你理解了如何计算这个环境的$q_\\pi(s;a)$。这可以很容易地手工完成（你不需要写任何代码，因为$v_\\pi(s)$在图4.1中给出）。\n",
    "\n",
    "<img src = \"RL Figure 4_1.png\">\n",
    "\n",
    "In the quizz you will be given some $s$ and $a$, and then have to compute $q_\\pi(s; a)$.\\\n",
    "在测验中，你会得到一些$s$和$a$，然后必须计算$q_\\pi(s; a)$。\n",
    "\n",
    "__Hint:__ One way to check that you are doing your computations correctly is as follows. Take e.g. state $s = 10$ and compute $q_\\pi(10; a)$ for all actions (left, down, right, up). In the last row of Figure 4.1 you can see the greedy policy w.r.t $v_\\pi(s)$. This is just\\\n",
    "检查计算是否正确的方法如下所示。假设$s = 10$，对所有动作（左，下，右，上）计算$q_\\pi(10;a)$。在图4.1的最后一行中，您可以看到关于$v_\\pi(s)$的贪婪策略。这只是\n",
    "$$\n",
    "\\pi'(s) = \\underset{a}{\\arg\\max} q_\\pi(s; a)\n",
    "$$\n",
    "You can now check that you in e.g. state $s = 10$ maximize $q_\\pi(10; a)$ with either action down or right. Also, you can check that you get $q_\\pi(1; down) = -19$.\\\n",
    "你现在可以检查你在例如，状态$s = 10$最大化$q_\\pi(10;a)$动作向下或向右。同样，您可以检查是否得到$q_\\pi(1;down) = -19$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\pi(1; left) &= -1 + (0) = -1 \\\\\n",
    "q_\\pi(1; down) &= -1 + (-18) = -19 \\\\\n",
    "q_\\pi(1; right) &= -1 + (-20) = -21 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Hence, $\\pi'(1) = left$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\pi(10; left) &= -1 + (-20) = -21 \\\\\n",
    "q_\\pi(10; down) &= -1 + (-14) = -15 \\\\\n",
    "q_\\pi(10; right) &= -1 + (-14) = -15 \\\\\n",
    "q_\\pi(10; up) &= -1 + (-20) = -21 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Hence, $\\pi'(10) = \\{down, right\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An MDP\n",
    "In this problem we consider the MDP shown in Figure 3.1. It has three state, $\\mathcal{S} = \\{A,B,C\\}$. In state $B$ and $C$ there are two possible actions called $x$ and $y$. In state $A$ only the action $x$ is available. The discount rate is $\\gamma = 0.5$, and we consider a uniform random $\\pi(a|s)$ that in in each state picks between all possible actions with equal probability.\\\n",
    "在这个问题中，我们考虑图3.1所示的MDP。它有三个状态，$\\mathcal{S} = \\{A,B,C\\}$。在状态$B$和$C$中有两种可能的动作称为$x$和$y$。在状态$A$中，只有动作$x$可用。折现率为$\\gamma = 0.5$，我们考虑一个均匀随机的$\\pi(a|s)$，它在每个状态下以相等的概率选择所有可能的行为。\n",
    "\n",
    "<img src = \"RL BA1 Figure 3_1.png\">\n",
    "\n",
    "__Figure 3.1:__ An MDP\n",
    "\n",
    "(a) It can be shown that $v_\\pi(B) = 0.356$. What is $v_\\pi(A)$ and $v_\\pi(C)$?\\\n",
    "可以得到$v_\\pi(B) = 0.356$。$v_\\pi(A)$和$v_\\pi(C)$是什么?\n",
    "\n",
    "(b) Given $v_\\pi(s)$ from part (a), find the policy that is greedy with respect to $v_\\pi$.\\\n",
    "给定(a)部分中的$v_\\pi(s)$，求出对$v_\\pi$贪婪的策略。\n",
    "\n",
    "(c) Assume that we start with an initial value function $v_1(A) = v_1(B) = v_1(C) = 2$. Perform one iteration with synchronous policy evaluation (do not use the in-place version!). What will $v_2(s)$ be?\\\n",
    "假设我们从一个初值函数$v_1(A) = v_1(B) = v_1(C) = 2$开始。使用同步策略评估执行一次迭代（不要使用就地版本!）$v_2(s)$是什么?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__\n",
    "\n",
    "(a) It can be shown that $v_\\pi(B) = 0.356$. What is $v_\\pi(A)$ and $v_\\pi(C)$?\n",
    "$$\n",
    "\\begin{align}\n",
    "v_\\pi(A) &= 1 \\times [1 \\times (-8 + 0.5 \\times v_\\pi(B))] \\\\\n",
    "v_\\pi(B) &= \n",
    "0.5 \\times [1 \\times (-2 + 0.5 \\times v_\\pi(C))] +\n",
    "0.5 \\times [0.5 \\times (2 + 0.5 \\times v_\\pi(A)) + 0.5 \\times (2 + 0.5 \\times v_\\pi(B))] \\\\\n",
    "v_\\pi(C) &= \n",
    "0.5 \\times [0.5 \\times (4 + 0.5 \\times v_\\pi(A)) + 0.5 \\times (4 + 0.5 \\times v_\\pi(B))] +\n",
    "0.5 \\times [1 \\times (8 + 0.5 \\times v_\\pi(B))]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computed v(A) = -7.822\n",
      "computed v(B) = 0.35568750000000005\n",
      "computed v(C) = 5.15575\n"
     ]
    }
   ],
   "source": [
    "vB = 0.356\n",
    "gamma = 0.5\n",
    "v_A = 1 * (1 * (-8 + gamma*vB))\n",
    "v_C = 0.5*(0.5*(4+gamma*v_A)+0.5*(4+gamma*vB)) + 0.5*(1*(8+gamma*vB))\n",
    "v_B = 0.5*(1*(-2+gamma*v_C)) + 0.5*(0.5*(2+gamma*v_A)+0.5*(2+gamma*vB))\n",
    "print(f\"computed v(A) = {v_A}\")\n",
    "print(f\"computed v(B) = {v_B}\")\n",
    "print(f\"computed v(C) = {v_C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Given $v_\\pi(s)$ from part (a), find the policy that is greedy with respect to $v_\\pi$.\n",
    "\n",
    "The greedy policy w.r.t $v_\\pi(s)$ is\n",
    "$$\n",
    "\\pi'(s) = \\underset{a}{\\arg\\max} q_\\pi(s; a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "q_\\pi(A, x) = 1 \\times (-8 + 0.5 \\times v_\\pi(B)) \\\\\n",
    "q_\\pi(B, x) = 1 \\times (-2 + 0.5 \\times v_\\pi(C)) \\\\\n",
    "q_\\pi(B, y) = 0.5 \\times (2 + 0.5 \\times v_\\pi(A)) + 0.5 \\times (2 + 0.5 \\times v_\\pi(B)) \\\\\n",
    "q_\\pi(C, x) = 0.5 \\times (4 + 0.5 \\times v_\\pi(A)) + 0.5 \\times (4 + 0.5 \\times v_\\pi(B)) \\\\\n",
    "q_\\pi(C, y) = 1 \\times (8 + 0.5 \\times v_\\pi(B))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computed q(A,x) = -7.822\n",
      "computed q(B,x) = 0.5778750000000001\n",
      "computed q(B,y) = 0.13349999999999995\n",
      "computed q(C,x) = 2.1334999999999997\n",
      "computed q(C,y) = 8.178\n"
     ]
    }
   ],
   "source": [
    "q_Ax = (1 * (-8 + gamma*vB))\n",
    "q_Bx = (1*(-2+gamma*v_C))\n",
    "q_By = (0.5*(2+gamma*v_A)+0.5*(2+gamma*vB))\n",
    "q_Cx = (0.5*(4+gamma*v_A)+0.5*(4+gamma*vB))\n",
    "q_Cy = (1*(8+gamma*vB))\n",
    "print(f\"computed q(A,x) = {q_Ax}\")\n",
    "print(f\"computed q(B,x) = {q_Bx}\")\n",
    "print(f\"computed q(B,y) = {q_By}\")\n",
    "print(f\"computed q(C,x) = {q_Cx}\")\n",
    "print(f\"computed q(C,y) = {q_Cy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Assume that we start with an initial value function $v_1(A) = v_1(B) = v_1(C) = 2$. Perform one iteration with synchronous policy evaluation (do not use the in-place version!). What will $v_2(s)$ be?\\\n",
    "假设我们从一个初值函数$v_1(A) = v_1(B) = v_1(C) = 2$开始。使用同步策略评估执行1次迭代（不要使用in-place版本！）$v_2(s)$是什么?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v(A) with 1 iteration = -7.0\n",
      "v(B) with 1 iteration = 1.0\n",
      "v(C) with 1 iteration = 7.0\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.5\n",
    "v_1A = 2\n",
    "v_1B = 2\n",
    "v_1C = 2\n",
    "\n",
    "v_2A = 1 * (1 * (-8 + gamma*v_1B))\n",
    "v_2B = 0.5*(1*(-2+gamma*v_1C)) + 0.5*(0.5*(2+gamma*v_1A)+0.5*(2+gamma*v_1B))\n",
    "v_2C = 0.5*(0.5*(4+gamma*v_1A)+0.5*(4+gamma*v_1B)) + 0.5*(1*(8+gamma*v_1B))\n",
    "\n",
    "print(f\"v(A) with 1 iteration = {v_2A}\")\n",
    "print(f\"v(B) with 1 iteration = {v_2B}\")\n",
    "print(f\"v(C) with 1 iteration = {v_2C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `FrozenLake8x8-v0`\n",
    "Consider the `FrozenLake8x8-v0` environment. It is similar to the `FrozenLake-v0` that was studied in Tinkering Notebook 2, but it consist of an $8 \\times 8$ grid and thus have 64 states.\\\n",
    "考虑`FrozenLake8x8-v0`环境。它类似于Tinkering Notebook 2中学习的`FrozenLake-v0`，但它由一个$8 \\times 8$ 的网格组成，因此有64个状态。\n",
    "\n",
    "Write a code that find an optimal policy $\\pi_*(s)$ and the corresponding value function $v_*(s)$.\\\n",
    "编写代码，查找最佳策略$\\pi_*(s)$和相应的值函数$v_*(s)$。\n",
    "\n",
    "In the quizz on you will be asked for example \"Which of these are optimal actions in state $s = 26$?\" or \"What is $v_*(26)$?\". So make sure that you can easily run code that can answer these types of questions for different states.\\\n",
    "在quizz中，你会被问到“在状态$s = 26$时，哪些是最优行为?”或者“$v_*(26)$是什么?”因此，请确保您可以轻松地运行可以回答不同状态下的这类问题的代码。\n",
    "\n",
    "__Hint:__ You can check that your code seems to be working by ensuring that you get to correct answer to the following:\\\n",
    "你可以检查你的代码似乎工作通过确保你得到以下的正确答案：\n",
    "* For the optimal policy $v_*(26) = 0.80$ (rounded to two decimals).\n",
    "* In $s = 26$ the optimal action is 0 (left)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
